

# Le retour des méthodes factorielles

L'objectif de cette partie est de comprendre l'évolution des différentes techniques de réduction de grands ensembles de données. Leur principe reste de réussir à exprimer sous forme de facteur(s), un ou plusieurs concepts non observés, latents, ou d'intérêts, par le biais de calculs sur les attributs ou propriétés d'une base de données. Replacées dans leur contexte de génèse et vocabulaire, ces méthodes de calculs feront ensuite l'objet d'une description générale, centrée sur les prérequis statistiques et connaissances des modèles mathématiques utilisés. Nous proposerons une application exploratoire via le logiciel R, par un cas pratique avec un dataset de tweets scrappés pour l'exemple sur la thématique du marketing TIC & NTIC.

Quatre algorithmes utilisés en analyse factorielle seront abordés, sous la lumière du cours de Philippe Malbos de Lyon 1 : [@malbos]

-   Analyse par Composante Principale (PCA)
-   Analyse Factorielle sur Données Mixtes (AFDM)
-   Analyse/Indexation Sémantique Latente (LSA/I)
-   Factorisation de Matrices Non-Négatives (NMF)

Développées dès 1904 par le psychologue Charles Spearman dans une ambition plus confirmatoire et hiérarchique, puis dans les années 1930 par les travaux de Hotelling [@thurstone1949] et ensuite les travaux de Benzecri dès 1963 dans une dimension cette fois, plus exploratoire, les méthodes d'analyses factorielles se sont proggressivement diluées dans la "boîte à outils" des différentes disciplines et courants académiques de la recherche scientifique. Ses principes de fonctionnement et autres calculs originels ont été confrontées à diverses sources et structures de données, que notre environnement de plus en plus numérique à amener à générer, à une vitesse s'accélérant toujours plus. @benzecri2006

A l'issue de ce chapitre il sera possible de dresser un panorama synthtique de la diversité des techniques utilisées ainsi que de comprendre les avantages et inconvénients de chacune, en termes tant, de structures de données, que de modèles mathématiques, ou bien encore de vocabulaire idoine à privilégier afin de pouvoir interpréter convenablement les résultats fournis par ces algorithmes dans un contexte de traitements statistiques de données. [@forsé] En effet, l'interaction avec ces interfaces mathématiques sont aujourd'hui largement informatisées, et le travail de d'analyste se complète également par le devoir de savoir expliciter ses "sorties logicielles". Afin de se soustraire pleinement de l'illusion visuelle, il sera nécessaire pour chaque utilisateur de s'intéresser aux coefficients plus qu'à leurs visualisations potentielles, et donc de raisonner en termes de relations de distances euclidiennes, d'interdépendance et d'écarts angulaires ou encore de corrélation, plutôt que de "proximité".

La diversité, parfois destabilisante aux premiers abords, des appellations que l'on trouve dans la littérature (ACP/I, AFC/M, LSA/I, NFM...) recoupent la même idée de synthèse des données appliquées selon *k* dimensions avec des déclinaisons robustes, complexes ou encore probabilistes.

## Méthodes & Données

### Principes généraux

L'ensemble des données suivent des traitement obéissants aux règles du calcul matriciel et leur lecture est un prérequis aux [explications](https://www.unilim.fr/pages_perso/jean.debord/math/matrices/matrices.htm) qui vont suivre. D'un point de vue historique, l'analyse factorielle est une idée issue des courants de recherche en psychologie. L'apport de cette science dans l'application computationnelle de l'analyse factorielle est importante, un des meilleurs packages jamais développé n'est autre que "psych". Les méthodes de calculs matriciels, comme nous avons pu le voir, ont également pu être généralisées par le développement de la puissance disponible, permettant de prendre en charge des traitements plus importants, liés à la taille, ou bien encore au caractère régulier ou singulier de ces objets.

L'idée essentielle est qu'un objet (physique, symbolique) peut-être décrit par un ensemble de propriétés, d'attributs observables et en ce sens, mesurables. Des relations peuvent exister entre les *p* variables précitées dont une synthèse factorielle de rang *k* est possible, exprimant des entitées latentes interprétables. Mathématiquement parlant, le développement accru des travaux de recherche autour de ces méthodes de calculs repose, entre autre, sur des questions comme le caractère unique ou multiple des facteurs / concepts sous jacents que l'on souhaite utiliser pour résumer les données, ou bien encore sur les modalités de calculs, liéaires ou non, des combinaisons factorielles souhaitées.[@fodor2002a]

L'idée principale est de trouver *k* combinaisons linéaires des *p* variables qui capturent succéssivement une part maximale de la variance, ce qui minimise réciproquement l'écart total d'erreurs, d'un effectif *n* d'observations, tel que *k* \<\< *p.*

La matrice carrée symétrique de covariance *K* est obtenue pour un vecteur *v* normé (\|\|v\|\| *= v*⊤*v=* 1) tel que *v*⊤ *Kv* soit maximal. Cela se traduit par la production une à une de p matrices *A* de corrélations de Pearson de dimensions nixnj, sur lesquelles on applique la méthode des multiplicateurs de Lagrange *λ* afin de maximiser la variance de chaque matrice Ap. La variance en est la somme pondérée, et sa valeur est reprise dans la cellule Knp de notre matrice de covariance. La diagonale contient alors l'ensemble des valeurs *λp,* qui ne sont autres que les bases des eigeinvectors, qui serviront de base à l'orthogonalité souhaitée de la projection de la ki dimensions par rapport à la ki-1.

Cette approche est un cas particulier d'un théorème d'algèbre liénaire qu'est le théorème spectral. Pour un ensemble de cas où les solutions sont impossibles, ou restent indéterminées, les matrices cibles ne peuvent être qu'approximatives, ou estimées (nature convexe ou non convexe des interfaces mathématiques utilisées)[@delporte2014]. Ces dernières sont alors plus désignées sous le symbole S. Dans le cas d'énonication du théorème, cette matrice S est égale à toute matrice A, multipliée par sa diagonale D et son inverse A-1, tel que S= ADA-1. Pour autant, la plupart des matrices que l'on peut trouver aujourd'hui ne sont pas carrées, et leur structure ne retrace pas une sédimentation et répétition d'un même phénomène de réponse à un questionnaire. En ce sens, pour un tableau d'occurrences de mots, une décomposition selon une généralisation du théorème spectral que l'on appelle Décomposition par Valeurs Singulières au matrice rectangulaires (SVD)

α

$\Sigma$.

### Données

Nous avons scrappé via le package rtweets un ensemble de 4 marqueurs : javascript, Cobol, Python, Java, selon les méthodologies robustes de construction de corpus déjà éprouvées[@balech2019]

> On peut supposer a priori que toute chose étant égale par elle même, les distinctions et structurations de la communication autour des langages de programmation soient similaires et uniformes, dans une première proposition relevant du marketing des technologies N/TIC.[@benavent]

Dans un premier temps, nous allons chercher les fichiers cibles que l'on charge dans une liste pour en automatiser l'action de lecture et d'aggrégation des bases de données les unes avec les autres. Chaque extraction de la plateforme Twitter rapporte un ensemble de n tweets, contenant 91 variables, propres à la caractérisation de ces derniers. L'on retrouve des propriétés comme la localisation, la durée de vie, les nombres de posts, abonnées, d'abonnements, nom de compte... 

C'est à cette étape que la fréquence de publication sert à filtrer manuellement et qualitativement les internautes afin d'auditer les comptes un à un.

```{r workingstorage, include=FALSE}

###ACTIVATION DES LIBRAIRIES


vect_exclude <- c("HdgapLD0AhlZiC2","Nvo1tQbwYQw8Msd","1SAq6KyxToDo4gA","HdgapLD0AhlZiC2","N8fCs","Mastersakthi007","_Java_Speaks","jiiva__","stepseven7","Java_jigga","java_star_","TheFieryBot","Udemy_Coupons1","Java_curry","ItforceM","Empleo_STANDBY","eworkDE","Java_Computer","cobol_g","Idealtutor2020","Master_Krishna2","Msd4Vj","TutorsPotential","helloworld_java","python_lady","investortat","AndromedaZevs","LegitEssayHelp","laxmanudt","holo_Python","ForPlaut","writerbayinc","jun50_python","lufa_presents","TeamPatriot007","hika_java","PYTHON_bot","sax_Java_sparow","java_capricorn","uhiiman","hjFGlLb8yjtrGNg","nestplus2","Vj__Anna__Blood","javascript_bot","Newt_python","profession_meme","chinryu","earaspi","JAVA__O","kyota_python","pihole_fosser","YonZeroBot","cobol_amaki","overflow_meme","python_72","Frankwe78123268","python_octopus","H_cobol_bot","ScientistBoo","DropoffInstant","Java_Meal2","java_mp","MooseMagnificat","Cill_Java","java_aaaaa","iampaamaran","moikrug","Aquaa_man1","uhiiman","kimuraan_python","kome_the_python","ho__01923","yst_java","yasai_sando","qiita_python","nonbiri_life7","SLJ_COBOL_SEKI","hkdtt698_d","yukuri_cana","GPW_mame","neco_engineer","ojisannm","16thnoteheartBt","C90751889","sa1o_kuma","rairaiv","bonaponta","cobol_07","LoremipsumBot","programmerjobs","ROkachimachi","rokuta_tech","11011_11010","ic0noclazm_","iH7YRwkLJVtfr0v","Kuro8Aym","kyu190a","nuclear_map","pontacyan7777","PRiMENON","sakama98","sake_engineer")
vect_exclu <-  as.data.frame(vect_exclude)
names(vect_exclude)[names(vect_exclude) == 'vect_exclude'] <- 'vect_include'
```

```{r reader}
###LECTURE DES FICHIERS
data <- list.files("C:/Users/jmonn/AppData/Local/Temp/Temp1_NLPBook-main.zip/NLPBook-main/NLPBook",pattern = "Exemple")
list_df <- lapply(data, function(x) readRDS(x))
df_brut <- bind_rows(list_df)
###

###ELIMINATION DOUBLONS
doublons <- which(duplicated(df_brut$status_id))
t_net_tex <- df_brut[-doublons,]
###

names(t_net_tex)[names(t_net_tex) == 'screen_name'] <- 'vect_exclude'
t_net_tex <- t_net_tex %>% anti_join(vect_exclu,by="vect_exclude")

names(t_net_tex)[names(t_net_tex) == 'vect_exclude'] <- 'screen_name'
```

Après cette étape de constitution, il est nécessaire de supprimer les valeurs et lignes en double. Cette opération permet d'obtenir les tweets dits "primaires" sur une période donnée allant du 30 juillet 2021 au 7 août 2021.De premiers calculs permettent d'observer que l'hypothèse précédente d'homogénéïté de la structure de communication sur Twitter autour de nos quatre langages de programmation n'est pas satisfaisante.

```{r descripter}

###CALCULS PRELIMINAIRES
#Pourcentages
t_1a <- table(t_net_tex$Marque)
t_net_tex <- t_net_tex%>%filter(is_retweet=="FALSE")
t_1b <- table(t_net_tex$Marque)
t_1perc <- (t_1b/t_1a)*100
t_1perc <- t(t_1perc)

#Dates et maturité du compte
t_net_tex$created_at <- date(t_net_tex$created_at)
t_net_tex$account_created_at <- date(t_net_tex$account_created_at)
t_net_tex$DureeV <- ymd(t_net_tex$created_at)-ymd(t_net_tex$account_created_at)
t_net_tex$DureeV <- as.integer(t_net_tex$DureeV)

#Sous ensemble pour visualisation
t <- t_net_tex%>%select(screen_name,Marque)%>%group_by(Marque)%>%count(screen_name)%>%arrange(desc(n))


ttab <- t %>%mutate(interv = cut(n, breaks=c(1,5,20,50,100,10000),labels = c("<5","<20","<50","100","+")))
t_tab <- table(ttab$Marque,ttab$interv)

library(vcd)

mosaic(~ interv + Marque, data = ttab,shade=T,legend=T, main = "Production des langages")

```

La répartition des tweets primaires pour chaque langage varie de 70% à 15%. Cette première observation peut justifier que l'on souhaite étudier plus en détails le rôles de certaines variables dans les effets de diffusion. La mosaïque nous permet également de voir que les effectifs ne sont pas uninamiment distribués. Nous déduisons la maturité du compte (DureeV) par la soustraction de la date démission du tweet récolté à la création du compte.Une première visualiation filtrée, permet de voir la répartition du nombre d'internautes produisant plus de 10 tweets sur la semaine d'étude, pour chaque langage.

Une première idée est que les communautés et les modes d'apparitions de ces langages étudiés sur Twitter semblent, sur l'échantillon donné, singuliers. Nous proposons de créer un sous ensemble de la base de données propres à chaque formats plébiscités.

```{r descri_plotter}
#Visualisation
t <- t_net_tex%>%select(screen_name,Marque)%>%group_by(Marque)%>%count(screen_name)%>%arrange(desc(n))
ty <- t_net_tex%>%select(screen_name,Marque)%>%group_by(Marque)%>%count(screen_name)%>%arrange(desc(n))%>%filter(n>3)

ty%>%ggplot(aes(n,reorder(screen_name,n)),size=n)+geom_point() + facet_wrap(~Marque) +scale_x_log10() + theme(axis.text.y.left = element_blank())
###
 
###EDITIONS DES FICHIERS
kable(t_1perc)
###
```

### Analyse par Composantes Principales (ACP/PCA)

L'ACP et ses dérivées appliquées à des données de comptage comme l'AFCM sous l'influence de J. Benzecri, ont longtemps étés les méthodes "reines", et restent aujourd'hui, toujours les plus fréquemmment utilisées. Elle vise à un but simple : représenter un ensemble de données comportant *p* variables, à un plus petit nombre entier *k*, tel que *k* combinaisons de ces variables représentent une grandes part de la variance de l'information exprimée dans la base de données. L'algortihme cherche alors *k* combinaisons linéaires possibles d'attributs, sans que ceux-ci soient trop génériques ou extrêmes, d'un ensemble de données à résumer. L'expression mathématique du résultat représente les *k* concepts supposés.

En admettant que les combinaisons linéaires passent par le centre *O* d'un nuage de points de coordonnées (Ox'' Oy''), la variance en chaque point *W* (Wx Wy) présent dans l'échantillon et différente de *O* est la distance au carré de la projection de chaque point W' Wx' Wy' à Ox" Oy". On peut donc appliquer ce calcul pour *k* dimensions propres au nombre de combinaisons ou facteurs exigés de manière plus ou moins supervisée. La part totale de variance correspond à la moyenne de ces variances locales et est ajustée par l'analyse factorielle afin d'être maximisée selon le procédé détaillé ci-dessus.

L'atteinte de ce premier objectif calculatoire de maximisation de la variance permet conséquemment de trouver l'écart d'erreur minimal. Ce dernier est la moyenne du carré des distances entre les points observés Wn et théoriques W'n sur la kn combinaison linéaire.

Le phénomène de prise de masse en fonction de l'âge permet d'admettre, selon l'espèce animale étudiée, un facteur (naturel) de "croissance (naturelle)".[@lemoine1981] D'autres exemples se concentrent sur des données de natures mixtes [@newhouse] sinon plus largement issues des sciences sociales, [@blanchard] ou bien encore pour des cas concrets de besoins d'évaluations d'élèves [@vallet1985]

Historiquement elle a été développée pour analyser des matrices de corrélations multiples où *X* est une matrice de *n* individus et *p* variables. Dans le domaine de l'analyse d'éléments textuels ce tableau correspond au *document-term-matrix* (dtm) et *document-feature-matrix* (dfm), où les "individus" sont alors considérés en tant que *documents* (tweets, reviews, litteratures...) auxquels nous rattachons en colonne, les termes, afin d'en compter chaque occurence.

Notre travail ici est donc de chercher à savoir si les paramètres structurels de chaque compte, traduisent une éventuelle présence différenciée. On sélectionne un sous ensemble de données quantitatives et l'on calcule volontairement ici deux nouvelles variables dépendantes : le volume total de caractères produit sur la plateforme pour un internaute ainsi que le dénombrement de ses tweets. Chaque tweet étant borné à 280 caractères, ces deux variables sont donc liées par un certain effet naturel de "lotissement", ou croissance par pallier. Contraint et limité par la puissance de calculs et ne pouvant donc faire une ACP sur 50 825 lignes, on filtre l'échantillon sur les tweets ayant rencontrés plus d'un retweet, et exigeons que le modèle soit factorisé en 3 dimensions (k=3). La population étudiée n'est alors plus que composée de 6475 individus et 7 variables quantitatves.

```{r pca}
memory.limit(15000000)
df_pca <- select(t_net_tex,Marque,screen_name,status_id,DureeV,retweet_count,followers_count,friends_count,statuses_count,display_text_width)
df_pca$score <- 1
df_pca <- df_pca%>%group_by(Marque,screen_name)%>%summarise(DureeV,followers_count,friends_count,statuses_count,Totalcr=sum(display_text_width), Totaltwt=sum(score),retweet_count)

###ELIMINATION DOUBLONS
doublons <- which(duplicated(df_pca$screen_name))
df_pca_u <- df_pca[-doublons,]

###MODEL PCA
df_pca_u%>%select(followers_count,friends_count,statuses_count,Totalcr,Totaltwt,DureeV,retweet_count)%>%head(20)
df_pca_uf <- df_pca_u%>%filter(retweet_count>1)
res.pca <- PCA(df_pca_uf,quali.sup = 1:2,scale.unit = TRUE, graph = FALSE,ncp =3)

var <- get_pca_var(res.pca)

set.seed (123)
my.cont.var <- rnorm (7)
# Colorer les variables en fonction de la variable continue
fviz_pca_var(res.pca, col.var = my.cont.var,
             gradient.cols = c("red", "Pink", "Purple"),
             legend.title = "Cont.Var")

fviz_eig(res.pca, 
         addlabels = TRUE, 
         ylim = c(0, 40))

```

Les eigenvectors et eigenvalues sont remarquables dans les cos2 et contributions de nos 7 variables au 3 dimensions souhaitées respectivement. Les deux premières dimensions expriment près de 50% de la variance et les variables naturelles issues de la plateforme sont distinctes de nos deux totaux, mais évoluent avec la maturitée (DureeV) du compte. L'effet de lotissement, ou de croissance par pallier exprimée précedemment s'observe ici dans le sens où ces propriétés peuvent être exprimée en une seule.

```{r pca_plotter}
par(mfrow=c(1,2))
corrplot(var$cos2, is.corr=FALSE)
corrplot(var$contrib, is.corr=FALSE) 

```

Ces deux matrices expriment différemment le graphique des dimensions précédentes. Si l'on peut raisonner en terme d'équivalence, la direction des flèches est propre à la couleur des cercles, la "taille" d'un vecteur est proportionnelle à la "taille" des points de corrélation ci-dessus, pour chacune des dimensions 1 et 2. Il est intéressant maintenant de pouvoir proposer un graphique des individus, tout en utilisant les étiquettes qualitatives afin de décrire la composition du nuage de points.

```{r pca_plotter_2}

fviz_pca_ind(res.pca,
             geom.ind = "point", # Montre les points seulement (mais pas le "text")
             col.ind = df_pca_uf$Marque, # colorer by groups
             palette = c("#00AFBB", "#E7B800", "#FC4E07","#AA4E07"),
             addEllipses = TRUE, # Ellipses de concentration
             legend.title = "Groups"
             )
```

De prime abord, nous remarquons que les ellipses de confiance se chevauchent et sont toutes centrées sur l'origine du repère. Pour autant, les facteurs exposés ici semblent influer sur la ventilation de la distribution des comptes des internautes.

Pour une analyse détaillée et les détails du code, se réferrer à [Kassambora](http://www.sthda.com/french/articles/38-methodes-des-composantes-principales-dans-r-guide-pratique/73-acp-analyse-en-composantes-principales-avec-r-l-essentiel/ "For more details"). Le dernier graphique, mélange des données qui ne sont pas traitables directement de manière simultanée par l'ACP, qui fonctionne sur des données exclusivement quantitatives. (La variable "Marque" n'était pas présente sur le graphique des eigeinvectors) Pour ce faire, on propose l'utilisation d'un algorithme factoriel admettant en entrées un fichier de données mixtes, afin de pouvoir pleinement voir les influences d'une variable qualitative propre au langage de programmation (Marque) du tableau de données ci-dessus. [@pages]

### Analyse Factorielle sur Données Mixtes (AFDM/FAMD)

Le tableau d'entrées est le même que celui de l'ACP, auquel nous rajoutons la variable des Marques.

```{r fadm}

df_pca_uf <- df_pca_uf%>%ungroup()
df_famd<- df_pca_uf%>%select(Marque,DureeV,retweet_count,followers_count,friends_count,statuses_count,Totalcr,Totaltwt)

res.famd <- FAMD(df_famd,graph = F, ncp = 3)
fviz_screeplot(res.famd)

```

Les trois premières dimensions groupes près de 40 % de la variance totale de l'échantillon.

```{r fadm_ed}
# Contribution à la première dimension
g1 <- fviz_contrib (res.famd, "var", axes = 1)
# Contribution à la deuxième dimension
g2 <- fviz_contrib (res.famd, "var", axes = 2)
#Contribution à la troisième dimension
g3 <- fviz_contrib (res.famd, "var", axes = 3)
plot_grid(g1,g2,g3,ncol=3,nrow = 1)
```

Leur description est renseignée pour chacune des dimensions, propre à l'activité mesurée sur le set de données, la maturité du compte, et sa marque. Nous présentons par la suite les résultats principaux d'une AFDM.

```{r afdm_plotter}

fviz_famd_var(res.famd, "quanti.var", col.var = "contrib", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)

fviz_famd_var(res.famd, "quali.var", col.var = "contrib", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
             )


fviz_famd_ind(res.famd, col.ind = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = T)
```

On peut donc obtenir sur le même modèle que précedemment, les eigen composants des variables quantitatives et qualitatives. La visualisation est similaire à celle obtenue par l'ACP, mais l'on peut faire de même avec la visualisation des modalités de notre variable qualitative, où l'on peut observer un positionnement légèrement différents en terme de contribution aux dimensions.

