# du SVD au NMF


```{r lsa} 
#packages pour le chapitre
library(dplyr)
library(ggplot2)
library(knitr)
library(lubridate)
library(FactoMineR)
library(factoextra)
library("corrplot")
library(ade4)
library(ExPosition)
library(syuzhet)
library(udpipe)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.textstats)
library(Biobase)
library(BiocManager)
library(cowplot)
library(NMF)
```


### SVD


### Analyse Sémantique Latente (ASL/LSA)

L'analyse sémantique latente s'est développée sous le travail des linguistiques, et la généralisation d'outils tels que les moteurs de recherche. [@evangelopoulos2012] Le retour sur histoire que nous fait l'auteur, ainsi que sa description des différents principes de calculs et vigilances pour l'analyste à garder en mémoire, renseigne sur les heuristiques de cette méthode : Son principe fondateur est de réussir à retrouver dans une collection de documents, un ensemble de patterns présents ou absents, dans un système d'équations s'exprimant chacune en fonction des autres. Cette expression réciproque s'opère à l'aide de la décomposittion spectrale de chacun de ses mots. En ce sens, elle peut se comprendre comme un résultat de la recherche allant dans la quantisation des vecteurs, la régression multinomiale, ou bien encore la topologie, comme identifiée par Fodor en 2002, ou d'autres applications croisées. [@song2009]

Le document d'entrée pour la LSA est un VSM, équivalent à la dfm énoncée ci-dessus, appelée A. Au sens de la théorie spectrale (SVD) on obtient :

A = UEVt

avec U = , V =, et E la matrice diagonale, *Vt* la transposée.

Nous nous intéressons donc à la décomposition du tableau précédent. Nous intégrons donc a notre analyse sur les variables, les éléments textuels. Nous supprimons les lemmes propres au entitées nommées étudiées : Cobol & Javascript.

```{r annot, include=FALSE}
vect_exclude <- c("HdgapLD0AhlZiC2","Nvo1tQbwYQw8Msd","1SAq6KyxToDo4gA","HdgapLD0AhlZiC2","N8fCs","Mastersakthi007","_Java_Speaks","jiiva__","stepseven7","Java_jigga","java_star_","TheFieryBot","Udemy_Coupons1","Java_curry","ItforceM","Empleo_STANDBY","eworkDE","Java_Computer","cobol_g","Idealtutor2020","Master_Krishna2","Msd4Vj","TutorsPotential","helloworld_java","python_lady","investortat","AndromedaZevs","LegitEssayHelp","laxmanudt","holo_Python","ForPlaut","writerbayinc","jun50_python","lufa_presents","TeamPatriot007","hika_java","PYTHON_bot","sax_Java_sparow","java_capricorn","uhiiman","hjFGlLb8yjtrGNg","nestplus2","Vj__Anna__Blood","javascript_bot","Newt_python","profession_meme","chinryu","earaspi","JAVA__O","kyota_python","pihole_fosser","YonZeroBot","cobol_amaki","overflow_meme","python_72","Frankwe78123268","python_octopus","H_cobol_bot","ScientistBoo","DropoffInstant","Java_Meal2","java_mp","MooseMagnificat","Cill_Java","java_aaaaa","iampaamaran","moikrug","Aquaa_man1","uhiiman","kimuraan_python","kome_the_python","ho__01923","yst_java","yasai_sando","qiita_python","nonbiri_life7","SLJ_COBOL_SEKI","hkdtt698_d","yukuri_cana","GPW_mame","neco_engineer","ojisannm","16thnoteheartBt","C90751889","sa1o_kuma","rairaiv","bonaponta","cobol_07","LoremipsumBot","programmerjobs","ROkachimachi","rokuta_tech","11011_11010","ic0noclazm_","iH7YRwkLJVtfr0v","Kuro8Aym","kyu190a","nuclear_map","pontacyan7777","PRiMENON","sakama98","sake_engineer")
vect_exclu <-  as.data.frame(vect_exclude)
names(vect_exclude)[names(vect_exclude) == 'vect_exclude'] <- 'vect_include'


df_trt_annot <- readRDS("Annotation_NMF_LSA.rds")
df_trt_annot <- df_trt_annot%>%filter(lemma!="Cobol")
df_trt_annot <- df_trt_annot%>%filter(lemma!="javascript")

names(df_trt_annot)[names(df_trt_annot) == 'screen_name'] <- 'vect_exclude'
df_trt_annot <- df_trt_annot %>% anti_join(vect_exclu,by="vect_exclude")
names(df_trt_annot)[names(df_trt_annot) == 'vect_exclude'] <- 'screen_name'
```

Pour les détails et l'implémentations du code, se réferrer à [C. Benavent.](http://r.benavent.fr/text.html)

```{r lsa}
library(udpipe)
library(quanteda)
library(quanteda.textmodels)
library(stringr)
vect_upos <- c("ADJ","NOUN","VERB")
annot_udd <- df_trt_annot%>%filter(upos%in%vect_upos)

annot_udd <- annot_udd%>%select(Marque,lemma)%>%filter(Marque=="Cobol")
annot_udd <- annot_udd%>%mutate(Off_Acc=ifelse(str_detect(lemma,"@.")==TRUE,"TRUE",
                                        ifelse(str_detect(lemma,"https.")==TRUE,"TRUE",
                                                          "FALSE")))
annot_udd <- annot_udd%>%filter(Off_Acc=="FALSE")

dfm_lsa <- dfm(annot_udd$lemma,
             tolower = TRUE,
             what = "word",
             docvars = "Marque")
mylsa <- textmodel_lsa(dfm_lsa,2)

proxD<-mylsa$docs[, 1:2]

library(Rtsne)
#rtsne
tsne_out <- Rtsne(proxD, dims = 2, initial_dims = 100,
  perplexity = 20, theta = 0.5, check_duplicates = FALSE,
  pca = TRUE, max_iter = 300)

x<-tsne_out$Y
x<-as.data.frame(x)
x$F1<-x[,1]
x$F2<-x[,2]
x<-cbind(annot_udd,x)

df<-subset(x) #pour un quartier
library(ggrepel)
lsacob <- ggplot(df,aes(x = F1,y=F2))+geom_point()

prox<-mylsa$features[, 1:2]
terms<-as.data.frame(prox)

#rtsne
tsne_out <- Rtsne(terms, dims = 2, initial_dims = 50,
  perplexity = 20, theta = 0.5, check_duplicates = FALSE,
  pca = TRUE, max_iter = 300)


plot(tsne_out$Y)

x<-tsne_out$Y

terms$term<-row.names(prox)
plot<-cbind(x,terms)
plot$F1<-plot[,1]
plot$F2<-plot[,2]

gglsacob <- ggplot(plot,aes(x = F1,y=F2))+geom_point()+geom_text(aes(label=term),hjust=0.5, vjust=0.5, size=2.5)

###
###

annot_udd <- df_trt_annot%>%filter(upos%in%vect_upos)

annot_udd <- annot_udd%>%select(Marque,lemma)%>%filter(Marque=="javascript")
annot_udd <- annot_udd%>%mutate(Off_Acc=ifelse(str_detect(lemma,"@.")==TRUE,"TRUE",
                                        ifelse(str_detect(lemma,"https.")==TRUE,"TRUE",
                                                          "FALSE")))
annot_udd <- annot_udd%>%filter(Off_Acc=="FALSE")

dfm_lsa <- dfm(annot_udd$lemma,
             tolower = TRUE,
             what = "word",
             docvars = "Marque")
dfm_lsa <-
  dfm_trim(
    dfm_lsa,
min_termfreq = 10)

mylsa <- textmodel_lsa(dfm_lsa,2)

proxD<-mylsa$docs[, 1:2]

library(Rtsne)
#rtsne
tsne_out <- Rtsne(proxD, dims = 2, initial_dims = 100,
  perplexity = 20, theta = 0.5, check_duplicates = FALSE,
  pca = TRUE, max_iter = 300)

x<-tsne_out$Y
x<-as.data.frame(x)
x$F1<-x[,1]
x$F2<-x[,2]
x<-cbind(annot_udd,x)

df<-subset(x) #pour un quartier
library(ggrepel)
lsajav <- ggplot(df,aes(x = F1,y=F2))+geom_point()

prox<-mylsa$features[, 1:2]
terms<-as.data.frame(prox)

#rtsne
tsne_out <- Rtsne(terms, dims = 2, initial_dims = 50,
  perplexity = 20, theta = 0.5, check_duplicates = FALSE,
  pca = TRUE, max_iter = 300)

plot(tsne_out$Y)

x<-tsne_out$Y

terms$term<-row.names(prox)
plot<-cbind(x,terms)
plot$F1<-plot[,1]
plot$F2<-plot[,2]

gglsajav <- ggplot(plot,aes(x = F1,y=F2))+geom_point()+geom_text(aes(label=term),hjust=0, vjust=0, size=3)

```

```{r lsaed, include=TRUE}
lsacob
gglsacob

lsajav
gglsajav
```

Les différentes visualisations produites montrent que les langages baignent, outre certains phénomènes de pollution, dans un vocabulaire emprunt à leurs domaines technico-fonctionnels propres. On observe la présence des domaines du Web en ce qui concerne "JS" et de la gestion financière des grands groupes pour ce qui concerne le vieil ami Cobol. En outre, les mots associés à ce dernier sont dans une thématique métier plus professionnelle, tant d'un point de vue de l'ingénierie informatique que financière. L'utilisation de javascript semble plus orientée vers son environnement informatique digital, web et outils propres (frameworks et apparentés) plus qu'au domaine fonctionnel sur lesquels il est potentiellement appliqué. Cela peut également expliquer en partie les différentes populations pressenties : évoquer "Cobol" nécessite de s'intéresser à l'informatique, ce langage est inconnu pour bon nombre d'informaticiens débutants et invisible du grand public, en revanche, plein de tutoriels et de parcours de formations proposent des travaux pratiques, qui permettent aux apprennants de développer un robot à l'architecture plus ou moins complexe, dont la durée de vie se limite pour les plus élémentaires qu'à un script javascript.

Cette première analyse sémantique qualifie différemment les comptes étudiés et complète l'analyse des premiers indicateurs soumis à l'ACP/AFDM par une restitution de leurs univers sémantiques. Se concentrant sur les éléments destructurés textuels, cet algortihme est capable de recomposer une structure vectorielle à partir d'estimations algébriques itératives empruntant aux règles de calculs matriciels précedemment énoncées. Réussir à croiser les différents éléments structurels et sémantiques, permet de qualifier un peu mieux la population d'internautes, et de rattacher des éléments d'informations librements exprimés, à des éléments plus latents.

Ici, un modèle simple, exploratoire et pédagoqiue de LSA a été mené. Cependant, difféfentes variantes, plus ou moins probabilistes ou supervisées existent, et leurs spécificités permet de comprendre les équivalences et traits communs entre les divers algortihmes retenus ici pour l'étude. [@gaussier2005][@ding2005]

### Non-negative Matrix Factorization

La branche mathématique appliquée au calcul matriciel développe depuis longtemps un important travail de recherche sur les propriétés mathématiques de cet objet, [@chen1984] dont les problèmes se classifient selon une échelle de difficulté NP. [@vavasis2007] Plusieurs questions ont été abordées, notamment sur l'interprétabilité des facteurs [@lee], l'ajustement de certains paramètres comme la co-entropie, [@li2014] la détection du nombre de composantes, [@shitov2017] ou encore la divergence *B.* [@votte]

Certaines propriétés des matrices offrent en effet des commodités opérationnelles dans la construction d'architectures techniques computationnelles diverses, notamment dans les domaines du Machine et Deep Learning, mais génère dans un usage statistique scientifique nécessairement de l'incertitude sur l'unicité du résultat obtenu. [@campbell1981]

Un important travail de recherche faisant l'objet d'une thèse en 2014 documente historiquement et techniquement les diverses approches de factorisation matricielle et plus précisément, celles appliquées aux matrices de données non-négatives dont l'hypothèse méthodologique de calculs admettent "la positivité des composantes" ou se basent sur leur "non-négativité". [@limem] Aujourd'hui, la généralisation de son utilisation est possible pour les utilisateurs de différents langages de programmation,[@gaujoux2010] et plusieurs travaux témoignent de ses applications courantes à divers secteurs : l'analyse d'image, le text mining, [@zurada2013] l'analyse spectrophotométrique, [@gillis2014] ou bien encore l'analyse financière,[@cazalet] et la détection des fake news. [@shu2019]

Ici, le premier pas de l'analyse est de créer une matrice A des termes et des documents. Le système considéré est donc de *m* langages exprimés via *y* tweets par *t* mots. On peut donc définir un ensemble *A* ∈ R mt de mots relatifs à ces dernières qu'il va falloir factoriser via cet algorithme NMF, en *k* dimensions, pour générer deux matrices *H* ∈ R tket *W* ∈ R mk représentant l'association aux dimensions du vocabulaire et des marques respectivement. Le regroupement de ce tableau depuis notre base d'étude, appelé "Document-feature-matrix" est un ensemble de features (Mots) associés à chaque groupe (Marques) qui permet de fournir un fichier d'entrée à l'algorithme d'implémentation de l'interface NMF, soit une matrice *m**t.* Notre modèle est dit simple approché, non informé ou pondéré.

Le fonctionnement de cet algorithme de factorisation est de décomposer le fichier d'entrée A en deux matrices, W, tel que *W= u x k* et H, tel que *H = k x v* ou k est ici déterminé à l'aide d'une fonction intégrée au package R "NMF", et déterminé selon le corpus fourni à *k* = 15 dimensions. Lorsque cet algorithme opère une coefficientisation par approximation de la décomposition du fichier A, en deux fichiers W & H de rang *k*, il nous permet de réaliser le produit matriciel de ces deux fichiers de sorties, et donc de proposer une réorganisation du fichier A selon *k*facteurs sous-jacents, et d'ainsi obtenir le rang de factorisation non-négative. De manière naturelle, il serait intéressant de savoir si l'on peut observer un modèle théorique de vocabulaires spécifiques dans la réorganisation finale obtenue.Ici donc, la matrice d'entrée A est composée des 4 langages précités, et de 3641 mots retenus pour l'étude, composer des verbes, noms et adjectifs.

```{r nmf, include=FALSE}
annot_udd <- df_trt_annot%>%filter(upos%in%vect_upos)

annot_udd <- annot_udd%>%select(Marque,lemma)
annot_udd <- annot_udd%>%mutate(Off_Acc=ifelse(str_detect(lemma,"@.")==TRUE,"TRUE",
                                        ifelse(str_detect(lemma,"https.")==TRUE,"TRUE",
                                                          "FALSE")))
annot_udd <- annot_udd%>%filter(Off_Acc=="FALSE")

mydfm_k <- dfm(annot_udd$lemma,
             tolower = TRUE,
             what = "word",
             groups = annot_udd$Marque)

mydfm.un.trim <-
  dfm_trim(
    mydfm_k,
min_termfreq = 30)

df_txt <- mydfm.un.trim %>% 
  convert(to = "matrix",docid_field=TRUE)

#detach("package:NMF", unload = TRUE)
my.nmf <- nmf(df_txt,15,nrun=3,.opt="vp3",seed = 1253)
#plot(my.nmf)

w <- my.nmf@fit@W
h <- my.nmf@fit@H
res <- as.data.frame(w%*%h)
#res$Marque <- rownames(res)

t_ <- as.data.frame(t(res))
t_ <- t_[1:nrow(t_),]
tcob<-t_%>%
  select(Cobol)
tcob <- tcob%>%
  arrange(desc(Cobol))%>%head(50)
tcob$Cobol <- rownames(tcob)

tjas<-t_%>%select(javascript)
tjas <- tjas%>%
  arrange(desc(javascript))%>%head(50)
tcob$javascript <- rownames(tjas)

rownames(tcob) <- NULL

```

Afin de visualiser les résultats produits, nous utilisons des fonctions de type heatmap, adaptées à la représentations de matrices.

```{r nmf_ed,include=TRUE}
aheatmap(my.nmf@fit@W)
aheatmap(my.nmf@fit@H)
```

Les représentations graphiques des matrices W et H permettent d'observer la distributivité des mots selon les facteurs (H) ou les variables (W), le gradient de coloration étant proportionnel à la covariance des éléments. [@gaujoux] En réalisant le produit matriciel de ces deux tableaux il est possible, d'obtenir la liste des mots les plus typiques de la modalité de variable étudiée, ici le langage de programmation. Les matrices de coefficients représentent des résultats intermédiaires, permettant de cenraliser dans un tableau de données l'ensemble des expressions linéaires possibles d'un système en fonction de ses paramètres et positions. Le consensus, s'obtient suites aux différentes itérations demandées, ici fixées à 30.

```{r nmf__ed}
basismap(my.nmf)
coefmap(my.nmf,Colv="basis")
consensusmap(my.nmf,Rowv=TRUE, Colv=TRUE, scale="none")
tcob
```

## Conclusions

Ces techniques se sont développées en se confrontant aux différentes structures de données et développements des courants de recherche en mathématiques. Leur existence n'empêche pas leur utilisation conjointe à des fins de classifications, ou réduction de dimensions. [@hassani2020]

Part ailleurs, d'autres modèles existent et permettre de comprendre les différentes équivalences entre les méthodes décrites ci-dessus. [@buntine2002] En ce sens, le travail de Pochon en 2020 dresse un panorama plus situé, des différentes techniques et de leurs enjeux, tant en termes d'accessibilité, que d'applicabilité. [@pochon]

A vos claviers ?
