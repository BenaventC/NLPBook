---
title: "trump  : les mots qui boostent les retweets"
author: "MIrM2020"
date: "07/11/2020"
output: html_document
bibliography : MIrM.bib
---

Dans cette analyse, l'objectif va être de comprendre quel sont les mots, ou les expressions, qui sont les ressorts de leurs partages.

On va employer caret, qui est un cadre général pour mener des évaluation de machine- learning, et de tidytext pour manipuler le corpus de tweets.

L'objectif va être de prédire à partir des termes, les chances qu'un tweet soit largement retwitté, et de comprendre quels sont les éléments qui contribuent le plus à expliquer la qualité des prédictions.  

le processus va se dérouler selon les étapes suivantes

1 - on constitue les corpus de donnée. Un fichier d'entrainement et un de test. Le fichier d'entraineemet est labellisé, etiqueté, annotés. Ce sont des termes équivalents qui déssignent un processus plus ou moins objectifs qui qualifie une chaine de caractère. Tel tweet est positif, tel autre ne l'est pas, tel tweets proteste, tel autre informe. Telle chaine de caractère est associée à un nombre de retweet élevé, tel autre à un nombre bas. 
2 - on nettoie le texte, on limite les signes candidats

inspiré de https://www.hvitfeldt.me/blog/binary-text-classification-with-tidytext-and-caret/



# Initiation

Les packages et les données


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE, message=FALSE,warning=FALSE)
library(tidyverse)
library(tidytext)
library(caret)


df<-readRDS("df_nrcliwc.rds")
```

### constituer le dataset

On prend les données du mandat (après 2016) pour ne pas mettre de confusion et rester dans un domaine homogène. (On peut déjà ennvisager une extension en jouant du temps, et en répétant l'analyse selon une fenêtre de temps - on reste dans le code statique)

On classe les tweet en deux catégories : ceux avec bcps de retweets et sont qui en on moins. On prend le critère de la médiane pour avoir des annotations équilibrées.On envisagera plus tard ue approche en terme de regression, en transformant cett variables par un log. La dichotomisation est ici une première approche, un première approximation.

On choisit de se concentrer sur la période présidentielle pour que le nombre de followers soit à peu près constant ( ce qui est vérifier),


```{r ML1}

#codage des deux classes

data<-df %>% 
  select(text, retweets,year) %>% 
  filter(year>2016)

median<-round(median(data$retweets),1)


data<-data %>% 
  mutate(retweets_cl=ifelse(retweets>median, "yes","no"))

ggplot(data, aes(retweets))+geom_histogram(binwidth = 0.1)+theme_minimal()+labs(title=paste0("médiane = ",median))+scale_x_log10()

data$id<-row.names(data) 

ggplot(data, aes(retweets_cl))+geom_bar()
```

## On va nettoyer le texte

 * supprimer les liens
 * identifier les ngrams
 * éliminer des stopword
 * éliminer les termes utilisés moins de k fois (ce sera 20)
 * on crée un tableau dfm en calculant les tf-idf ( on pondère la préquence d'un terme par l'inverse de la fréquence des documents dans lesquels il apparait)
 *
 
```{r ML2}

#nettoyage des données
##les liens

data_clean <- data %>%
  mutate(top = retweets_cl,
         text = str_replace_all(text, " ?(f|ht)tp(s?)://(.*)[.][a-z]+", "")) %>%
  select(id, top, text)


## les stop_word

data_counts <- map_df(1:3,
                      ~ unnest_tokens(data_clean, word, text, 
                                      token = "ngrams", n = .x)) %>%
  anti_join(stop_words, by = "word") %>%
  count(id, word, sort = TRUE)


## mots assez frequents
words_10 <- data_counts %>%
  group_by(word) %>%
  summarise(n = n()) %>% 
  filter(n >= 30) %>%
  select(word)%>%drop_na()

#we will right-join this to our data.frame before we will calculate the tf_idf and cast it to a document term matrix.

data_dtm <- data_counts %>%
  right_join(words_10, by = "word") %>%
  bind_tf_idf(word, id, n) %>%
  cast_dtm(id, word, tf_idf)


#We create this meta data.frame which acts as a intermediate from our first data set since some tweets might have disappeared completely after the reduction.

meta <- tibble(id = as.character(dimnames(data_dtm)[[1]])) %>%
  left_join(data_clean[!duplicated(data_clean$id), ], by = "id")

```

### caret sur la scène

On commence à employer les fonction de caret qui permettent de distinguer les sets d'entrainement et de test, mais aussi, de tester l'efficacité des combinaision d'hyper paramètre propre à chaque model. Cartet donne accès à plusieurs dizaienes de modèles différents? 

Nous resterons concentré sur un naive bayes qui est le classique de la classification et un simple réseau de neurones. nnet.


les données du processus d'apprentissage. Le corpus est partitionné en un corpus d'entranement et un corpus de texte.Le processus d'apprentissage est controllé par une méthode de crossvalidation en trois blocs.



```{r ML2}

trainIndex <- createDataPartition(meta$top, p = 0.7, list = FALSE, times = 1) #on partitionne
data_df_train <- data_dtm[trainIndex, ] %>% as.matrix() %>% as.data.frame() #on definit le training set
data_df_test <- data_dtm[-trainIndex, ] %>% as.matrix() %>% as.data.frame() #on définit le test set

response_train <- meta$top[trainIndex] #on définit l'annotation

trctrl <- trainControl(method = "cv", 3, classProbs=TRUE, savePredictions = TRUE) # on définit la stratégie d'entrainement. Ici on utilise une méthode de cross validation fondée sur un découpage en 5 échantillons. 
```

### Un naive bayes

Le modèle naive bayes est populaire car simple. Il calcule une probabilité conditionnelle que l'on simplifie en faisant l'hypothèse ( excessive) de l'indépendance des probabilité conditionnelle, ce qui revient à calculer la quantité suivante
p(Positive|w1, w2, w3...)~= p(Positive)*p(w1|positive)*p(w2|positive)*...

On entraine le modèle avec un lissage  de laplace pour éviter le problème des proba nulles qui résulte du fait qu'un mot, peut ne pas appartenir à l'ensemble d'entrainement.(voir)[https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece]

```{r ML3}
nb_mod <- train(x = data_df_train,
                y = as.factor(response_train),
                method = "naive_bayes",
                trControl = trctrl,
                tuneGrid = data.frame(laplace = 1,       
                                      usekernel = FALSE,
                                      adjust = FALSE))
```

Examinons les résultats. Mais d'abord un rappel des indicateurs clés. 


La prediction apparait dans le tableau croisés de la prédiction et de la réalité.Pour être concret pensons aux tests covid. Ils peuvent peuvent rendre compte de la présence du virus à raison, ce sont les vrais positifs et symétriquement rendre compte de l'absence de maladie : les vrais négatifs. Ce sont les prédictions correctes. Les prédictions incorrecte sont de deux types : dans le premier on annonce une contamination alors qu'il n'y en a pas : ce sont les faux positifs. Ils alertent sans raison. Leur presque symétrique sont les faux négatifs : ils annoncent que le virus est absent alors qu'il est tapi dans nos cellules. Le presque symétrique vient que les deux erreurs n'ont pas le même poids : la première génère de fausses peurs, la seconde peut conduire à la mort. (ce n'est pas toujours le cas)


Realité      faux vrai
Prediction    
faux         TN   FP
vrai         FN   TP

Les indicateurs sont calculé à partir de ce tableau

 * la précision (accuracy) représentent le % de bonne prédictions :  Accuracy=TP+TN/TP+TN+FP+FN
 * le recall ou ( sensiTPR=TP/TP+FN (sensitivity/ recall)

 * kappa : un indice de concordance qui tient compte des accord s du à la chance.
 * leurs erreurs types


TNR=TN/TN+FP (specificity)
PPV=TP/TP+FP (precision/ positive predictive value)
NPV=TN/TN+FN (negative predictive value)
Informedness=TPR+TNR-1
Markedness=PPV+NPV-1


 
Mais la qualité interne n'est pas suffisante, on va donc tester en predisant les valeurs à partir de l'échantillon test. 


```{r ML4}
library(MLeval)
nb_mod$results
nb_pred <- predict(nb_mod,
                   newdata = data_df_test, prob=TRUE)
nb_cm <- confusionMatrix(nb_pred, as.factor(meta[-trainIndex, ]$top))
nb_cm
```


### Passons à un réseau de neurones et un RDF


```{r ML5}

##rdf
trctrl <- trainControl(method = "cv", 3,classProbs=TRUE, savePredictions = TRUE)
model_grid <- expand.grid(
   mtry = 20                                    # mtry specified here
   ,splitrule = "gini"
   ,min.node.size = 20
 )

ranger_mod <- train(x = data_df_train,
                    y = as.factor(response_train),
                    method = "ranger",
                    trControl = trctrl,tuneGrid = model_grid,
                    importance="impurity")
ranger_mod$results
ranger_pred <- predict(ranger_mod,
                     newdata = data_df_test, prob=TRUE)
ranger_cm <- confusionMatrix(ranger_pred, as.factor(meta[-trainIndex, ]$top))
ranger_cm

##nn
trctrl <- trainControl(method = "cv", 3,classProbs=TRUE, savePredictions = TRUE)

nnet_mod <- train(x = data_df_train,
                    y = as.factor(response_train),
                    method = "nnet",
                    trControl = trctrl,
                    tuneGrid = data.frame(size = 2,
                                          decay = 0.001),
                    MaxNWts = 15000, importance="impurity")

nnet_pred <- predict(nnet_mod,
                     newdata = data_df_test, prob=TRUE)
nnet_cm <- confusionMatrix(nnet_pred, as.factor(meta[-trainIndex, ]$top))
nnet_cm
#nnet_mod$finalModel

```

# multilayer

```{r ML7}

trctrl <- trainControl(method = "cv", 3 ,classProbs = TRUE)

neuralGrid <-expand.grid(
  layer1 = 3,
  layer2 = 3,
  layer3 = 0,
  decay=0.0005
)
neural_mod <- train(x=data_df_train,y = response_train,
    method = "mlpWeightDecayML",
    tuneGrid = neuralGrid, # cannot pass parameter hidden directly!!
    trControl = trainControl(method = "none"))
    
neural_pred <- predict(neural_mod,
                     newdata = data_df_test, prob=TRUE)
neural_cm <- confusionMatrix(neural_pred, as.factor(meta[-trainIndex, ]$top))
neural_cm
  
```


### revenons à la comparaison des modèles

```{r ML6}


#comparaison des modèles

mod_results <- rbind(
  nb_cm$overall,
  nnet_cm$overall,
  ranger_cm$overall
  ) %>%
  as.data.frame() %>%
  mutate(model = c("Naive-Bayes", "Neural network","RF"))

mod_results %>%
  ggplot(aes(model, Accuracy)) +
  geom_bar(stat="identity") +
  ylim(0, 1) +
  geom_hline(yintercept = mod_results$AccuracyNull[1],
             color = "red")
```


## Une analyse plus précise de la performance

https://rvprasad.medium.com/informedness-and-markedness-20e3f54d63bc


avec MLeval qui fournit 4 graphes

```{r ML6}


library(plotROC)
library(MLeval)


res <- evalm(list(nnet_mod,nb_mod, ranger_mod),gnames=c('nn','nb', 'rf'))


```

## mais il faut expliquer

Prédire est une chose, encore faut-il pouvoir expliquer. Les spécialistes du machines learning ont développé des outils à cette fin L'un entre eux est vip 

Un principe général est d'examiner comment le modèle se comporte si on enlève la variable, quelques sera l'accroissement de l'erreur. Une variable qui contribue fortement risque d'affecter fortement ce paramètres. En prenant les variables une à une ont peut alors établir leur importance.


```{r ML5}

#explication
library(vip)

vip1<-vip(nnet_mod, num_features = 40, geom = "point", horizontal = TRUE, 
    aesthetics = list(color = "firebrick", shape = 1, size = 3)) +
  theme_minimal()
vip1

vip2<-vip(nb_mod, num_features = 40, geom = "point", horizontal = TRUE, 
    aesthetics = list(color = "firebrick", shape = 1, size = 3)) +
  theme_minimal()

vip3<-vip(ranger_mod, num_features = 40, geom = "point", horizontal = TRUE, 
    aesthetics = list(color = "firebrick", shape = 1, size = 3)) +
  theme_minimal()
vip3
#vip(nnet_mod, method = "firm")

##p4 <- vip(nnet_mod, method = "permute", metric = "auc", pred_wrapper = predict, target = c("abolish"),reference_class = "neg")

```





# Pour finir un exercice de tuning


Un modèle est meilleur, peut il être encore meilleur ? oui si on en controle les hyperparamètres par une grille de leurs valeurs.

On va regarder si on peut améliorer le réseau de neurone en jouant sur le nombre d'unités cachées, et de taux de decay. 

Attention ca peut être time intensive (plus d'une dizaine d'heures pour le code suivant sans parallélisation). On essaye d'accélerer le processus avec le package doParallel qui va répartir les calculs sur 3  des 4 processeurs de la machine  (On en garde un pour l'OS).

```{r ML7}
t1<-Sys.time()
t1
library(doParallel)
cl <- makePSOCKcluster(3)
registerDoParallel(cl)

#tuning

trctrl <- trainControl(method = "cv", 3,classProbs = TRUE)

nnetGrid <-  expand.grid(size = seq(from = 1, to = 4, by = 1),
                        decay = seq(from = 0.003, to = 0.030, by = 0.002))

nnet_mod_t <- train(x=data_df_train,
                   y = response_train,
                    method = "nnet",
                    trControl = trctrl, tuneGrid=nnetGrid, MaxNWts = 16000)
plot(nnet_mod_t)
stopCluster(cl)
t2<-Sys.time()
t2-t1

```
