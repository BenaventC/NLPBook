[["index.html", "NLP avec r et en français - un Manuel synthétique Chapitre 1 Préambule 1.1 Cours et séminaires 1.2 La structure du livre 1.3 Les jeux de données 1.4 Les ressources 1.5 Disponibilité", " NLP avec r et en français - un Manuel synthétique Sophie Balech, Julien Monnot, Christophe Benavent 2021-09-12 Chapitre 1 Préambule 1 Lécosystème r sest enrichi ces dernières années à grande vitesse dans le domaine du traitement du langage naturel. Le but de ce manuel est de donner une synthèse des méthodes et des modèles disponibles à chacune des étapes de traitement des données textuelles : collecte, pré-processing, modélisation, visualisation. Sa vocation est pratique, on sappuie sur des corpus caractéristiques, et les scripts sont totalement reproductible. On y laissera germer quelques considérations plus méthodologiques, voire épistémologiques. On ouvrira chaque fois que cest possible et nécessaire la discussion aux questions théoriques et éthiques de ces méthodes. Leur réalisation computationnelle est le fruit souvent dune longue histoire, au cours de laquelle les linguistes ont semé des idées essentielles quont systématisées les informaticiens. Si ce champs est désormais désigné par lacronyme NLP, pour Natural Langage Processing auquel correspond en français le TAL (Traitement Dutomatique du Langage naturel), lexpression de linguistique computationnelle convient peut-être mieux. On soignera la bibliographie de manière synthétique pour en faire un état de lart essentiel et actualisé. La rédaction de louvrage est mené avec une règle de reproductibilité et de transparence, cest le pourquoi le choix de ce support et des jeux de données associés. Il sera dynamique, modifié à mesure de nos cours, séminaires, ateliers et observations des lecteurs. 1.1 Cours et séminaires La liste des cours et séminaires où il sera présenté et utilisé en 2021-2022. Colloque Marketing digital 2-3 septembre 2021 : NLP : un panorama AFM 1er Décembre 2021 : Ecole Doctorale de Gestion à la Sorbonne - février 2022, cours : NLP pour la recherche en gestion Dauphine master 204 - Cours Text Analytics, Novembre 2021 Master Siren - Dauphine : intoduction aux Datasciences - mai 2022 Lille Master Data Science Ecole Doctorals EOS Nanterre : intoduction aux méthodes NLP 1.2 La structure du livre L analyse NLP peut être analysée comme un processus qui va de la production jusquà la diffusion des analyses. Elle est aussi traversée par des évolutions profondes de méthodes qui ont complexifié au sens formel les modèles initiaux. Ce sont ces techniques que nous introduisons ici. Cependant, lapprentissage automatique appliqué à des milliards de données submerge la statistique. Les modèles de langage distribués par les grands acteurs du gidital, comprennent des dizaines, voir des centaines de milliards de paramètres. On les évoquerons dans les derniers chapîtres. Le plan suit une logique progresse qui va du simple au très compliqué, et de lacquisition des données à leur traitement et leur modélisation. Acquisition des données : directe, api et scrapping Corpus dtm et cooccurence AFC et typologie lannotation syntaxique et lexicale analyse du sentiment et sa généralisation Word embedness Factorial models Topic analysis ML classique deep learning et transformer méthodes génératives : parce que la prochaine étape cest quand on appliquera ces méthode sur la productions textuelles des bots. 1.3 Les jeux de données Au cours du développement, plusieurs cas pratiques - souvent réduits en volume pour rester exemplaires, seront employés. Les données sont partagées dans le répertoire .\\data\\ du repository `` En voici la présentation systématique. Trump Twitter Archive : Lintégralité des tweets de Trump jusquà son banissement le 8 Janvier 2021. Confinement Jour J Citations : un recueil de citations littéraires pour de petits exemples et ponctuer le texte aride dun peu de littérature et de poésie. Trip advisor Polynésie, un extrait dun corpus établi par Pierre Ghewy et Sebastien de lUPF Airbnb : paris 2019, bruxelles 2020 à partir de InsideAirbnb - adding data to the debate Covid : le corpus confinement jour, ainsi que celui de Banda. PMP : titre et abstract de tout les article de la revue Politique et Management Public de 1982 à 2020. Le corpus a été constitué à linititive de Manel Benzarafa avec le soutien de la revue et des ses éditeurs. David Bowie : Les articles des magazines musicaux relatif à ,la sortie des 23 album de lartiste. Constitué par Elias Benavent. Appli de fitness Langages / Julien Lensemble des codes et données sont disponibles dans le repositery NLP Book pour la reproductibilités). Les amendements et améliorations sont souhaitées et attendues. 1.4 Les ressources Ce livre est écrit en Markdown (Allaire et al. 2021) et construit avec le package Bookdown (Xie 2021). Le code sappuie très largement sur tidyverse et emploie largement les ressources de ggplot et de dplyr . On recommande détudier ces deux ressources en parallèle. Une mention particulière doit être faite sur la question du traitement du texte dont stringr est un des outils fondamentaux, même si ardu. Il propose une multitude de fonctions pour traiter les chaînes de caractères. Son étude est recommandée de concert avec la connaissance des regex. Dautres outils transersaux sont utilisés dans le manuel. Pour les tableaux, nous opterons la plus part du temps pour les solutions de flextable. On supposera que les lecteurs ont une connaissance satisfaisante de ces outils génériques, mais à chaque fois quon les introduira, on les expliquera. 1.4.1 Les packages Les data sciences sont devenue un jeu de lego. Les briques de base sont les bibliothèques, des packages qui rassemblent un jeu plus ou moins étendu et cohérent de fonctions. On en utilise dans ce manuel un grand nombre. Les packages seront introduits au fur et à mesure de leur première utilisation. On le signale par un commentaire #library(lib). En voici la liste complète, pour reproduire les exemples il est recommandé des les installer tous. knitr::opts_chunk$set(echo = TRUE, message=FALSE,warning=FALSE) #boite à outils et viz library(tidyverse) # inclut ggplot pour la viz, readr et library(readr) #lecture de fichier csv library(scales) # pour affiner les échelles de ggplot #Vizualisation souvent en développement de ggplot ( inclus dans tidyverse) library(cowplot) #pour créer des graphiques composés library(ggridges) # le joy division touch de ggplot library(pheatmap) #carte de chaleur library(flextable) #mise en forme de tableaux #networks library(igraph) #le standard pour les représentations de réseaux library(ggraph) #un complément compatible ggplot du précédent # Accéder aux données library(rtweet) # une interface efficace pour interroger l&#39;api de Twitter library(revtools) #acces aux bases bibliographiques library(rvest) #pour le scrapping # OCR et pdf library(tesseract) #reconnaissance de caractère library(pdftools) #gérer la lecture des pdf # Analyse de donnée library(FactoMineR) #le classique de l&#39;AF et ses variantes library(factoextra) # NLP library(tokenizers) library(quanteda) library(quanteda.textmodels) library(quanteda.textplots) library(quanteda.textstats) library(udpipe) #annotation syntaxique library(tidytext) #lda glove et autres library(cleanNLP) #annotation syntaxique library(syuzhet) #analyse du sentiment #install.packages(&quot;remotes&quot;) #remotes::install_github(&quot;bmschmidt/wordVectors&quot;) library(wordVectors) library(ade4) library(ExPosition) library(Biobase) library(BiocManager) library(NMF) #non negative factoring ##reconnaissance de langue library(cld3) # c&#39;est google qui gère library(textcat) #ngram based #Analyse du sentiment library(syuzhet) #des sentiments , des émotions #mise en page des tableaux library(flextable) # #Statistiques et modèles library(lme4) library(jtools) library(interactions) library(&quot;corrplot&quot;) #ML library(caret) library(vip) library(plotROC) library(MLeval) #Palettes library(colorspace) #pour les couleurs library(viridis) library(viridisLite) #utilitaires library(citr) # un outil interactif pour insérer des références library(doParallel) #Template pour ggplot theme_set(theme_bw()) 1.5 Disponibilité Lensemble du code est disponible sur github. Sous licence creative common ( comment met-on le badge ?) citation. ( un doi serait bien, au mini un bib) References "],["intro.html", "Chapitre 2 Introduction 2.1 Une réflexion ancienne et un nouveau champ méthodologique 2.2 Les facteurs de développement de lusage en science sociale 2.3 De nouvelles méthodologies pour les sciences sociales 2.4 Un nouvel objet : 2.5 Des comptable à lindustrie de la langue 2.6 Conclusion", " Chapitre 2 Introduction Le texte connaît une double révolution. la première est celle de son système de production, il se produit désormais tant de textes que personne ne peut plus tous les lire, même en réduisant son effort à sa propre sphère dintérêt et de compétence, la seconde est celle de sa lecture, cest une lecture conditionnée et recommandée. La production primaire de textes voit son volume croître exponentiellement. Elle doit être comparée à des époques pas si éloignées ou le texte était copiés, puis imprimés. Ce qui était destiné à être reproduit était le résultats dun processus long et exigeant qui permettait à un petit groupe de léttrés et dimprimeur de produire lessentiel de ce qui sera lu. La révolution digitale fait que quiconque via des interfaces simples peut confier sous forme décrit ses états dâme.Elle se soumet ensuite à ceux qui en contrôlent les flux et en exploitent les contenus, qui les mettent en avant ou les écartent, définissant la composition de ce que chacun va lire. La diffusion de cette production suit des lois puissances, cest ainsi que la révolution de la lecture est venue avec les moteurs de recherche, et les pratiques de curations (ref), cest une lecture sélectionnée et digérée par les moteurs de recommandation. (ref). Sil ne fallait quun exemple on pourrait évoquer la transformation radicale de la littérature dite scientifique dont la production double presque tout les dix ans .http://blogs.nature.com/news/2014/05/global-scientific-output-doubles-every-nine-years.html A cette production exponentiellement croissante sajoute un effort dinventaire, des standards sont proposés, lindexation a donné naissance à limmatriculation de la moindre note, linteropérabilité est de mise, le réseau des co-citations est maintenu en temps réel. Les scores qualifient autant les articles que leurs auteurs et les revues qui les accueillent. Le monde de la recherche, celui qui va vers linconnu est désormais totalement balisé et quantifié. Le volume est si grand que la production automatique de résumés, et de synthèses va être indispensable. Le NLP ( en français le TAL) est au coeur de ces technologies, il se nourrit de plus en plus dintelligence artificielle. Nous en verrons de nombreux exemples à tout les stades du traitement : identifier la langue, mesurer le sentiment, isoler des sujets, calculer une relation syntaxique. Le NLP est aussi une nouvelle ressource pour les chercheurs en sciences sociales à la fois par les matériaux empiriques et les méthodes danalyse. Cest une mouvemennt qui affecte toutes les shs. Lemballement de la production de texte génère une nouvelle matière détude pour le sociologues, les gestionnaires, les économistes, les psychologues. Si dans ce manuel, on choisit de présenter les différentes facettes de ce qui sappelle TAl, NPL, Text Mining, dans une approche procédurale qui suit les principales étapes du traitement des données. On rendra compte à chaque étape des techniques disponibles, et on illustre dexemples. Nous suivrons ici une approche plus fidèle au processus de traitement des données, lequel peut connaître une stratégie inférentielle et exploratoire - quelles informations sont utiles au sein dun corpus de texte -, tout aussi bien quune stratégie hypothético-déductive. Nous resterons agnostique sur cette question, restant délibérément à un niveau technique et procédural. 2.1 Une réflexion ancienne et un nouveau champ méthodologique On se doit pas se faire aveugler par léclat de la nouveauté, les techniques daujourdhui dépendent didées semées depuis longtemps dans au moins deux champs disciplinaires : la linguistique et linformatique. On peut en synthétiser lidée avec ce mème et son annotation. Il expriment deux idées. La première est une tension du champs entre la langue comme structure et le langage comme capacité et usage. linguistique Les pratiques et techniques que nous allons étudier ne tombent pas de mars mais résultent de plusieurs flux de pensées qui se croisent se confortent et amène lénergie pour créer un nouveau bras dans le champs étendu de létude de la langue et du langage. Penser la langue est un effort constant qui a commencé il y a fort longtemps, certainement avec les sophistes, et lidée quen maniant le langage il est possible de convaincre. Les sophistes : plier le langage à ses intérêt est une première sciences du langage qui produit une connaissances des dispositifs les plus efficaces. Pas sur que cette discipline aie trouvé un chemin de vérité; mais elle reste commune et actuelle, cest loeuvre de la publicité. La rhétorique nest pas une discipline morte, elle se développe de manière concrète dans toute les agences publicitaires. Donnons quelques points de repère en commençant par des définition, puis en évoquant trois idées essentielles qui vont prospérer avec le développement de la linguistique computationnelle et de lintelligence artificielle. Ces trois idées se relatives aux principales branches de la linguistique : à savoir la syntaxe, la sémantique et la pragmatique. Nous serons silencieux sur la phonologie (étude de la formation des sons et de la phonétiques) dont limportance est considérable quand il sagit de traiter la production et les interactions orales. Pour ne donner quun exemple, la prosodie (le rythme données aux phrases) est un objet essentiel dans linformatique affective. 2.1.1 Langue, langage et texte parole Et cets sans doute par celà quil faut commencer. La langue cest lensemble des règles formelles et moins formelles qui constituent une parole, ce quon se dit de lun à lautre ou de lun à aux autres. Le langage est la capacité à produire cette parole. Linscription de cette parole par lécriture constitue le texte. Le miracle du passage de la parole au signe est celui du symbole. Parmi les distinctions terminologiques proposées par Ferdinand de Saussure au début de siècle dernier, celles de langue, langage et de parole se sont révélées particulièrement pertinentes et elles sont toujours utilisées de nos jours. Langage: faculté inhérente et universelle de lhumain de construire des langues (des codes) pour communiquer. (Leclerc 1989:15) Le langage réfère à des facultés psychologique permettant de communiquer à laide dun système de communication quelconque. Le langage est inné. Langue: système de communication conventionnel particulier. Par « système », il faut comprendre que ce nest pas seulement une collection déléments mais bien un ensemble structuré composé déléments et de règles permettant de décrire un comportement régulier (pensez à la conjugaison de verbes en français par exemple). La langue est acquise. Le langage et la langue sopposent donc par le fait que lun (la langue) est la manifestation dune faculté propre à lhumain (le langage). Parole: une des deux composantes du langage qui consiste en lutilisation de la langue. La parole est en fait le résultat de lutilisation de la langue et du langage, et constitue ce qui est produit lorsque lon communique avec nos pairs. Le texte : Il est la transcription de la parole,même si le plus souvent, sa production est directe sans traduction du langage oral. 2.1.2 Syntaxe et grammaire générative Chomsky et sa grammaire générative. En dépit de leur très grande diversité. Le projet sappuit sur lidée quun nombre de règles finies doit produire une infinité dennoncés. Une grammaire est générative dans la mesure où elle possède cette propriété. Lanalyse est ainsi tournée vers la compétence, et le linguiste sinterresse à lidéal quun locuteur qui connaissant ces règles seraient en mesure de produit toute forme de discours. Observant que les enfants apprennent enracinant le phénomène linguistique dans la cortalisation du langage, il apporte une idée forte et structuraliste dune équivalence des langues. Tesnière et les arbres syntaxiques. les treebanks contemporains sinscrivent dans cette perspective et nourrissent les analyseurs ( parser) syntaxiques du langage naturel qui constituent désormais la première couche dun traitement de données textuelles. La grammaire générative a conduit la linguistique dans un tournant formel où la langue est étudiée indépendemment de ses locuteurs. On pourra méditer le pourquoi les algorithmes génératifs de deep learning contemporains (le fameux GPT3) peuvent former des phrases syntaxiquement correcte mais absurde. 2.1.3 Sémantique : La conception distributionnelle la tradition lexicologique le lexique est affaire ancienne, le français est aidé par des expériences les fondamentales : le littré, lacadémie française et les dictionnaires des éditeurs. pour étudier un lanage il faut se rapporter à des formes stables, les dictionnaires les fournissent et fournisse les normes pour les coder. Un moment clé a été de penser le signe, Saussure apporte une idée fondamentale que dans le symbole, le signe et le signifiant sont les deux faces dune même monnaie, quil existe une relation entre lartefact et lidée. Quun signe particulier puisse signifier une idée. cest un penseur de la correspondance. Selon Saussure, la langue est le résultat dune convention sociale transmise par la société à lindividu et sur laquelle ce dernier na quun rôle accessoire. Par opposition, la parole est lutilisation personnelle de la langue (toutes les variantes personnelles possibles: style, rythme, syntaxe, prononciation, etc.).Le changement de la langue relève dun individu mais son acceptation relève de la communauté et des institutions. ex.: le verbe « jouer » conjugué «jousent » est pour linstant considéré comme une variante individuelle (parole), une exception, et il le demeurera tant quil ne sera pas accepté dans la communauté (les locuteurs du français dans ce cas-ci). Sa conception du signe répond à cette approche conventionnelle : la dualité du signe comme signifiant et signifié est opérée. Dans le traitement des données textuelle le signifié est le terme cible de lanalyse, pour en découvrir son signifié on se tourne vers son contexte : lensemble des signifiés. Cest une idée ancienne qua proposé Firth dans les années 30. (Firth 1957) et lidée distributionnelle. un mot trouve son sens dans ceux qui lui sont le plus associés. cest le contexte qui donne le sens Lidée de quantifier le langage nest pas nouvelle Zipf. Encore moins sil faut compter les occurences et les cooccurences des mots.Un vaste mouvement sest formé dans les années soixante autour de la lexicologiue stimulée par lécole française danalyse de données. Le descendant de ce mouvement se retrouve dans lexcellent iramutek de léquipe de toulouse, il a été précedé par le fameur Alceste. Nous y consacrerons un chapitre plein sur le plan technique. Mais il est important de souligner que cette école française de lanalyse textuelle ne se limite pas au comptage. Un logiciel comme trope qui dailleurs ne connait aucun équivalent dans lécosystème que nous allons explorer manifeste aussi cette inventivité. Sy exprime pleinement la logique distributionnelle. 2.1.4 Pragmatique les fonctions et acte du langage Si la grammaire générative se tourne délibéremment vers la compétence et ignore la performance, cest à dire la production dénnoncés par les humains en situation dinteraction, un autre courrant de la linguistique sest emparé de la question, le courrant pragmatique. Le grand classique est la théorie des fonctions du language, qui sous-tendent la production du message, lacte de parole,proposée par Jakobson. Inspiré par la cybernétique la structure de son modèle est celle dun acte de communication. Jackobson identifie les éléments de lévènement discursif (speech event) et les fonctions qui lui sont associée. Pour le paraphraser, un DESTINATEUR envoie un MESSAGE à un DESTINATEUR mais pour être compris il requiet un CONTEXTE dont les acteurs acteurs de lévenement discursif sont capables de se saisir et de verbaliser , dun CODE au moins partiellement commun et dun CONTACT, un canal physique et une connection psychologique. (Linguistics and Poetics 1981)àlire ici la fonction référentielle ou représentative (aussi dénommée sémiotique ou symbolique), où lénoncé donne létat des choses , où le message dénote un contexte ;J emploie aussi les termes de dénotatif ou cognitive. la fonction expressive ( emotive), où le sujet exprime son attitude propre à légard de ce dont il parle ; la fonction conative, lorsque lénoncé vise à agir sur le destinataire; elle sexprime grammaticalement par limpératif ou le vocatif. la fonction phatique,empruntée à Malinoswki où lénoncé révèle les liens ou maintient les contacts entre le locuteur et linterlocuteur ; la fonction métalinguistique ou métacommunicative, qui fait référence au code linguistique lui-même ;quil soit théorisé ou internalisé par le locuteur comme la prose de Mr Jourdain. la fonction poétique, où lénoncé est doté dune valeur en tant que tel, valeur apportant un pouvoir créateur et dont Jakobson illustre avec lexemple que la jeune fille qui a lhabitude de désigner Harry par Horrible harry sans pouvoir expliquer pourquoi il ne serait pas lodieux, le dégoûtant, ou le terrible Harry alors que sans sen rendre compte elle emploie une paronomasie/alliteration : la ressemblance des mots produit un effet poétique. John Langshaw Austin sintéressant à la fonction conative développe le concept dacte de langage, introduisant lidée fondamentale que les actes de langage (la production dun énoncé) nont pas destinés à décrire le monde tel quil est mais à agir sur le monde par le bias du destinataire. Parler cest aussi faire. La théorie des actes de langage est dabord une catégorisation des actes. La fonction illocutoire dun acte de langage est, dans la théorie linguistique de John Langshaw Austin, le message convoyé par un énoncé au-delà de son sens immédiat, celui que traduit sa fonction locutoire. Par exemple, le fait, à table, de prononcer la phrase « Est-ce quil y a du sel ? » na pas, du seul fait de sa formulation, seulement pour fonction de sinformer sur la présence de sel dans la maison (ou dans le plat, contenu locutoire de lénoncé) mais exprime plutôt que lon voudrait saler son plat (fonction illocutoire) et se traduit généralement par le fait que lun des convives réagit, par exemple en passant la salière au locuteur, ce qui est la fonction perlocutoire de lénoncé1. * Locutoire * Illocutoire * Perlocutoire Cest cette idée qui est au sousbassement de la théorie des actes de langages. Genette et lintertextualité, le palimpseste. cest une question de sens, le sens dun texte vient de ses prédecesseurs de ceux à qui ils se réfèrent. Les textes se parlent lun lautre, et ce nest pas dans leur contenu quon trouvera une vérité dans dans le rapport quils établissent avec leur prédecesseur par lappareil des notes et des bibliographies. Austin et lidée que le langage nest ^pas que communication mais performation . ce quon dirt agit sur le monde La narrativité 2.1.5 la linguistique computationnelle le frottement de la linguistique et de linformatique se produit à propos de questions pratiques. Les apports de la fouille de données les nomenclatures une convergence nécessaire Le monde des bibliothèques et celui de la GED. 2.2 Les facteurs de développement de lusage en science sociale Ces développements sont favorisés par un environnement fertile dont trois facteurs se renforcent mutuellement. Ils conduisent à lélaboration de nouvelles méthodes. la naissance de langue universelle lemergence vaste ensemble de données textuelle la naissance dune communauté épistémique, de pratique et 2.2.1 Une lingua franca Le premier est lexpansion de deux langages, proprement statistique pour r et plus généraliste pour Python. Le propre de ces langages est, prenons le cas de r, de permettre délaborer des fonctions, dont un ensemble cohérent pour réaliser certaines tâches peut être rassemblé dans une bibliothèque appelée package (et chargé par library(nomdupackage)). On dispose désormais de milliers de packages (17 788 sur le CRAN) destinés à résoudre un nombre incalculable de tâches. hornik Coder une analyse revient ainsi à jouer avec un immense jeu de lego, dont de nombreuses pièces sont déjà pré-assemblées. Dun point de vue pratique, les lignes décriture sont fortement simplifiées permettant à un chercheur sans grande compétence de codage deffectuer simplement des opérations complexes. En retour, cette facilitation de lanalyse abonde le stock de solutions. 2.2.2 La multiplication des sources de données. Le troisième est la multiplication des sources de données et leur facilité daccès. le contenu écrit des réseaux sociaux les rapports dactivités des entreprises, les compte-rendu archivé de réunion Les avis des consommateurs sur les catalogues de produit Les articles et les revues scientifiques Même les livres Les plus évidentes sont proposées par les bases darticles de presse telles qu presseurop ou factiva. Les bases de données bibliographiques sont dans la même veine particuièrement intéressante et pensée pour ces usages. Les données privées, et en particulier celles des réseaux sociaux, même si un péage doit être payé pour accéder aux APIs, popularisent le traitement de données massives. Les forums et sites davis de consommateurs sont pour les sociologues de la consommation et les specialiste du comportement de consommation une ressource directe et précieuses. Le mouvement des données ouvertes (open data) proposent et facilitent laccès à des milliers de corps de données : grand débat. 2.2.3 Une communauté Le second facteur de développement , intimement lié au premier, est la constitution dune large communauté de développeurs et dutilisateurs qui se retrouvent aujourdhui dans des plateformes diverses. Le savoir, autrement dit des codes commentés se trouvent dans une varété importante de lieux : Des plateformes de dépots telle que Github qui rassemblent une trentaine de millions de developpeurs et datascientits. Des plateformes de Q&amp;A (question et réponses) telles que Stalk Over Flow, Des tutoriaux de toute sortes Des blogs ou des fédération de blog de blogs (BloggeR), Des revues (Journal of Statistical Software) et de bookdown. Des ressources abondantes sont ainsi disponibles et facilitent la formation des chercheurs et des data scientists et la résolution de leurs problèmes pratiques, quiconque narrive pas à résoudre un problème a une bonne chance de trouver la solution dun autre, à un degré de circonstance près. Elles sont dautant plus utiles que certaines règles ou conventions simposent pour fluidifier léchange. La principale est celle de lexemple reproductible. La seconde est le maintien dune éthique du partage qui encourage à partager le code, et dont une littérature importante étudie leffet positif sur les performances économiques et la durabilité [rauter]. Les externalités de réseaux y sont fortes Toutes les conditions sont réunies pour engendrer une effervescence créative. Python ou r, sont dans cet univers en rapide expansion, les langues véhiculaires qui favorise une innovation constante. Les statistiques de github en témoigne : près de 50 millions dutiliseurs, 128 millions de \" repositories\" et 23 millions de propriétaires. source voir aussi https://towardsdatascience.com/githubs-path-to-128m-public-repositories-f6f656ab56b1 2.3 De nouvelles méthodologies pour les sciences sociales Pour les chercheurs en sciences sociales (et en premier lieu pour les chercheurs en gestion où toutes les sciences sociales se croisent) cette révolution textuelle offre de nouvelles opportunités dobtenir et danalyser des données solides pour vérifier ses hypothèses et mener lenquête. Ce sont de nouveaux terrains, de nouvelles méthodes et un nouvel objet de recherche qui se dessine dans le foisonnement du champs. 2.3.1 Nouveaux terrains : La multiplication des sources de données, associée à leur normalisation rencontre une multiplication de techniques provenant de multiples disciplinaires et qui convergent dans un langage commun. . production abondante davis de consommateurs, de discours de dirigeants, de compte-rendus de conseils, darticles techniques,la linguistique computationnelle, de la fouille de données, des moteurs de recommandation, de la traduction automatique, et des ressources nouvelles et précieuses pour traiter labondance des données 2.3.2 Nouvelles méthodes : Un nouveau paradigme méthodologique se construit à la croisée de données abondantes et de techniques de traitement intelligentes. Il permet daller plus loin que lanalyse lexicale traditionnelle en incorporant des éléments syntaxiques, sémantiques, et pragmatiques, proposés par lensemble des outils des techniques de traitement du langage naturel. Il se dessine surtout une nouvelle approche méthodologique qui prend place entre lanalyse qualitative, et les traditionnelles enquêtes par questionnaires capables de traiter des corpus dune taille inédite. Le travail de (Humphreys and Wang 2018) en donne une première synthèse dans le cadre dun processus qui sarticule autour des différentes phases dune recherche : la formulation de la question de recherche, la définition des construits, la récolte des données, lopérationnalisation des construits, et enfin linterprétation, lanalyse et la validation des résultats obtenus. 2.4 Un nouvel objet : On pourrait croire quavec des données massives et des techniques intelligente on assiste à un retour du positivisme qui bénéficierait enfin des instruments de mesure et de calculs qui ont permis aux les chercheur plus proches de la matière des succès majeurs. Sans doute, ladministration de la preuve va être faciliter par ces techniques et encourager levidence based policy (REF) et résoudre en partie la crise de la réplication et de la reproductibilité. Mais à mesure que ce développe lappareillage de méthode et de données, moins on peut supposer que lobservateur est neutre. Les téléscopes géants, les synchrotron, naffecte ni les galaxies lointaines ni les atomes proches. Le propre des données que lont est amené à étudier est de résulter de la confrontation dun système dobservation (certains préfèrent de parler de surveillance), et dun agent qui a des buts, une connaissance, et des ressources. Le dispositif de mesure est en lui-même performatif. Lexemple le plus évident est celui des systèmes de notation, qui sous prétexte de transparence donne la distribution des répondants précédents. Lagent qui va noter choisit la valeur en fonction dune norme apparente - la note majoritaire- et de sa propre intention - se manifester ou se confondre à la foule. Pour se donner une idée plus précise de ce mouvement, examinons quelques publications récentes dans les champs qui nous concernent. 2.4.1 Sociologie et histoire classes sociales avec word to vec en sociologie (Kozlowski, Taddy, and Evans 2019). Larticle révolution française [PNAS) On citera cependant jean-baptiste Coulmont et son obstination à étudier les entités nommées, prénoms et autres marqueurs culturels de lidentité et des classes. et au luxembourg 2.4.2 Psychologie Très tôt la psychogie sest intéressée au langage, pas seulement comme produit des processus psychologiques, mais comme expression de ceux-ci. Dans le champs de la psychologie de léducation et avec une forte motivation positiviste, dès les années 60 sest posée la question de la mesure de la difficulté dun texte pour un niveau déducation donné. La mesure de la lisibilité des texte sest développée profitant à dautre secteurs tels que ceux de la propagande. Dans cette même perspective, la richesse lexicographique comme représentant les compétences a a son tour développé de nouvelles instrumentations. James W. Pennebaker a développé son approche à partir de létude des traumas; donnant une grande importance à la production discursive des patients. Sa contribution majeure est létablissement dun ensemble de dictionnaires destinés à mesurer des caractéristiques du discours. Un instrument quon présentera dans le chapitre 7 (à vérifier)(Tausczik and Pennebaker 2010). Son approche se poursuit en psychiatrie avec lanalyse des troubles du langage, et a connu un coup déclat avec la demonstration que lanalyse des messages sur les réseaux sociaux comme facebook permet de détecter des risques de dépression.(Eichstaedt et al. 2018). 2.4.3 Management La finance et lanalyse du sentiment Dans le champ du management, on trouvera des synthèses pour la recherche en éthique (Lock and Seele 2015), en comportement du consommateur (Humphreys and Wang 2018) en management public (Anastasopoulos, Moldogaziev, and Scott 2017) ou en organisation (Kobayashi et al. 2018) , 2.4.4 Economie economie des brevets intervention des institutions mesure de linnovation 2.5 Des comptable à lindustrie de la langue La situation nouvelle qui est la notre est que lorsque la parole disparaissait avec le vent, elle laisse désormais des traces et senregistre. Lironie est quau titre de la protection de la vie privée, cet enregistrement systèmatique doit être mis à notre disposition. On a le choix : rien nen faire, les détruire, les donner pour bénéficier de son potentiel de connaissance. Nous sommes passé de la parole au texte. Si seule la prole de dieux et celle des champs étaient transcrites, cest désormais aussi celle du vulgum. Si sa précision est incertaine, son volume a gagné de nombreuses échelles. Cette matière ne sorganise plus dans les papyrus et autres manuscrits, ni même dans les livres sués par les callygraphieurs, elle sincruste dans un édifice de plus en plus complexe dinterfaces textuelles et vocales. La parole est comme absorbée par les machines. Elle ne senvolent plus avec le vent, elle sédimente dans les data center. Le langage a acquis une dimension matérielle quil na presque jamais connu. Il gagne de lautonomie avec les systèmes génératifs : chat bot, transciption, traduction, résumés. lhistoire se définit son écriture. lécriture est le produit dune société de procès-verbal, de comptabilité et ça se poursuit. Voilà qui facilite le travail de lhistorien, du sociologue et de léconomiste. Dans les années 90 sest dessinée une société de linformation, sauvage jusqu à Napters, et le rêve du peer to peer, elle sest socialisée dans les années 2000, platformisée dans les années 2010, généralisée pour la décennie qui nous concerne. Toute cette architecture sappuie sur les données quon y injecte, et en premier le texte. la transcription de la parole, une recodification. Et des traitements très concentrés. 2.6 Conclusion le point dentrée de la technique est privilégié dans ce manuel. Mais on se donnera des espaces de réflexion, dinterrogation, des espaces epistémologiques (Comment étudier le langage par le langage ?) et anthropologique (quelle est lorigine et la spécificité du langage humain en dépit de ses innombrables variétés ?). Une première parenthèse est expérientielle, cest en faisant que nous avons découvert une autre écriture. Lexpérience de ce livre, quon partage avec de nombreux utilisateurs de ces nouveaux outils, est celle dune écriture programmatique, performative. Ecrire cest faire, les meta-langage transforme la transcription de la parole en une nouvelle connaissance.On peut agir sur la parole, sur le texte, le tordre, le presser, le décoder . On peut lire les foules. Les langages tels que la linguistique les étudie sont verbaux, dautres sont iconiques, architecturaux, graphiques, chorégraphiques, musicaux. Elles se rencontre dans le flux dune parole qui associe le texte à limage dans des rapports dillustation ou de commentaire, jouant du contrepoint à travers les médias. par le texte, le sociologue, léconomiste ou le gestionnaire veulent coprendre la génèse et la détermination des choix. Etudions le texte. Lacte de parole se réalise dans un lieu à un moment avec des protagonistes, dans une atmosphère, avec une histoire, les mots qui sen échappent ne sont que des traces, autant que des photographies. Ces données se sédimentent dans les grands bassins du cloud et dans les corpus constitués méthodiquement. References "],["la-diversité-et-la-nature-des-corpus.html", "Chapitre 3 La diversité et la nature des Corpus 3.1 Des types de corpus 3.2 Problèmes déchantillon", " Chapitre 3 La diversité et la nature des Corpus 3.1 Des types de corpus Les critères : Nombre de documents des document court vs documents long : le twitt vs le livre Texte vernaculaire - langue soutenue locuteur unique ou multiples interaction ou nom des locuteurs degré de structuration : séquentielle, plan spatiale ( ex = le curriculum viate, la fiche de brevet,) 3.2 Problèmes déchantillon Un corpus reste un échantillon. Dans ce chapitre nous avons appris comment faire la cueillette dans les sources de textes et constituer matériellement un corpus. Il reste à traiter la question de la représentativité. La collecte doit rester raisonnée. Les unités de texte. Une unité de texte : un chaine de caractère intégrée dans un document. Celui ci peut être un livre un article, une note, une transcription, Un document Un ou des auteurs du document Une date Un endroit Un contexte : les unités précedente, et subséquentes. Unités de production et de reception, Un texte est produit et puis il est lu, peut-être.Analyser le texte peut se faire dans deux perspectives, celle de la production et celle de la réception. Les corpus doivent être construit en fonction de ce critère. Examiner la question de lengagement dans ce cadre est essentiel, certains acteurs sur un sujet donnée sont amenés à parler plus que les autres et développent un surcroit de voix. la question du biais de selection Un corpus est un ensemble de documents. Ils peuvent être courts, les tweets, pas trop long - abstract articles court - long ( article de recherche, ou très long (livres). La collecte peut se faire dabord sur des matériaux primaires, numérisés sous forme dimages, et dans lesquels en analysant les pixels on peut reconnaitre un texte. "],["constitution-du-corpus.html", "Chapitre 4 Constitution du corpus 4.1 La gestion des documents numériques 4.2 Lexploitation de base de données textuelles 4.3 Lire le web : Scrapping 4.4 Limportance croissante des API 4.5 Conclusion", " Chapitre 4 Constitution du corpus Objectifs du chapitre : ** explorer différente techniques de collectes de données : exploitation de bases textuelles, méthodes de scrapping, APIS, extraction de document pdf, extration de texte dans des images, et une perspective oral avec les techniques de speech2 tex.** La constitution dun corpus est la première étape dun projet NLP. Il se définit dabord par la constitution dune collection de textes dont la provenance est la nature peut être diverse. Dans ce chapitre on va examiner plusieurs techniques de collecte, et on conclue avec quelques réflexions que la questions de la constitution de léchantillon. Lexploitation de bases textuelles Les méthodes de scrapping Le recours aux APIs La collection de document pas que textuels Les sources orales et on conclue avec quelques réflexions que la questions de la constitution de léchantillon. 4.1 La gestion des documents numériques Dans certains cas le matérieux se présentatera sous formes de document numériques tels un pdf, ou même de simples images, voir aussi https://cran.r-project.org/web/packages/fulltext/fulltext.pdf 4.1.1 Extraire du texte des pdf Le package pdftools est parfaitement adapté à la tâche. Des fonctions simples extraient différents éléments du pdf : * les information relative au document pdf lui-même * La liste des polices employées * Les attachements * La table des matières ( si elle a été encodée) * et naturellement le texte dans un ordre de droite à gauche et de ligne à ligne, reconnaissant cependant les retrour chariot, et sauts de lignes. Chaque page est contenue dans une ligne. library(pdftools) info &lt;- pdf_info(&quot;./pdf/2021neoliberalismegouverner_Meunier_Esprit.pdf&quot;) info ## $version ## [1] &quot;1.4&quot; ## ## $pages ## [1] 12 ## ## $encrypted ## [1] FALSE ## ## $linearized ## [1] TRUE ## ## $keys ## $keys$Author ## [1] &quot;&quot; ## ## $keys$Creator ## [1] &quot;&quot; ## ## $keys$Keywords ## [1] &quot;&quot; ## ## $keys$Producer ## [1] &quot;TCPDF 6.2.12 (http://www.tcpdf.org)&quot; ## ## $keys$Subject ## [1] &quot;&quot; ## ## $keys$Title ## [1] &quot;Le néolibéralisme et l\\220art de gouverner&quot; ## ## $keys$Trapped ## [1] &quot;&quot; ## ## ## $created ## [1] &quot;2021-05-04 17:33:26 CEST&quot; ## ## $modified ## [1] &quot;2021-07-02 14:10:59 CEST&quot; ## ## $metadata ## [1] &quot;&lt;?xpacket begin=\\&quot;ï»¿\\&quot; id=\\&quot;W5M0MpCehiHzreSzNTczkc9d\\&quot;?&gt;\\n&lt;x:xmpmeta xmlns:x=\\&quot;adobe:ns:meta/\\&quot; x:xmptk=\\&quot;Adobe XMP Core 5.6-c017 91.164464, 2020/06/15-10:20:05 \\&quot;&gt;\\n &lt;rdf:RDF xmlns:rdf=\\&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#\\&quot;&gt;\\n &lt;rdf:Description rdf:about=\\&quot;\\&quot;\\n xmlns:dc=\\&quot;http://purl.org/dc/elements/1.1/\\&quot;\\n xmlns:xmp=\\&quot;http://ns.adobe.com/xap/1.0/\\&quot;\\n xmlns:pdf=\\&quot;http://ns.adobe.com/pdf/1.3/\\&quot;\\n xmlns:xmpMM=\\&quot;http://ns.adobe.com/xap/1.0/mm/\\&quot;\\n xmlns:pdfaExtension=\\&quot;http://www.aiim.org/pdfa/ns/extension/\\&quot;\\n xmlns:pdfaSchema=\\&quot;http://www.aiim.org/pdfa/ns/schema#\\&quot;\\n xmlns:pdfaProperty=\\&quot;http://www.aiim.org/pdfa/ns/property#\\&quot;&gt;\\n &lt;dc:format&gt;application/pdf&lt;/dc:format&gt;\\n &lt;dc:title&gt;\\n &lt;rdf:Alt&gt;\\n &lt;rdf:li xml:lang=\\&quot;x-default\\&quot;&gt;Le nÃ©olibÃ©ralisme et lâ&lt;U+0080&gt;&lt;U+0099&gt;art de gouverner&lt;/rdf:li&gt;\\n &lt;/rdf:Alt&gt;\\n &lt;/dc:title&gt;\\n &lt;dc:creator&gt;\\n &lt;rdf:Seq&gt;\\n &lt;rdf:li/&gt;\\n &lt;/rdf:Seq&gt;\\n &lt;/dc:creator&gt;\\n &lt;dc:description&gt;\\n &lt;rdf:Alt&gt;\\n &lt;rdf:li xml:lang=\\&quot;x-default\\&quot;/&gt;\\n &lt;/rdf:Alt&gt;\\n &lt;/dc:description&gt;\\n &lt;dc:subject&gt;\\n &lt;rdf:Bag&gt;\\n &lt;rdf:li/&gt;\\n &lt;/rdf:Bag&gt;\\n &lt;/dc:subject&gt;\\n &lt;xmp:CreateDate&gt;2021-05-04T17:33:26+02:00&lt;/xmp:CreateDate&gt;\\n &lt;xmp:CreatorTool/&gt;\\n &lt;xmp:ModifyDate&gt;2021-07-02T14:10:59+02:00&lt;/xmp:ModifyDate&gt;\\n &lt;xmp:MetadataDate&gt;2021-07-02T14:10:59+02:00&lt;/xmp:MetadataDate&gt;\\n &lt;pdf:Keywords/&gt;\\n &lt;pdf:Producer&gt;TCPDF 6.2.12 (http://www.tcpdf.org)&lt;/pdf:Producer&gt;\\n &lt;xmpMM:DocumentID&gt;uuid:95265141-0cc7-e3b8-5dff-27561dd19960&lt;/xmpMM:DocumentID&gt;\\n &lt;xmpMM:InstanceID&gt;uuid:71358868-c7aa-4ef5-8b30-f18fa6312a88&lt;/xmpMM:InstanceID&gt;\\n &lt;pdfaExtension:schemas&gt;\\n &lt;rdf:Bag&gt;\\n &lt;rdf:li rdf:parseType=\\&quot;Resource\\&quot;&gt;\\n &lt;pdfaSchema:namespaceURI&gt;http://ns.adobe.com/pdf/1.3/&lt;/pdfaSchema:namespaceURI&gt;\\n &lt;pdfaSchema:prefix&gt;pdf&lt;/pdfaSchema:prefix&gt;\\n &lt;pdfaSchema:schema&gt;Adobe PDF Schema&lt;/pdfaSchema:schema&gt;\\n &lt;/rdf:li&gt;\\n &lt;rdf:li rdf:parseType=\\&quot;Resource\\&quot;&gt;\\n &lt;pdfaSchema:namespaceURI&gt;http://ns.adobe.com/xap/1.0/mm/&lt;/pdfaSchema:namespaceURI&gt;\\n &lt;pdfaSchema:prefix&gt;xmpMM&lt;/pdfaSchema:prefix&gt;\\n &lt;pdfaSchema:schema&gt;XMP Media Management Schema&lt;/pdfaSchema:schema&gt;\\n &lt;pdfaSchema:property&gt;\\n &lt;rdf:Seq&gt;\\n &lt;rdf:li rdf:parseType=\\&quot;Resource\\&quot;&gt;\\n &lt;pdfaProperty:category&gt;internal&lt;/pdfaProperty:category&gt;\\n &lt;pdfaProperty:description&gt;UUID based identifier for specific incarnation of a document&lt;/pdfaProperty:description&gt;\\n &lt;pdfaProperty:name&gt;InstanceID&lt;/pdfaProperty:name&gt;\\n &lt;pdfaProperty:valueType&gt;URI&lt;/pdfaProperty:valueType&gt;\\n &lt;/rdf:li&gt;\\n &lt;/rdf:Seq&gt;\\n &lt;/pdfaSchema:property&gt;\\n &lt;/rdf:li&gt;\\n &lt;rdf:li rdf:parseType=\\&quot;Resource\\&quot;&gt;\\n &lt;pdfaSchema:namespaceURI&gt;http://www.aiim.org/pdfa/ns/id/&lt;/pdfaSchema:namespaceURI&gt;\\n &lt;pdfaSchema:prefix&gt;pdfaid&lt;/pdfaSchema:prefix&gt;\\n &lt;pdfaSchema:schema&gt;PDF/A ID Schema&lt;/pdfaSchema:schema&gt;\\n &lt;pdfaSchema:property&gt;\\n &lt;rdf:Seq&gt;\\n &lt;rdf:li rdf:parseType=\\&quot;Resource\\&quot;&gt;\\n &lt;pdfaProperty:category&gt;internal&lt;/pdfaProperty:category&gt;\\n &lt;pdfaProperty:description&gt;Part of PDF/A standard&lt;/pdfaProperty:description&gt;\\n &lt;pdfaProperty:name&gt;part&lt;/pdfaProperty:name&gt;\\n &lt;pdfaProperty:valueType&gt;Integer&lt;/pdfaProperty:valueType&gt;\\n &lt;/rdf:li&gt;\\n &lt;rdf:li rdf:parseType=\\&quot;Resource\\&quot;&gt;\\n &lt;pdfaProperty:category&gt;internal&lt;/pdfaProperty:category&gt;\\n &lt;pdfaProperty:description&gt;Amendment of PDF/A standard&lt;/pdfaProperty:description&gt;\\n &lt;pdfaProperty:name&gt;amd&lt;/pdfaProperty:name&gt;\\n &lt;pdfaProperty:valueType&gt;Text&lt;/pdfaProperty:valueType&gt;\\n &lt;/rdf:li&gt;\\n &lt;rdf:li rdf:parseType=\\&quot;Resource\\&quot;&gt;\\n &lt;pdfaProperty:category&gt;internal&lt;/pdfaProperty:category&gt;\\n &lt;pdfaProperty:description&gt;Conformance level of PDF/A standard&lt;/pdfaProperty:description&gt;\\n &lt;pdfaProperty:name&gt;conformance&lt;/pdfaProperty:name&gt;\\n &lt;pdfaProperty:valueType&gt;Text&lt;/pdfaProperty:valueType&gt;\\n &lt;/rdf:li&gt;\\n &lt;/rdf:Seq&gt;\\n &lt;/pdfaSchema:property&gt;\\n &lt;/rdf:li&gt;\\n &lt;/rdf:Bag&gt;\\n &lt;/pdfaExtension:schemas&gt;\\n &lt;/rdf:Description&gt;\\n &lt;/rdf:RDF&gt;\\n&lt;/x:xmpmeta&gt;\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n&lt;?xpacket end=\\&quot;w\\&quot;?&gt;&quot; ## ## $locked ## [1] FALSE ## ## $attachments ## [1] FALSE ## ## $layout ## [1] &quot;single_page&quot; fonts &lt;- pdf_fonts(&quot;./pdf/2021neoliberalismegouverner_Meunier_Esprit.pdf&quot;) files &lt;- pdf_attachments(&quot;./pdf/2021neoliberalismegouverner_Meunier_Esprit.pdf&quot;) toc &lt;- pdf_toc(&quot;./pdf/2021neoliberalismegouverner_Meunier_Esprit.pdf&quot;) #il n&#39;y a pas de table des matière dans ce texte text &lt;- pdf_text(&quot;./pdf/2021neoliberalismegouverner_Meunier_Esprit.pdf&quot;) cat(text[[1]]) # pour afficher le texte de la page 1 ## Le néolibéralisme ## et lart de gouverner ## À propos de Naissance de la biopolitique ## de Michel Foucault ## François Meunier ## ## ## ## ## O n dit parfois du métier de lhistorien quil consiste avant tout ## à découper en périodes, à indiquer les ruptures dans le temps ## historique, à montrer les changements denvironnement et ## de paradigme. Cest à ce travail que se consacre Michel Foucault dans ## son célèbre cours de 1978-1979 au Collège de France connu sous le ## nom de Naissance de la biopolitique 1. Il devait porter initialement sur la ## « biopolitique », un mot chatoyant recouvrant les pratiques politiques ## contemporaines autour du vivant (santé, démographie, sexualité, etc.). ## Mais Foucault voulait montrer dabord à quel point la venue du libé- ## ralisme avait modifié en profondeur les pratiques gouvernementales. ## Première rupture, celle advenue à la fin du xviiie siècle avec le libéralisme ## économique classique, selon lequel le marché devient linstance clé dans ## lart de gouverner, donnant à laction publique un lieu de légitimation en ## même temps que des limites. Seconde rupture, celle qui sépare libéralisme ## et néolibéralisme, que Foucault situe dans laprès-guerre en Allemagne, ## avec ce quon appelle « lordolibéralisme ». ## ## ## Équivocité du néolibéralisme ## Reprenant, quelque quarante ans après, le fil de ce cours, nous remettons ## ici en cause le découpage historique. Dabord, il nous semble que ce ## ## 1 - Michel Foucault, Naissance de la biopolitique. Cours au Collège de France (1978-1979), Paris, ## EHESS/Seuil/Gallimard, 2004. ## ## ## ## ## · ESPRIT · Mai 2021 83/ Il va falloir traiter ce texte en analysant précisément sa composition. Et en définissant une séquence dopérations logiques qui permette un premier nettoyage du texte. Dans lexemple on va de plus essayer de respecter la structure en paragraphe du texte. Suprimer haut et bas de pages Supprimer les sauts de ligne Identifier les sauts de paragraphe Enlever les notes de bas de page Corriger lhyphénation () regrouper les document en un seul bloc de texte le splitter en autant de paragraphes. On va utiliser des fonctions de traitement de chaines de caractère avec Stringret le recours à lart ( ici simple) des regex auxquels on consacre un développement dans le chapitres X. tex&lt;- as.data.frame(text) tex[1,] ## [1] &quot;Le néolibéralisme\\net lart de gouverner\\nÀ propos de Naissance de la biopolitique\\nde Michel Foucault\\nFrançois Meunier\\n\\n\\n\\n\\nO n dit parfois du métier de lhistorien quil consiste avant tout\\n à découper en périodes, à indiquer les ruptures dans le temps\\n historique, à montrer les changements denvironnement et\\nde paradigme. Cest à ce travail que se consacre Michel Foucault dans\\nson célèbre cours de 1978-1979 au Collège de France connu sous le\\nnom de Naissance de la biopolitique 1. Il devait porter initialement sur la\\n« biopolitique », un mot chatoyant recouvrant les pratiques politiques\\ncontemporaines autour du vivant (santé, démographie, sexualité, etc.).\\nMais Foucault voulait montrer dabord à quel point la venue du libé-\\nralisme avait modifié en profondeur les pratiques gouvernementales.\\nPremière rupture, celle advenue à la fin du xviiie siècle avec le libéralisme\\néconomique classique, selon lequel le marché devient linstance clé dans\\nlart de gouverner, donnant à laction publique un lieu de légitimation en\\nmême temps que des limites. Seconde rupture, celle qui sépare libéralisme\\net néolibéralisme, que Foucault situe dans laprès-guerre en Allemagne,\\navec ce quon appelle « lordolibéralisme ».\\n\\n\\nÉquivocité du néolibéralisme\\nReprenant, quelque quarante ans après, le fil de ce cours, nous remettons\\nici en cause le découpage historique. Dabord, il nous semble que ce\\n\\n1 - Michel Foucault, Naissance de la biopolitique. Cours au Collège de France (1978-1979), Paris,\\nEHESS/Seuil/Gallimard, 2004.\\n\\n\\n\\n\\n· ESPRIT · Mai 2021 83/\\n&quot; t_reg&lt;-str_replace(tex$text,&quot;[\\\\s+].*Meunier[\\n]+&quot;, &quot; &quot;) # entete droite ## on selectionne tout bloc de texte qui commence par un nombre indéterminée de blanc qui s&#39;achève par n&#39;importe quel caractère répétés mais terimé par la séquence Meunier suivie de sauts de ligne. t_reg&lt;-str_replace(t_reg,&quot;[\\\\s+].*gouverner[\\n]+&quot;, &quot; &quot;) # entete gauche t_reg&lt;-str_replace_all(t_reg,&quot;[\\\\s+].*2021[\\n]&quot;, &quot; &quot;) # bas de page gauche t_reg&lt;-str_replace_all(t_reg,&quot;ESPRIT.*[\\n]&quot;, &quot; &quot;) # bas de page droit #on marque les paragraphes avec la chaine XXX pour les splitter dans un second temps t_reg&lt;-str_replace_all(t_reg,&quot;\\n\\n\\n&quot;, &quot;XXX&quot;) # On supprime les saut de ligne en les remplaçant par un espace t_reg&lt;-str_replace_all(t_reg,&quot;[\\n]&quot;, &quot; &quot;) #on enlève les notes de bas de page t_reg&lt;-str_replace_all(t_reg,&quot;\\\\d\\\\s[\\\\-].*XXX&quot;, &quot;XXX&quot;) #on regroupe les pages t&lt;-paste(unlist(t(t_reg)), collapse=&quot; &quot;) #on enlève les notes dans le texte t&lt;-str_replace_all(t,&quot;[A-Z|a-z]+\\\\d\\\\s[\\\\-]&quot;, &quot; &quot;) t&lt;-str_replace_all(t,&quot;\\\\d\\\\d\\\\s[\\\\-]&quot;, &quot; &quot;) #hyphenation t&lt;-str_replace_all(t,&quot;[A-Z|a-z]+[\\\\-]\\\\s&quot;, &quot;&quot;) #pour enlever les espaces excedentaires t&lt;-str_squish(t) t ## [1] &quot;Le néolibéralisme À propos de Naissance de la biopolitique de Michel Foucault O n dit parfois du métier de lhistorien quil consiste avant tout à découper en périodes, à indiquer les ruptures dans le temps historique, à montrer les changements denvironnement et de paradigme. Cest à ce travail que se consacre Michel Foucault dans son célèbre cours de 1978-1979 au Collège de France connu sous le nom de Naissance de la biopolitique 1. Il devait porter initialement sur la « biopolitique », un mot chatoyant recouvrant les pratiques politiques contemporaines autour du vivant (santé, démographie, sexualité, etc.). Mais Foucault voulait montrer dabord à quel point la venue du libé- ralisme avait modifié en profondeur les pratiques gouvernementales. Première rupture, celle advenue à la fin du xviiie siècle avec le libéralisme économique classique, selon lequel le marché devient linstance clé dans lart de gouverner, donnant à laction publique un lieu de légitimation en même temps que des limites. Seconde rupture, celle qui sépare libéralisme et néolibéralisme, que Foucault situe dans laprès-guerre en Allemagne, avec ce quon appelle « lordolibéralisme ».XXXÉquivocité du néolibéralisme Reprenant, quelque quarante ans après, le fil de ce cours, nous remettons ici en cause le découpage historique. Dabord, il nous semble que ce XXX · nest pas autour de la notion de marché quil faut attacher la genèse du libéralisme classique, mais plutôt autour de lidée dune société capable de sorganiser en dehors du prince. Ensuite, la rupture constituante du néolibéralisme se situe postérieurement à larrivée de Reagan et Thatcher au pouvoir, lorsquon aura théorisé et mis en pratique la financiarisation de léconomie comme instance ordonnatrice (cela donc après le cours de Foucault). Sagissant de lordolibéralisme, il présente de fortes continuités avec le libéralisme classique, même sil garde les marques dun mer- cantilisme qui sest développé tardivement en Allemagne. Il nest guère étonnant que ce courant se soit fondu si aisément dans le modèle social de marché auquel on associe davantage la social-démocratie allemande que le libéralisme débridé. Cela devrait aider à mieux caractériser ce quil faut entendre par « néo- libéralisme », un mot devenu équivoque. Si lon peut créditer Foucault dêtre parmi ceux qui lont inventé, cest plus chez lui par commodité verbale2. Lorsquil rédigea le résumé du cours au terme de son année, cest significativement le seul mot de « libéralisme » quil a retenu. Le cours bénéficie toujours dune forte aura. En effet, il est la seule incursion de Foucault dans lhistoire contemporaine ; il introduit le concept de gouvernementalité, qui a acquis une certaine place dans la science politique. Son style attire, mélange décrit et doral où la pensée se construit par bonds successifs et inattendus, avançant « en écrevisse » comme il le dit. Le texte déroute aussi parce quil ne cherche pas à construire un contre-modèle quand il analyse le courant intellectuel libéral. La dissection des textes saccompagne à lévidence chez lui dune certaine fascination. Il rabroue même son public quand celui-ci voudrait le voir glisser vers des objections trop faciles au libéralisme3. Il nest pas étonnant que des milieux se réclamant du libéralisme, y compris poli- tique, sen réclament presque autant que ses adversaires.XXX XXX Enfin et surtout, le texte se concentre pour lessentiel sur lAllemagne, un pays que Foucault connaissait très bien pour y avoir enseigné : « Dans cette seconde moitié du xxe siècle, le libéralisme est un mot qui nous vient dAllemagne. » Cest lordolibéralisme quil désigne comme « néolibéral ». Il traite assez peu, de façon surprenante, de ce quon appelle lÉcole de Chicago, que Foucault appelle lanarcho-libéralisme, née dans les années 1930, dont linfluence a été majeure dans le renversement idéologique opéré à lépoque de Reagan aux États-Unis : « Je ne suis pas sûr davoir le temps de parler des Américains. »XXXLâge classique Léconomie politique, à partir de Turgot et Smith, sest bâtie sur une critique du régime mercantiliste. Le mercantilisme, cest lidéologie dun État en constitution, qui organise lhégémonie du prince, qui pour cela capte des richesses, sorganise administrativement, privilégie la bonne collecte des impôts et lexportation, et où un ordre juridique se substitue au droit divin du souverain. Cette critique avait commencé sur le plan des idées politiques. Chez Locke et Spinoza, le citoyen naissait et la liberté politique était réclamée. Mais on ne touchait pas encore à lorganisation sociale du royaume. Le pas en avant fait par léconomie politique a été de donner toute sa place à la nouvelle classe des marchands, consciente désormais de contribuer à lenrichissement du royaume. On quittait une vision assez prédatrice, où léchange était essentiellement un jeu à somme nulle, où ce que gagnait un pays était une perte pour lautre et où le talent du prince consistait à ce que son pays se sorte bien de cette confrontation. La vision classique est inverse : il y a possibilité dun échange équitable, qui se fait fina- lement au bénéfice  « à lintérêt »  des deux parties4. Il y a possibilité dune croissance endogène où le marchand réinvestit son profit dans des activités nouvelles, une morale que le puritain anglais avait parfaitement intériorisée. Il sintroduit à plein la notion de concurrence qui gomme les situations de rente. Naissait dans la foulée la notion dintérêt général et de bien commun, davantage au centre des intérêts individuels dans la XXX · tradition britannique que lexpression dune souveraineté première dans la tradition française. Foucault décrit cette transition mais force le trait quand il indique, dans une phrase significative, que cet âge classique est celui où le marché devient un principe de régulation politique en remplacement de lordre juridique qui prévalait auparavant. Selon lui, cest désormais la légi- timité marchande qui non seulement limite mais organise et structure la décision publique. Elle devient le « lieu de véridiction » de laction gou- vernementale : « Ce lieu de formation de la vérité [], il faut le laisser jouer avec le moins dinterventions possibles pour quil puisse et formuler sa vérité et la proposer comme règle et norme à la pratique gouvernementale. Ce lieu de vérité, cest bien entendu non pas la tête des économistes, mais le marché 5. » Mais Foucault anticipe de près de deux siècles. Dune part, la prévalence du droit est plus manifeste encore à lépoque classique quà âge préclassique ; les structures de marché sapprofondissent et sappuient sur des contrats établis, le mot étant dailleurs repris dans la notion de contrat social qui naît à cette période. Dautre part, il ny a pas pour les classiques une rupture dans la conception du marché. Les prix auraient été avant cette période, dit Foucault, des prix ordonnés selon des critères déquité ou de stabilité sociale, des prix « justes » et non, comme postérieurement, des prix régis par la loi de loffre et la demande. Cest ce que dément une bonne part de lhistoriographie moderne, à partir dauteurs comme de Roover ou Todeschini6, pour qui le juste prix nétait rien dautre que le prix de marché, mais dun marché mis en position de bien fonctionner, une idée qui fera apparaître progressivement le concept de concurrence, très net chez les auteurs de la seconde scolastique dans lEspagne de la fin du xvie siècle, celui de monopole étant déjà ancien. La rupture, majeure, avec le mercantilisme existe, mais elle est que la société peut se libérer du prince, que léconomie peut fonctionner malgré sa dispersion, sans coordination venue den haut. La fameuse « main invisible », dans le seul passage de La Richesse des nations où Smith la mentionne, y est comme une image pour résoudre ce paradoxe dune XXX économie qui évite le chaos alors que les centres de décision (de pouvoir) sont dispersés, chacun deux ne prenant nullement en compte la décision des autres. Il sagit là dune notion commune qui exprime la différence entre la cause finale dune action et lintention des agents7. Sur ce point, nos économistes restent même en retrait du libéralisme politique dun Locke ou dun Montesquieu pour qui la société fonctionne non pas malgré sa dispersion mais grâce à une dispersion des pouvoirs et des centres de décision, qui devient lélément régulateur par lequel on atteint le bien commun. Dans ce contexte, le gouvernement prend une place très différente. La société connaît des contraintes et interactions multiples et lart de gou- verner consiste à les prendre en compte. Il sintroduit une rationalité différente, une rationalité des fins, que Foucault décrit très bien. La question posée est celle de lutilité ou de lefficacité des effets induits dune mesure. Mais ce qui est important Ce qui est important et moins et moins bien vu par Foucault est que bien vu par Foucault est que cette rationalité nouvelle peut justifier cette rationalité nouvelle peut autant lintervention publique que son justifier autant lintervention retrait. On pourra soutenir dans un cas publique que son retrait. que lautonomie de la société signifie que le gouvernement est en trop, quil perturbe lordre économique ou quil interfère de façon coercitive sur la liberté individuelle. Mais on peut alternativement affirmer que le gouvernement a en main des guides et préceptes, une rationalité économique, qui lautorisent et qui même le poussent à intervenir dans lordre économique. On voit poindre ici une bifurcation profonde, toujours actuelle, qui verra tout à la fois un Hayek libertarien ou un Keynes interventionniste se réclamer de la tradition économique libérale. À vrai dire, pour ce second courant libéral, on peut corriger la phrase de Foucault citée plus haut : autant que dans le marché, le lieu de vérité est dans la tête des économistes. Ils affichent déjà leur prétention en venant XXX · comme des ingénieurs sociaux, comme les Locke et Montesquieu lont été en matière dinstitutions politiques, devisant le bon mécanisme ou le bon montage. On le voit déjà chez Quesnay, un physiocrate qui précède Smith, dans ses réflexions sur ce que doit être un « bon impôt », cest-à-dire un impôt efficace, dans le sens de lintérêt bien compris du royaume. Turgot le sera plus encore. Et au siècle suivant, des économistes comme Cournot ou Bertrand introduiront lidée dun calcul économique aux fins dune utilité sociale maximale, et cela dans des domaines comme la décision de construire un pont, où le marché est absent et na rien à dire.XXX Lordolibéralisme en Allemagne Cest un sujet détonnement, mal couvert par les historiens, que le libé- ralisme ait eu une place si réduite dans la riche tradition intellectuelle allemande, Kant faisant bien sûr une immense exception. Remontant hardiment dans le temps, lune des explications peut tenir au luthéria- nisme dans sa version allemande, qui a été tout autant une école dindi- vidualisme face au divin que de soumission face au pouvoir temporel, un terrain peu propice à lélaboration dun pacte collectif donnant sa légitimité au pouvoir, comme dans la tradition anglaise ou française. Un autre facteur déterminant tient au retard allemand à construire son État national. Il la fait tout du long du xviiie siècle (sagissant de la Prusse) et du xixe siècle sous la houlette prussienne, avec un vif ressen- timent vis-à-vis des autres États européens déjà fortement constitués, et particulièrement de la Grande-Bretagne qui imposait son libéralisme à coups de libre-échange. Le romantisme, au début de ce siècle, en a été le pendant culturel, avec une forte dimension nationaliste, contredisant lidéal libéral des Lumières. Le Zollverein (une union douanière entre États allemands mise sur pied dans les années 1830) exprimait bien le vu des élites libérales à louest de lAllemagne, mais était dans la réalité un projet dessence protectionniste actionné par la Prusse et théorisé par Friedrich List comme laffirmation dune souveraineté propre. Il a échoué en tant que projet politique. Au fond, lAllemagne a connu son moment mercantiliste, mais avec un siècle et demi de retard. Ce sont les notions de richesse de lÉtat, de levées dimpôt, de contrôle de la monnaie, de commerce extérieurXXX devenu lenjeu dâpres batailles avec les pays voisins qui prévalaient. Un épisode intellectuel intéressant, qui allait préfigurer les tenants de llibéralisme et les distinguer des libéraux tant classiques quétats-uniens, a été celui des « sciences camérales ». Il sagissait décoles administratives mises en place par le roi de Prusse pour former le personnel capable dassurer la puissance du souverain, non par des moyens militaires (cela allait venir plus tard), mais par une gestion réglée du pays, par voie de « police », selon le mot retenu à lépoque et commenté longuement par Foucault. Cette tradition sest poursuivie à lépoque de Bismarck puis sous Weimar avec ce grand pré-keynésien qua été Rathenau. Les penseurs de lordolibéralisme ont tout à la fois bousculé cette tradition et en ont subi linfluence. Les deux grands noms sont Wilhelm Röpke et Walter Eucken, suivis par Ludwig Erhard, futur chancelier, qui a donné au mouvement sa consistance politique. Ils ont bâti toute lossature idéologique du Parti chrétien-démocrate allemand, sous le terme d« éco- nomie sociale de marché », un terme que le SPD allait adopter lui aussi, soucieux, devant la popularité de la notion, de ne pas être durablement exclu du pouvoir. Lordolibéralisme se caractérise dabord par un rejet de lintervention directe de lÉtat dans le jeu de léconomie : ni rôle stabilisateur, ni rôle redistributeur. Second principe, proche du premier, le gouvernant doit écouter ce que dit le système des prix, cest-à-dire linformation quapporte le fonctionnement dun marché concurrentiel. Mais il sagit ici dun choix presque autant forcé quidéologique. Tony Judt, dans son his- toire de laprès-guerre8, signale cet épisode déterminant qua été le début de la guerre froide. Les États-Unis ont imposé à la Grande-Bretagne et à la France de se réarmer. Mais bien sûr pas à lAllemagne. Or celle-ci gardait largement intact son appareil industriel de guerre, qui sest mis naturellement à tourner pour nourrir le reste des pays européens en produits venus de la mécanique ou de la chimie. Le modèle extraverti propre à lAllemagne était lancé, une voie quont trouvée plus tard le Japon et les autres pays asiatiques. Sappuyer sur la logique du marché international devenait alors lélément clé dune stratégie imposée par lenvironnement autant que par la doctrine. Notons le contraste avec la XXX · France : trente ans après, au moment où Foucault faisait son cours, il y avait encore des prix administrés. Mais écouter le système des prix suppose que le marché fonctionne bien, quil repose sur un socle approprié : une fixité de la monnaie dune part, et surtout une action délibérée de lÉtat pour imposer le principe de concurrence. Le marché nest pas un ordre transcendant que toute initiative de lÉtat irait perturber ; il y a au contraire lidée quil est fragile, quil y a une pente naturelle vers la formation de monopoles et de col- lusions, quil ne fonctionne pas naturellement sans un cadre très rigide que précisément lÉtat apporte9. Pour préserver la concurrence, il faut dailleurs encourager les entreprises familiales (le Mittelstand dont on sait aujourdhui le succès économique), lartisanat et une agriculture formée de petites exploitations. On nest pas surpris alors du compromis trouvé entre la France et lAllemagne au moment du traité de Rome : un pivot venu de Paris, à savoir la politique agricole commune quacceptaient de plus ou moins bon gré nos ordolibéraux, et, venue de Bonn, une solide autorité de la concurrence, avec un soutien plus mitigé de Paris. Ainsi, lÉtat intervient en amont, sur la structure plutôt que sur ses effets. Il pose la « règle », un mot toujours très fort pour les Allemands. La structure, ce sera davantage lentreprise que le consommateur, ce sera davantage le droit que les dispositifs économiques et fiscaux. Par cohérence, lordolibéralisme avait une vue très restrictive de ce que devait être lÉtat social, bien loin du projet bismarckien. Cela en raison du primat donné aux prix et à lentreprise. Une santé, une éducation et une culture socialisées ou gratuites, cétait intervenir à rebours puisquon niait lapport du système de prix dans lallocation des ressources. La protection sociale était pour eux laffaire des individus. Si lon devait aider, cétait uni- quement par le jeu des revenus, en préservant les mécanismes de marché. Cela na bien sûr pas résisté aux contraintes politiques du moment, sous linfluence notamment du SPD, de la tradition bismarckienne et des milieux catholiques, si lon se rappelle linfluence quavait eue longuement le premier parti social-chrétien dEurope, le Zentrum, né en 1870. Mais, à elle seule, la concurrence ne saurait suffire. Röpke, cité par Foucault, le disait avec force : « Ne demandons pas à la concurrence plus quelle XXX ne peut donner. Elle est un principe dordre et de direction dans le domaine particulier de léconomie de marché et de la division du travail, mais non un principe sur lequel il serait possible dériger la société tout entière. Moralement et sociologiquement, elle est un principe dangereux, plutôt dissolvant quunifiant. Si la concurrence ne doit pas agir comme un explosif social ni dégénérer en même temps, elle présuppose un encadrement dautant plus fort, en dehors de léconomie, un cadre politique et moral dautant plus solide 10. » Foucault use de litote quand il interprète cette phrase comme une « ambiguïté » du libéralisme à lallemande. On voit ici la différence avec lautre versant, disons keynésien ou acti- viste, dans la bifurcation libérale mentionnée plus haut. Le camp activiste voyait le rôle de lÉtat en creux en quelque sorte, dans les défaillances du marché quil fallait compenser, nhésitant pas à se substituer à lui sil le fallait. Lordolibéralisme insistait au contraire pour une action de lÉtat en amont, consistant à créer les conditions par lequel le marché continuerait à jouer pleinement son rôle, pour éviter les interventions en aval. Ce débat persiste pleinement aujourdhui. Le libéralisme des écono- mistes américains, pour y venir, était plus extrême : cest parce quil y a inévitablement des défaillances de lÉtat quil faut y substituer le marché.XXXLÉcole de Chicago Foucault ne traite que cursivement des représentants de lécole améri- caine du libéralisme économique, dont Frank Knight, Henry Simons, George Stigler et, plus tard, Milton Friedman. Ces économistes allaient transposer dans lordre social, en le poussant à lextrême, une autre tradition économique libérale, née dans les années 1870, appelée néo- classique ou marginaliste. Dans un marché bien réglé, selon ce courant, les prix, le taux dintérêt ou les salaires simposent de façon transcendante aux entreprises et aux ménages. Ceux-ci sont immergés dans un monde dont les paramètres leur échappent. Ils reçoivent des informations externes et y répondent, méca- niquement, en ajustant leur comportement. La règle du profit maximum nest quune règle de survie de lentreprise. Lindividu est représenté via un modèle très sommaire, lhomo conomicus, égoïste et optimisateur, dont on ne trouve pas la moindre trace chez les pionniers du libéralisme 1XXX · économique, pas plus que chez les fondateurs de lordolibéralisme ou dailleurs chez Hayek. Le système des prix est comme le système nerveux, celui qui transmet les informations aux agents, qui les motive et les stimule. Chez ces libéraux américains, toute perturbation à son endroit est a priori néfaste. Par exemple, une organisation en syndicats ou un salaire minimum, parce quils sortent du marché « libre », se retournent finalement contre les travailleurs (et les plus modestes, pour faire bonne mesure) en créant du chômage. On voit la différence avec Adam Smith, qui recommande que les travailleurs sunissent dans la négociation sala- riale face à des patrons qui ont toute facilité, vu leur faible nombre, dorganiser la collusion entre eux11 ; de même quavec lordolibéralisme allemand qui, dès 1951, promulguait les premières lois daprès-guerre sur la codétermination et les comités dentreprise12. Les agents étant emmaillotés dans un tissu complexe dincitations exo- gènes, leur autonomie est extraordinairement réduite. Foucault disait de lâge classique que « le nouvel art gouvernemental consomme de la liberté ». Ici, il ny a plus une once de liberté. Lindividu est comme une molécule réa- gissant, selon des lois doptimisation, aux impulsions externes fournies par le marché. Léconomiste devient celui qui dit : « Incitations ! » La gouvernementalité par le marché est radicale et lanalyse de Foucault commence ici à prendre son sens : « L homo conomicus, cest celui qui est éminemment gouvernable. » Les règles de marché deviennent les étalons dune bonne action publique ; elles en donnent les codes. Foucault introduit ici une distinction entre ce quil appelle le sujet de droit et le sujet dintérêt : le sujet dintérêt répond à des incitations venues de lextérieur et renvoie rationnellement sa réponse ; le sujet de droit est mû par des motivations intrinsèques13. Par exemple, un interdit légal, assorti dune sanction ou dune peine, est un interdit pour le sujet de droit, mais un coût pour le sujet dintérêt. Le délinquant chez Gary Becker, un économiste de Chicago, calculera les avantages et les coûts de son acte (dont lamende ou la prison) et prendra sa décision en conséquence (et en cela nest pas délinquant, ou alors nous le sommes tous).XXX 1XXX On retrouvera toutefois au sein de cette école américaine la bifurcation décrite plus haut entre libertariens et activistes. On nabandonnera pas forcément lingénierie sociale chère à léconomiste, mais imbriquée dans lordre du marché : sil y a, par exemple, une discrimination par largent dans laccès à léducation, on distribuera des coupons pour permettre aux gens de tous les milieux de se présenter à lécole privée de leur choix, préservant ainsi la concurrence. Friedman recommandait le revenu uni- versel de base. Un courant intéressant a aussi émergé sous la désignation de market design, à savoir comment structurer un marché afin quil réponde à certains objectifs de politique économique ou sociale, un marché censé donc être asservi à la cause gouvernementale, à faire délibérément partie de sa panoplie dinstruments14. La relation État-marché est à deux voies. Si dailleurs les individus sont soumis aux incitations et quun Léviathan apprend à bien les manier, tout lui devient possible. Un pas sera franchi quelque dix ans après le cours de Foucault : celui de la montée en régime dun nouvel « espace de véridiction », le marché financier. Le voici qui, mieux que le marché des biens et du travail, pourra allouer les ressources au sein de léconomie et répartir le risque, avec des frictions minimales. Cest la valorisation en continu des actifs sur les marchés financiers qui est le juge de paix, y compris dans lallocation de capital aux entreprises, y compris, prétend-on, dans la gestion des entreprises. Sil faut caractériser en quelques mots ce quest le néolibéralisme dans le champ économique, on dira quil est laddition du primat donné aux messages des prix, de la mise en retrait de lÉtat, de la plus grande fluidité donnée aux marchés financiers et de linstrument-marché pour remédier aux défaillances que le marché peut entraîner et que laction de lÉtat entraîne à coup sûr. On est très loin alors du libéralisme classique, très loin aussi de sa variante allemande daprès-guerre, lesquels nous donnent, à nen pas douter, de meilleures pistes pour affronter les défis de léco- nomie globale de demain.XXX 1XXX · Powered by TCPDF (www.tcpdf.org)&quot; #On découpe en paragraphes t&lt;- str_split(t, &quot;XXX&quot;,simplify = TRUE) t2&lt;-as.data.frame(t(t)) Plus les textes sont standardisés et plus simple est le processus dimportation des pdf. Si lon souhaite aller plus loin on recommande par exemple https://ropensci.org/blog/2018/12/14/pdftools-20/ pour extraire un tableau. ( à développer en 4 ou 5 lignes avec des références) 4.1.2 la numérisation et lOCR Dimmenses archives sont numérisées, ce qui signifie quon en a prise une image. Linformation est contenu dans les pixels, et lenjeu est de reconnaitre parmis eux des formes caractéristiques : alphabet, ponctuation à travers de multiples variations. Les plus fortes sont celles manuscrites, mais lécriture typographique est aussi très variables dans ses formes. Cest un enjeu industriel anciens. La reconnaissance optique des caractère a cependant fait dimmense progrès et atteint des niveau de performance élevé. Le traitement des adresses a sans doute été le problème principal qui a stimulé les technologies de lOCR. La qualité du matériaux est essentiel, et sassurer que les expéditeurs choisissent un modèle conventiel et normé de rédaction de la ladresse est une condition de leur succcès. La situation idéale ressemble à ceci. Modèle de rédaction correcte dune adresse postale La réalité ressemble souvent à celà à çà Dans un environnement en science sociale la situation est moins complexe, les documents analysés ne seront le plus souvent pas des documents mansuscrits ( sauf pour les médiévistes), mais le scan de document plus structurés. Par exemple les jpg Une solution pour r est tesseract. Cest un package qui permet daccéder au programme du même nom, développé à lorigine Chez Hewlett-Packard Laboratories entre 1985 et 1994, avec quelques modifications supplémentaires apportées en 1996 pour le portage sur Windows, et sur C en 1998.Tesseract a été mis en open Source par HP En 2005. Et de 2006 à novembre 2018, Google a continué à le développer. Il sappuie sur des réseaux neuronaux de type LSTM. Cest une petite mais puissante intelligence artificielle qui supporte plus dune centaine de langues. Testons-le sans attendre avec le texte suivant. Extrait du premier article du premier numéro de la revue \" Etalages\" Publiée en France de 1909 à 1938. Limage est un extrait du document numérisé fournit par la BNF. Lettre de motivation https://gabriben.github.io/NLP.html#introduction #library(tesseract) tesseract_download(&quot;fra&quot;) #pour télécharger le modèle de langage ## [1] &quot;C:\\\\Users\\\\33623\\\\AppData\\\\Local\\\\tesseract4\\\\tesseract4\\\\tessdata/fra.traineddata&quot; t1&lt;-Sys.time() text &lt;- tesseract::ocr(&quot;./images/N1_avril1909b.jpeg&quot;, engine = &quot;fra&quot;) t2&lt;-Sys.time() t&lt;- t2-t1 #pour compter le temps de calcul cat(text) #pour afficher le texte avec sa mise en page ## En fondant la Revue Internationale de ## l&#39;Etalage, nous avons cédé à une ambition ## qui. peut s&#39;exprimer en trois mots : Faire ## uvre ulile. ## ## Faire uvre utile en facilitant la tâche du ## commerçant détaillant par une documen- ## tation sérieuse et raisonnée se rapportant aux ## choses de sa profession, et notamment aux ## meilleurs procédés pour attirer la clientèle. ## ## Faire uvre utile surtout en vulgarisant ## pratiquement lart appliqué à l&#39;Etalage. ## ## Si notre carrière de publiciste s&#39;honore sans ## fausse modestie davoir doté dautres corpo- ## rations de revues professionnelles qui rendent ## des services, nous devons déclarer en toute. ## sincérité que la fondation d&#39;aucune de celles- ; ## ci ne répondait à un tel besoin. ## ## Aussi est-ce dun pied ferme que nous ## nous engageons dans le vaste champ dac- ## tions qui s&#39;ouvre devant nous. ## ## L&#39;Etalage!.. Quel sujet intéressant et ## captivant au premier chef. N&#39;est-il pas vrai- ## ment incompréhensible qu&#39;aucuñe plume ## autorisée nait jamais tenté, en France, d&#39;en ## établir les règles, den vulgariser les principes. ## Et pourtant, en envisageant la chose à un ## point de vue général, ne peut-on pas dire que ## l&#39;Etalage est un des ornements de nos cités ## qui met sans cesse en évidence, aux yeux ## de tous, la supériorité du goût et les apti- ## tudes artistiques dun pays ! Tandis quau ## point dé vue de leurs propres intérêts, nos ## lecteurs ne doivent-ils pas considérer lEta- ## lage comme l&#39;aimant qui attire les affaires ! ## ## On le comprend si bien aujourdhui que le ## commerçant qui se désintéresse de son éta- ## lage est une exception appelée de plus en ## plus à disparaître. ## ## Et cependant, parmi ceux qui se rendent ## un compte exact de limportance du rôle de ## VEtalage dans les affaires de détail, combien ## peu possèdent à fond lart de présenter leurs ## produits sous leur meilleur jour et de la façon ## la plus favorable. È ## ## Quoi de surprenant dailleurs à cela. En #tesseract_info() #voir les langues disponibles t ## Time difference of 2.193058 secs Pour améliorer la performance qui peut se mesurer au niveau des lettres mais doit surtout lêtre au niveau des mots, deux stratégies sont possibles, la première de préprocessing, la seconde de postprocessing avec un mécanisme de détection et de correction derreur. Le preprocessing consiste à traiter limage en renforçant les contrastes , en éliminant le bruit, on en rend les pixels mieux digestes pour tesseract. Cest ce à quoi sattache le pakage magick qui offre un bouquet de fonctions à cette fin. Nous laissons le lecteur le tester seul. Le post-processing un introduire des mécanismes de correction derreurs au niveau des mots.Pour une idée de ce type de développement voir Gabriel, Yadir, Xiaojie, Mingyu Naturellement, un paramètre important est la vitesse de traitement des images. Dans un projet complet on peut être amener à traiter des centaines images en boucle. Dans notre exemple la durée est de t secondes, autrement dit 6 images à la minute ou 360 à lheure 4.1.3 Du speech au texte La tradition méthodologique de la sociologie est celle de lentretien, avec toute sorte dacteur. Elle aboutit à la production de transcriptions, plus ou moins détaillées et précise. Mais des textes On peut désormais enregistrer da paroles des interfaces vocales. le speech to text est de plus en plus efficace, voir lAPI de google. Il existe déja des packages sur r qui permettent daccéder aux solutions de google langage qui necéssite une clé dAPI. https://cran.r-project.org/web/packages/googleLanguageR/vignettes/setup.html On ne fait quentre-ouvrir le sujet, mais il est certainement un des futurs du NLP. 4.2 Lexploitation de base de données textuelles 4.2.1 le cas europresse On commence par un exemple simple en utilisant la base europresse. lobjectif est de constituer un fichier de références bibliographiques, exploitable via r. Dans europresse , nous avons fait une recherche sur les articles comprenant le terme \" vaccination\" dans la presse nationale françaises, constituées de 14 titres. On retient les 150 derniers articles au 16 Juillet 2021. On utilise revtools pour sa fonction dimportation des fichier *.RIS et de transformation en data frame, library(revtools) df &lt;- read_bibliography(iconv(&quot;./data/20210719013820.ris&quot;)) flextable(head(df,3)) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-eb962b02{border-collapse:collapse;}.cl-eb8bc770{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-eb8bee6c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-eb8c3c3c{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-eb8c3c3d{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-eb8c3c3e{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} labeltypetitleauthorabstractjournalpagesyearlanguageurlDAOPBy.Cecilia.Kang_2021_TheNewYorTimNEWSFacebook Says Biden Is Scapegoating Over Vaccine FalsehoodsBy Cecilia KangThe social network and the Biden administration have engaged in an increasingly rancorous war of words over the issue of vaccine misinformation. WASHINGTON -- Facebook and the Biden administration engaged in an ...The New York TimesB 32021Englishhttps://nouveau.europresse.com/Link/PARIS10T_1/news%c2%b720210719%c2%b7NY%c2%b772161119/07/202119/07/2021By.Matt.Stevens_2021_TheNewYorTimNEWSRules for Audiences Can Spin HeadsBy Matt StevensVaccination and mask requirements vary by venue. It's a weird pandemic summer for the performing arts. During its preview performances in June, New York Classical Theater was allowed to put ...The New York TimesC 12021Englishhttps://nouveau.europresse.com/Link/PARIS10T_1/news%c2%b720210719%c2%b7NY%c2%b745642019/07/202119/07/2021By.Lisa.Lerer_2021_TheNewYorTimNEWSThe Republican Path From Warp Speed Praise To Vaccine OppositionBy Lisa Lerer... 'll try to answer it. Have a comment? We're all ears. Email us at &lt;occ.email&gt; onpolitics@nytimes.com&lt;/occ.email&gt; or message me on Twitter at @llerer . By the numbers: $15 billion ... That's roughly the ...The New York TimesA 172021Englishhttps://nouveau.europresse.com/Link/PARIS10T_1/news%c2%b720210719%c2%b7NY%c2%b771554919/07/202119/07/2021 df&lt;-df%&gt;% mutate(jour=substring(DA,1,2)) g22&lt;-ggplot(df, aes(x=jour))+ geom_bar()+labs(x=NULL,y=&quot;Fréquence&quot;)+ geom_vline(xintercept=12, linetype=&quot;dashed&quot;, color = &quot;red&quot;)+ facet_grid(vars(journal)) g22 # screen_topics() revtools nest pas fait que pour importer des données au format bibliographique .ris, ou au format .bib, et de les transformer un tableau observations - variables ( bref, un dateframe). Il a des fonctions de visualisations rapides fort efficace. La plus spectaculaire est un outil de visualisation qui sappuie sur deux modèles de détections de topics (ce sujet sera lobjet du chapitre 8), paramétrables de manière interactive en quelques minutes, et conçu avec shiny, le package star des graphes interactifs. Cest un super outils pour avoir un premier coup doeil sur les données, un plug in super pratique. On lapplique sur nos données. Lallure de linterface est la suivante. screen_topic Linterface nétant pas programmatique, on exporte quelques images en jpeg (un bouton dans linterface permet de faire celà sans effort) et on les récupère avec cowplot, le package qui permet dassembler des graphes et que nous utiliserons systématiquement dans ce cours. p1 &lt;- ggdraw() + draw_image(&quot;./images/topic_espace.png&quot;) p2 &lt;- ggdraw() + draw_image(&quot;./images/topic_topic.png&quot;) p3 &lt;- ggdraw() + draw_image(&quot;./images/topic1.png&quot;) p4 &lt;- ggdraw() + draw_image(&quot;./images/topic5.png&quot;) plot_grid(p1, p2 , ncol=2) Et pour illustrer les graphiques des termes les plus proches du topic 1 et du topic 5. Lun est relatif à lactualité austrlienne, lautre à lactualité anglaise. plot_grid(p3, p4 , ncol=1) La méthode est sympa, rapide, sur le pouce, mais pas suffisante pour aller au-delà et noatmment comparer les lignes éditoriales des deux titres que nous avons choisis. A ce stade de lanalyse cest déjà beaucoup. 4.2.2 Jouer avec les bases bibliographiques Fulltext https://books.ropensci.org/fulltext/data-sources.html 4.3 Lire le web : Scrapping Le scrapping correspond à un internet sauvage où la collecte dinformation se traduit par une technique de chasseurs-cueilleurs, le glanage. cest lactivité qui consiste à moissonner les informations disponibles sur le net en simulant et en automatisant la lecture par un navigateur ( on préfère lexpression des quebecois : des butinuers). Elle consiste à construire un robot capable de lire et denregistrer les informations disponibles sous forme html puis à les distribuer (parsing) dans des tableaux structurés, selon une stratégie dexploration du web préalablement définie. En réalité le scrapping pose deux problèmes : celui de la structure de recherche. Cest le problème que relève les spiders, des robots qui recherchent dans les pages des liens, et vont de proche en proche, de lien en lien, pour explorer un domaine.Ils peuvent être plus systématique et prendre avantage de lorganisation dun site web pour enummérer les pages. celui de la collecte de linformation sur chacune des pages. Il sappuie sur le principe que le langage html est un langage à balise où le contenu et le contenant sont clairement séparés. Par exemple, dans le corps de texte dune page on définira un titre par la balise dont linstruction sachève par la balise . On sépare ainsi clairement le contenu de la forme. ` Un titre de niveau 1 (un gros titre) &lt;p&gt;Un paragraphe.&lt;/p&gt; &lt;h2&gt;Un titre de niveau 2 (un sous titre)&lt;/h2&gt; &lt;p&gt;Un paragraphe.&lt;/p&gt; &lt;h3&gt;Un titre de niveau 3 (un sous-sous titre)&lt;/h3&gt; &lt;p&gt;Etc.&lt;/p&gt; ` Ultérieurement on pourra définirs les propriété graphiques dune balise par des CSS. par exemple avec ceci les paragraphes seront publiés en caractère bleu. p{ color: blue; } Ce qui nous intéresse nest pas la décoration, mais le fait que les développeurs définissent des balises spécifiques pour chacun des éléments de leurs page web, et que si nous savons les repérer , nous avons le moyen de mieux lire le texte. Les balises sont la cible du scrapping 4.3.1 rvest avec r De nombreuses ressources sont disponibles, mais pour en rester à r , le package rvest permet de réaliser des extractions simples mais suffisantes pour de nombreux usages. une application rvest https://www.r-bloggers.com/2018/10/first-release-and-update-dates-of-r-packages-statistics/ le package rvest est générique https://community.rstudio.com/t/scraping-messages-in-forum-using-rvest/27846/2 library(rvest) # Scrape thread titles, thread links, authors and number of views start &lt;- &quot;https://uberzone.fr/threads/si-la-vaccination-devient-obligatoire-vous-feriez-vous-vacciner-ou-changeriez-vous-de-corps-de-metier.17425&quot; x&lt;-c(&quot;/page-2&quot;, &quot;/page-3&quot;, &quot;/page-4&quot;) for (val in x){ url&lt;-paste0(start,val) h &lt;- read_html(url) post &lt;- h %&gt;% html_nodes(&quot;.bbWrapper&quot;) %&gt;% html_text()%&gt;% str_replace_all(pattern = &quot;\\t|\\r|\\n&quot;, replacement = &quot;&quot;) post #authors &lt;- h %&gt;% # html_nodes(&quot;.username--style2 &quot;) %&gt;% # html_text() %&gt;% # str_replace_all(pattern = &quot;\\t|\\r|\\n&quot;, replacement = &quot;&quot;) # Create master dataset (and scrape messages in each thread in process) master_data &lt;- tibble(post) rds_name&lt;-paste0(&quot;./data/df_&quot;,substr(val,2,6),&quot;.rds&quot;) saveRDS(master_data,rds_name) } head(master_data) ## # A tibble: 6 x 1 ## post ## &lt;chr&gt; ## 1 &quot;Je comprends pas pourquoi persistez-vous à vouloir convaincre alors que vous~ ## 2 &quot;Shibani a dit:Je comprends pas pourquoi persistez-vous à vouloir convaincre ~ ## 3 &quot;*****\\&quot;Celui qui ne pète pas et ne rote pas explose\\&quot;*****&quot; ## 4 &quot;mez a dit:Ta cirrhose et ton Cancer du poumon (je te les souhaite pas faut ~ ## 5 &quot;Shibani a dit:Et puis sache que comme je lai déjà dit tu peux acheter ton p~ ## 6 &quot;*****\\&quot;Celui qui ne pète pas et ne rote pas explose\\&quot;*****&quot; 4.3.2 Des problèmes pratiques, juridiques et éthiques La pratique du scrapping se heurte dabord à une question technique. ce nest pas un excercice facile, et il doit être confier à des spécialistes. Il se heurte aussi à différents problèmes dordre éthique et juridique. Si la pratique nest pas interdite en tant que telle, elle se confronte à différents droits et principes éthiques En termes pratiques, le scrapping crée des risques pour les sites : Le risque de deny of service, cest à dire de saturer ou de parasiter un système et de sexposer à ses contre-mesures. Il contribue à la complexification du web, et implique une consommation excessive de ressources energétiques. Et des risques pour la qualité dU recueil de données Le risque dinformation parcellaires, tronquées, inexactes qui résultent de ces contre-mesures. Les producteurs développent des stratégies moins naives. Lexemple des pages numérotée par ordre de production auxquels on substitue un nombre au hasard pour annihilier linformation temporelle. le risque matériel de mal lire les informations, pour des raison dencodage approximatifs. En termes de droits même les conditions légales relèvent de différents droits : De la propriété intellectuelle, Du respect de la vie privée, Du droit de la concurrence qui sans linterdire, condamne la copie laissant espérer quune transformation des données fasse quil y échappeR. Cependant des facilités et tolérances sont souvent accordées quand cest dans un objectif de recherche et que des précautions minimales danonymisation ou de pseudonymisation sont prises, et que les règles de conservation et de destruction des données sont précisées. En termes éthiques Un principe éthique essentiel dans la recherche, et ailleurs, et de ne pas nuire à la soci2té dans son ensemble, hors cette technique participe à la robotisation du web (plus de 50% du trafic résulterait de la circulation des spi.ders , scrapers, sniffers et autres bots, comme dans la forêt une éthique écologique revient à préveler le minimal nécessaire pour létude entreprise 4.4 Limportance croissante des API Les API doivent être considérées comme la voie normale daccès à linformation, du moins en droit. Elles relèvent du contrat. Le recours aux APIs est civilisé, ne serait-ce parce quon introduit une sorte détiquette, des règles de courtoisie, un système de reconnaissance réciproque et dattribution de droits. Sur le plan méthodologique elles présentent davantage de donner aux requêtes un caractère reproductible , mêmes si les bases visées peuvent varier. Elles asurent une grande fiabilité des données. Lutilisation dAPI lève lambiguïté légale qui accompagne le scraping et peut ainsi paraître comme plus civilisée. Elle nécessite naturellement que le gestionnaire de la base de données fournisse les moyens de sidentifier et de requêter, elle peut avoir linconvénient dêtre coûteuse quand laccès est payant, ce qui sera de plus en plus le cas. 4.4.1 Un tour dhorizon La plus part des grandes plateformes offrent des API plus ou moins ouvertes, examinons-en quelques une pour comprendre plus clairement leur intérêt méthodologique. On va se concentrer sur trois exemples : le firehose de tweeter, lapi de google maps, la Crunchbase. Twitter nest pas quun réseau social, cest une gigantesque base de données qui enregistre les engagements et les humeurs de 500 millions dhumains à travers la planète et les centres dintérêt. Elle permet potentiellement de saisir les opinions à différentes échelles géeographique et temporelle, y compris les plus locales et les plus courtes. Elle a le défaut de souffrir fortement de biais de sélection, le premier étant le biais dengagement. Les passionnés dun sujets parlent plus que les autres, une parôle mieux contrôlée. Le cas de Google maps est passionnant à plus dun égard. le premier dentre eux est que dans leffort dindicer chaque objet de la planête, la base de données devient un référentiel universel, plus quune représentation intéressée du monde. Quand lutilisateur communs cherche un chemin optimal, lanalyste de donnée trouve un socle pour ordonner le monde. La Crunchbase construite par le média Techcrunch repertorie les créations de start-up et les levées de fonds quelles ont obtenues. Elle recence les dirigeants, les acquisitions, décrit les business model. intégrité des bases de données, universalité des élément, interopérabilité, disponibilité Les problèmes posés : justesse , précision et représentativité. leur constitution nest pas aléatoire, leurs couverture reste partielle. accessibilité, la privatisation du commun. Si pour le chercheur les APIS sont sur un plan de principe une merveille sur un plan plus social elle instaure des inégalités daccès énormes aux données qui permettent de valoriser la connaissance. Ce mécanisme opère via deux canaux. Le premier est celui de la tarification qui ségrège les chercheurs en fonctions des ressources dont ils disposent. Le second passe par la couverture du champs, les données les plus précises et les plus denses se trouvent dans les régions les plus riches. des catégorisations peu délibérées 4.4.2 un point de vue plus technique https://www.dataquest.io/blog/r-api-tutorial/ 4.4.3 Un exemple avec Rtweet https://cran.r-project.org/web/packages/rtweet/vignettes/intro.html Plusieurs packages de r permettent dinterroger le firehose ( la bouche dincendie!) de twitter. https://www.rdocumentation.org/packages/rtweet/versions/0.7.0 Lauthentification ne nécesssite par de clé API, il suffit davoir son compte twitter ouvert. Cependant la fonction lookup_coords requiert davoir une clé dapi ou google cloud map. Elle permet de selectionner sur un critère géographique. https://developer.twitter.com/en/docs/tutorials/getting-started-with-r-and-v2-of-the-twitter-api #une boucle pour multiplier les hashtag x&lt;-c(&quot;#getaround&quot;,&quot;#Uber&quot;, &quot;#heetch&quot;) for (val in x) { tweets &lt;- search_tweets(val,n=20000,retryonratelimit = TRUE)%&gt;% #geocode = lookup_coords(&quot;france&quot;) mutate(search=val) write_rds(tweets,paste0(&quot;tweets_&quot;,substring(val,2),&quot;.rds&quot;)) } df_blablacar&lt;-readRDS(&quot;./data/tweets_blablacar.rds&quot;) df_uber&lt;-readRDS(&quot;./data/tweets_uber.rds&quot;) df_heetch&lt;-readRDS(&quot;./data/tweets_heetch.rds&quot;) df&lt;-rbind(df_blablacar,df_uber ) ls(df_blablacar) foo&lt;-df %&gt;% select(account_lang, geo_coords,country_code, country, account_lang,place_name) On laisse le lecteur explorer les différentes fonctionnalités du package. On aime cependant celle-ci qui sample le flux courrant au taux annoncé de 1%. Voici lextraction de ce qui se dit en france pendant 10 mn (600s). La procédure peut donner une sorte de benchmark auquel on peut comparer une recherche plus spécifique. rt &lt;- stream_tweets(lookup_coords(&quot;france&quot;), timeout = 600) 4.4.4 Un autre exemple google map serait bien mais api fermée, il en faut une ouverte. IMDB ? 4.5 Conclusion Dans ce chapitre nous aurons égratigné des sujets techniques de constitution de corpus en envisageant différents moyens dacccès Scrapping API Pdf texte dans les images une ouverture à loral On soulignera la technicité On observera létendue des domaines à exploiter. "],["explorer-et-visualiser-le-corpus.html", "Chapitre 5 Explorer et visualiser le corpus 5.1 Kwic 5.2 Explorer le corpus", " Chapitre 5 Explorer et visualiser le corpus Un des meilleur conseils quon puisse donner, est de lire soi-même le texte avant de le confier aux machines.Comme dans le cas de grand corpus, il est difficile den lire lensemble, certains outils plus ou moins interactif, permettent de se donner rapidement une idée des contenus. 5.1 Kwic Le premier réflexe dans la lecture dun corpus est de chercher dans quels contextes sont utilisé des mots cibles. Cest lobjet dune vieille technique les : Key word in context. exemple 5.2 Explorer le corpus Avant de procéder aux analyses du corpus, il est souvent utile de le représenter. On va utiliser le package Corpora explore à cette fin. Il permet de préparer un corpus et de le visualiser de manière interactive avec la génération dune app shiny. Malheuresuement nous ne savons pas rendre compte de la dynamique de loutil. On peut naviguer aisement dans lensemble de texte. On va utiliser une collection de donnée préparée avec Manel Benzarafa de lUniversité paris Nanterre, et qui comprend lintégralité des résumés, auteurs etc.. relatifs aux articles publiés dans PMP. Une base bibliographique intégrale. 1025 articles la compose. library(readr) #install.packages(&quot;corporaexplorer&quot;) library(corporaexplorer) library(readr) PMP &lt;- read_csv(&quot;data/PMPLast.csv&quot;) PMP&lt;-PMP %&gt;% select(Key, Author, Title, Issue, 3, 11) PMP&lt;-PMP%&gt;% rename(Text=6, Annee=5) %&gt;% filter(Text!=&quot;Null&quot; &amp; !is.na(Annee)) corpus &lt;- prepare_data(PMP, date_based_corpus =FALSE, grouping_variable = &quot;Annee&quot;, # change grouping variable within_group_identifier = &quot;Title&quot;, columns_doc_info = colnames(df)[1:4], tile_length_range = c(2, 10), use_matrix = FALSE ) #explore(corpus) Dans la photo décran suivante, on teste les termes \" politique\" et management. Chaque tuile ( tile) représente un des 1025 abstracts qui composent le corpus. Les couleurs correspondent à la fréquence des deux termes. Exploration des abstracts de PMP 5.2.1 reprendre le topic de revtools "],["préparation-des-données.html", "Chapitre 6 Préparation des données 6.1 Manipuler des chaines de caractères 6.2 Nettoyer le texte 6.3 Corriger le texte 6.4 Identifier les sources", " Chapitre 6 Préparation des données Avant de se lancer dans lanalyse, il est nécessaire de préparer le texte, de le pré-traiter. Son format fondamental est celui dune chaine de caractères, sans signification particulière mais composé à partir dun alphbat, cun jeux de signes déterminés et démobralement. 1/0 pour lelangage bianre, AGCP pour ladan, 26 caractère de base pour lalphabet, sans compter les accents. Ces variations sont lobjet de convention en informatique. et de certaines opérations. traiter du texte cest avant tout disposer dopérateurs pour manipuler ces éléments élémentaire. la base est davoir des outils pour les manipuler. Le langage avant dêtre signifiant est signifié, littéralement produit comme une chaine de signes qui dans lusage suit certaine convention. Par exemple la satisfaction peut sexprimé par mmmm, une forte satisfaction par un mmmmmmmmmmmmmmmmmmmmm. Pour distinguer les significations, il faut dabord compter. les mmm sont sans doute courants car conventionnels (ce mot est à deux doigts dêtre incorporé au dictionnaire de lAcadémie Française, sil nétait quune onomatopée), les mmmmmmmmmmmmmmmmmmmmm sont sans doute beaucoups plus rares. De plus on trouvera des hum des hummm, des mmmmhummmm\". On comprend quà la nuance de lintensité que le locuteur veut exprimer, toute ces morphologies se rapportent à une même idée. Comment le rammener à une même formes est une question essentielle même si elle semble excessivement technique. 6.1 Manipuler des chaines de caractères Il faut donc traiter le texte, avant même de sengager dans des modèles compliqués. Il faut savoir traiter des chaines de caractères pour en réduire la diversité, et en produire des chaines grammaticalement exacte. Cest un travail dartisan, celui des des imprimeurs et de leurs coorecteur. Et en particulier dun métier celui du compositeur, ou ouvrier de la casse, qui distribue des caractères de plomb en séquences dans des casiers de bois. cmpositeur Lartisan navait pas de choix, la précision était essentielle pour éviter la coquille. Le texte moderne, numérique, est lobjet de plus daller et retours. Les mots quon pianotent sont corrigés avant même dêtre frappés. Les gestes techniques sont différents mais sarticulent sur une même idée : la langue écrite, du moins les langues alphabétiques sont des chaines de caractères dont la formation suit des règles fluctuantes à travers lhistoire mais contraignante à chaque moments. Les conventiosn peuvent changer, mais dans son temps elle simposent définitivement. Personne nécrirait deffert, pour dire dessert. Et pourtant la graphie du s était un f jusquau XVI ème siècle (trouver la source)! https://www.cairn.info/revue-la-linguistique-2003-1-page-3.htm De nombreuses ressources sont disponibles pour traiter ces chaines de caractères. On utilisera surtout Stringer qui est est un des composants essentiels de tidyverse. Dautres packages sont équivalents : stringi par exemple. 6.1.1 Les opérations sur les chaînes de caractères mettre en minuscule. Lalphabet se présente au moins en deux versions : des majuscules et des minuscules, il est souvent nécessaire de réduire le texte à une seule casse pour en réduire la variété, sauf si les majuscules signalent une information spécifique et socialement conventionnelle. Un mot qui débute par une majuscule signale un nom commun, désormais conceptualisé comme une entité nommée appartenant à différentes catégories : noms de lieux, noms de personnes, noms dorganisation ou lexpression dun sentiment, au sein des chats, la majuscule en série signale un niveau de langage loud, un cri , une engueulade, la véhémence. rechercher une chaine de caractères; remplacer une chaine de caractères extraire une chaine de caractère dun emplacement à lautre supprimer une chaine de caractères. Les nombres, concaténer des chaines de caractères. Le texte peut être divisés en unités. Un paragraphe par exemple, ou un titre. Si la manipulation deslaquelle ? vaccin ? 6.1.2 La technique des expressions régulières (regex) Il ne suffit pas de chercher une chaine de caractère particulière, il faut souvent saisir un ensemble de variations qui suivent un motif determiné et qui répond à une sorte de loi générale. Par exemple si je veux retrouvrer dans un corpus lensemble des mots relatif au monde de lhôpital, nous chercherions aussi le mot hopital. Nombreux seront les locuteur qui omeetent laccent circonflexe. Une formule pour trouver ces deux varietés serait dutiliser un opérateur, \"(), pour définir une option . soit lun soit lautre : h(ô,o)pital Une expression régulière est un masque qui permet didentifier des formes principales et leurs variétés. Il sappuit sur une codification dont quelques éléments clés permettent de se donner une bonne idée de la logique générale le ^, indique que la forme commence par le caractère qui suit ^A le . signifie nimporte quel caractère. le regex ^a. signifiera ainsi nimporte quelle chaine de caractère qui commence par a est est suivi de nimporte quel caractère. le * la répétition indéfinie du caractère . Dun point de vue linguistique les regex travaillent sur la morphologie et ses variations, indépendemment des règles de grammaires mais profitant de leur régularité. Les mots sont généralement composés dune racine, de suffixe et de préfixe qui contiennent les flexions grammaticales et sémantiques. des exemples : la négation : visible et in-visible. la conjugaison : aime et aim-ât la numération : fraise et fraise-s le genre : épicier et épicière-s. 6.1.3 Un fondement profond et ancien Le langage des regex a répondu dabord aux besoin des informaticiens, et sappuie sur une construction mathématique sophistiquée : les automatates finis https://swtch.com/~rsc/regexp/regexp1.html don t un des contributeurs essentiels à été doi.org/10.1145/363347.363387 Ken Thompson fondateur de Grepl a method for locating specific character strings embedded in character text is described and an implementation of this method in the form of a compiler is discussed. The compiler accepts a regular expression as source language and produces an IBM 7094 program as object language. The object program then accepts the text to be searched as input and produces a signal every time an embedded string in the text matches the given regular expression. Examples, problems, and solutions are also presented. https://swtch.com/~rsc/regexp/regexp1.html 6.1.4 Des applications très pratiques et à ceux qui face à des questions de métier, par exemple les professionnel de marketing direct ou des services postaux, ont été amené à traiter de jeux de données textuels limités tel quune adresse postale. dectecter une entité nommée : la majuscule détecter une adresse détecter une date détecter un compte détecter une url 6.2 Nettoyer le texte enlever les mentions enlever les url enlever ou recoder les emojis enlever la ponctuation enlever les nombres 6.3 Corriger le texte Si certains corpus sont par les conditions de leur production presque parfait du point de vue grammatical et lexical, cest le cas en principe des articles de presse et des documents officiels, dautres qui sappuient sur une langue vernaculaire on des graphies plus incertaines et des syntaxes approximatives. Dans un tiers des cas le mot \" opinion\" sorthographie opignons. Chaque mot du lexique sévanouit dans des morphologies nombreuses et approximatives. Cest un obstacle à lanalyse car la variété morphologique est aléatoire. plusieurs stratégies sont possibles. La première est de corriger le texte notamment en employant des outils de corrections efficaces. 6.3.1 La correction orthographique automatique voir hunspell https://cran.r-project.org/web/packages/hunspell/vignettes/intro.html#Custom_Dictionaries 6.3.2 Analyse ciblée par les regex Une application des regex est lanalyse ciblée dun certain nombre de termes. LA corection est partielle mais couvre les cibles essentielles exemple des gestes barières dans le flux twitter 6.4 Identifier les sources Les acteurs : Des professionnels de la politiques et les institutions quils dirigent Journaliste et professionnels de la communication Experts et universitaires les marques et leur community manager les bots les trolls les activistes les acteurs de la vie politique 6.4.1 Identifier la langue Dans lanalyse des contenus sociaux, les textes viennent de sources multiples et confuses. Elles peuvent être aisément multilingue. Analyser un corpus dentretien, une collection de discours, pose peu la questions des locuteurs car ils sont bien identifiés. Ce nest pas le cas dans les réseaux sociaux où les buts sont multiples et plus ou moins avoués. Les corpus peuvent être multi-langues. Par exemple, dans les corpus davis dhôtes sur Airbnb, les avis sont formulés dans une large variété de langues. Il va falloir en tenir compte et une tâche préliminaire sera de détecter les langues pour séparer les corpus. Le package textcat offre une solution basée sur la fréquence des ngrams (des suites de 2, 3,  n motsquon étudiera dans le Chapitre 4) et qui compare la distribution du texte ciblé avec celles des distributions typiques des langues. (insérer le schema) Google propose un algo plus sophistiqué : cld3, car fondé sur un réseau de neurones assez profond. Comparons les. On utilise un jeu de donnée Airbnb à Bruxelles (été 2020), ville qui accueillant les institutions européennnes est une des plus cosmopolite qui soit avec des fonctionnaires venant de toute lEurope, sexprimant dans une large variété de langue, sans compter les représentations des autres pays du monde, publiques et privées. En terme de durée de calcul, la différence en temps de calcul est faramineuse : 7 secondes contre 7 minutes, ce qui sexplique car texcat sappuyant sur la distribution des ngrammes doit les calculer pour les 36000 observations que nous avons retenues. BXL2021 &lt;- read_csv(&quot;./data/reviewsBXL2021.csv&quot;) BXL2021$Year&lt;- as.numeric(format(as.Date(BXL2021$date, format=&quot;%Y-%m-%d&quot;),&quot;%Y&quot;)) BXL2021&lt;- BXL2021 %&gt;% filter(Year&gt;2019) # on filtre sur la période de confinement #library(cld3) t1&lt;-Sys.time() cld3&lt;-as.data.frame(detect_language(BXL2021$comments))%&gt;%rename(cld3=1) t2&lt;-Sys.time() t_cld3&lt;-t2-t1 #on calcule la durée de l&#39;opération en faisant la différence du temps de départ et d&#39;arrivée #library(textcat) t1&lt;-Sys.time() textcat&lt;-textcat(BXL2021$comments) t2&lt;-Sys.time() t_texcat&lt;-t2-t1 foo&lt;-cbind(cld3, textcat) Examinons les résultats et la distribution des langues identifiées par les deux systèmes. Si lordre est respecté, des différences sobservent, cld3 identifie du chinois qui ne fait pas partie du répertoire de texcat. g1&lt;-foo%&gt;%mutate(n=1)%&gt;%group_by(textcat)%&gt;%summarise(n=sum(n))%&gt;% ggplot(aes(x=reorder(textcat,n), y=n))+geom_bar(stat=&quot;identity&quot;)+coord_flip()+scale_y_log10() g2&lt;-foo%&gt;%mutate(n=1)%&gt;% group_by(cld3)%&gt;% summarise(n=sum(n))%&gt;% ggplot(aes(x=reorder(cld3,n), y=n))+ geom_bar(stat=&quot;identity&quot;)+coord_flip()+scale_y_log10() plot_grid(g1, g2, labels = c(&#39;Texcat&#39;,&#39;Cld3&#39;), label_size = 12) Examions maintenant la convergence des méthodes en représentant la répartition du résultat dun système dans les langue de lautre. Si la convergence est parfaite 1000% des textes classé en Français par Textact devrait se retrouver dans 100% de ces textes classé par cld3 et réciproquement. foo1 &lt;-foo %&gt;% mutate(n=1)%&gt;%group_by(textcat) %&gt;%summarise(n=sum(n)) foo1&lt;-foo%&gt;% left_join(foo1) %&gt;% filter(n&gt;10) table&lt;-table(foo1$cld3,foo1$textcat) foo1&lt;-as.data.frame(prop.table(table,2)) ggplot(foo1, aes(reorder(Var2, Freq),Var1)) + geom_tile(aes(fill = Freq)) + scale_fill_gradient(low = &quot;White&quot;,high = &quot;Blue&quot;)+ theme_bw()+ theme(axis.text.x=element_text(angle = 45, hjust =1))+coord_flip() foo1 &lt;-foo %&gt;% mutate(n=1)%&gt;%group_by(cld3) %&gt;%summarise(n=sum(n)) foo1&lt;-foo%&gt;% left_join(foo1)%&gt;%filter(n&gt;10) table&lt;-table(foo1$cld3,foo1$textcat) foo1&lt;-as.data.frame(prop.table(table,1)) ggplot(foo1, aes(reorder(Var1, Freq),Var2)) + geom_tile(aes(fill = Freq, label=Freq)) + scale_fill_gradient(low = &quot;White&quot;,high = &quot;Red&quot;)+ theme_bw()+ theme(axis.text.x=element_text(angle = 45, hjust =1))+coord_flip() table2&lt;-as.data.frame(table) %&gt;% mutate(Freq=log10(Freq+1)) %&gt;% pivot_wider(names_from = Var1, values_from = Freq) %&gt;% column_to_rownames( var = &quot;Var2&quot;) pheatmap(table2 , color = inferno(10)) chi2&lt;- chisq.test(table) chi2&lt;- as.data.frame(chi2$residual) table2&lt;-chi2 %&gt;% mutate(Freq=Freq^2)%&gt;% pivot_wider(names_from = Var1, values_from = Freq) %&gt;% column_to_rownames( var = &quot;Var2&quot;) pheatmap(table2 , color = inferno(20, direction=1)) 6.4.2 Identifier les plagiats et réutilisations Dans la définition du corpus il peut être utile de se concentrer sur les contenus originaux Une autre question est de detection les contenus riginaux des contenu réutilisés ou carrément plagiés. https://github.com/ropensci/textreuse https://journal.r-project.org/archive/2020/RJ-2020-017/index.html 6.4.3 Identifier les fakes https://github.com/sherylWM/Fake-News-Detection-using-Twitter 6.4.4 Identifier les trolls http://golovchenko.github.io/tutorials/snatrolls 6.4.5 Identifier les bots botometer botchecks pour un benchmark https://rpubs.com/xil865/528096 detecter les fakes https://blogs.rstudio.com/ai/posts/2020-08-18-deepfake/ "],["une-première-analyse-quantitative.html", "Chapitre 7 Une première analyse quantitative 7.1 Comptons les mots 7.2 la production dans le temps 7.3 Lisibilité et complexité lexicale 7.4 Comptons les mots 7.5 Conclusion", " Chapitre 7 Une première analyse quantitative Une première manière daborder un texte ou un corpus est volumétrique. Quel volume de texte? Quelle longueur ? Combien de mots ? quelles variations? A cette fin on utilise le cas des tweets de Donald Trump. Des premiers aux derniers, jusquau moment de son banissement en Janvier 2021, après sa défaite aux élections présidentielles. Chargeons le fichier de données. df &lt;- read_csv(&quot;./data/TrumpTwitterArchive01-08-2021.csv&quot;) nrow&lt;-nrow(df) #nombre de ligne ncol&lt;-ncol(df) #nombre de colonne 7.1 Comptons les mots Il y 56571 tweets et 9 variables. On peut vouloir compter le nombre de mots. A cette fin on emploie une fonction de stringr : str_count. (On reviendra sur la question de la manipulation des chaines de caractères dans un chapitre ad hoc) df$nb_mots&lt;-str_count(df$text, &quot; &quot;)+1 # l&#39;astuce : compter les espaces et ajouter 1, pour compter les mots sum_mots&lt;-sum(df$nb_mots) #ON COMPTE LE NOMBRE DE MOTS ggplot(df, aes(x=nb_mots))+ geom_histogram(fill=&quot;deepskyblue3&quot;)+ labs(title=paste0(&quot;Nombre total de mots du corpus : &quot;,sum_mots), x=&quot;Nombre de mots par post&quot;, y=&quot;Fréquence&quot;) Figure 7.1: Distribution du nombre de mots par post La bimodalité provient surement du changement de taille maximum effectué en septembre 2017, le passage de 180 caractères max à 280. On peut le vérifier en examinant cette même distribution - par les courbes de densité - pour chacune des années, avec cette technique rendue fameuse par la pochette de lalbum de Joy Division : un graphique en crêtes (ridges plot) avec ggridges. Le résultat remarquable est que si Trump dans un premier temps exploite cette nouvel fonctionnalité, il en revient avec un phrasé de 20 mots en moyenne, gardant cependant à loccasion dautre contenu en 50 mots environ. df$Year&lt;-format(df$date, format = &quot;%Y&quot;) #on extrait l&#39;année de la date foo&lt;- df %&gt;% filter(Year!=&quot;2021&quot;) ggplot(foo,aes(x = nb_mots, y = Year, group = Year)) + geom_density_ridges(scale = 3, fill=&quot;peachpuff&quot;)+ theme_ridges() + scale_x_continuous(limits = c(1, 70), expand = c(0, 0)) + coord_cartesian(clip = &quot;off&quot;)+ labs(x=&quot;Nombre de mots par post&quot;, y=NULL) Figure 7.2: Evolution de la distribution du nombre de mots 7.2 la production dans le temps Concluons en examinant le nombre de tweets produit au cours du temps. La fonction ts_plot employée viens de rtweet que nous avons employée au chapitre 2. Elle emploie ggplot, et permet de représenter les évolutions selon différentes échelles de temps. On se rapplera quaprès une carrrière immobilière menée dans les casinos, le golf et les hôtels, lappétit médiatique de trump sest réalisé dans the apprenctice, de 2004 à 2015. Cest un pro de la TV, il a une formation de popstar. Il sera élu en Décembre 2016 pour prendre le pouvoir en Janvier. ## plot time series of tweets ts_plot( foo, &quot;1 month&quot;, color=&quot;darkblue&quot;, size=1.1) + theme(plot.title = element_text(face = &quot;bold&quot;)) + labs( x = NULL, y = &quot;Nombre de tweets par mois&quot;,title = &quot;Fréquence des posts twitters Donald Trump&quot;)+ scale_x_datetime(date_breaks = &quot;1 year&quot;, labels = scales::label_date_short()) Figure 7.3: Evolution de la production mensuelle des tweets de Trump #raf : labeliser avec les dates clés 7.3 Lisibilité et complexité lexicale Pour aller un peu plus loin - nous savons désormais que trump aime une forme courte en 21 mots, et que son expérience de twtitter est longue, on peut sintéresser à des paramètres clés relatifs aux conditions de la reception: les textes sont-ils aisés à lire ? sont-ils sophistiqués ? Introduisons deux quantifications utiles du texte : la lisibilité et la complexité lexicale. Ce sont des classiques, les critères initiaux de lanalyse quantitative du texte. Ils sont toujours utiles. 7.3.1 Les indices de lisibilité La lisibilité est une notion aussi ancienne que sa mesure (par exemple Coleman and Liau (1975)). Elle répond à la question du degré de maitrise requis pour lire un texte en sappyant sur les caractéristique objective du texte plutôt que sur sa perception. Il sagissait donc dévaluer la complexité dun texte. Deux critères principaux sont généralement considérés : la complexité des mots capturée par le nombre moyen de syllabes par mot, et la complexité des phrases mesurée par le nombre de mots. Le nombre dindicateurs est considérable et le package compagnon de quanteda , quanteda.textstats , en fournit des dizaines. Dans lexemple suivant, on se contente dun grand classique, le plus ancien, lindice de Flesch (Flesch 1948) et de ses constituants: le nombre moyen de syllabes par mot et le nombre moyen de mots par phrase. foo&lt;-df %&gt;% filter(isRetweet==FALSE) # on ne prend pas en compte les RT readability&lt;-textstat_readability(foo$text, measure = c(&quot;Flesch&quot;,&quot;meanSentenceLength&quot;, &quot;meanWordSyllables&quot;), min_sentence_length = 3,max_sentence_length = 1000) #la fonction de calcul de lisibilité foo&lt;-cbind(foo,readability[,2:4]) foo1&lt;-foo %&gt;% group_by(Year) %&gt;% summarise(Flesch=mean(Flesch, na.rm=TRUE), SentenceLength= mean(meanSentenceLength, na.rm=TRUE), WordSyllables= mean(meanWordSyllables, na.rm=TRUE)) %&gt;% gather(variable, value, -Year) ggplot(foo1,aes(x=Year, y=value, group=variable))+ geom_line(size=1.2, aes(color=variable), stat=&quot;identity&quot;)+ facet_wrap(vars(variable), scale=&quot;free&quot;, ncol=1)+ labs(title = &quot;Evolution de la lisibilité des tweets de Trump&quot;, x=NULL, y=NULL) Figure 7.4: Evolution de la lisibilité moyenne des tweets de Trump Pour aider le lecteur à donner un sens, voici labaque proposée par Flesch lui-même. Flesch On peut aussi prendre pour références les éléments suivants: All Plain English examples in this book score at least 60. Here are the scores of some reading materials Ive tested. These are average scores of random samples. ( source ?) Comics 92 Consumer ads in magazines 82 Readers Digest 65 Time 52 Wall Street Journal 43 Harvard Business Review 43 Harvard Law Review 32 Auto insurance policy 10 Trump ne parait pas être sa caricature, non niveau de lisibilité correspond à la Licence. Readers Digest est beaucoup plus simple, il se situe au dessus de la Harvard Business Review ! 7.3.2 Les indices de complexité lexicale La complexité lexicale rend compte de la diversité du vocabulaire, elle consiste à rapporter le nombre de mot uniques sur le nombre total de mots. La difficulté est que la taille des corpus joue fortement sur cette mesure et que lorsque cette taille est hétérogène, lindicateur marque plus cette variété que les variations de complexité lexicale.(Tweedie and Baayen 1998) Dans notre univers trumpesque, ce nest pas trop sensible, dautant plus que nous allons moyenner les tweets par période.Notons au passage que si nous moyennons la diversité lexicale de chaque tweet, une autre approche pourrait être de concatener lensemble des tweets dune période (un jour, une semaine) pour approcher cette variable à une autre échelle, qui couvre lensemble des sujets dintérêt de trump, que les tweets fractionnent nécessairement. Ce qui en en question dans la mise en pratique nest pas seulement la question du choix de lindice mais aussi la définition de lunité de calcul. La diversité lexicale concerne sans doute plus le discours que la phrase. On poursuite avec quanteda https://quanteda.io/reference/textstat_lexdiv.html On choisit de ne travailler sur deux des multiples indicateurs disponibles : le CTTR de caroll qui rapporte le nombre de mots distincts ( V) sur le nombre de mots exprimés. Avec ce critère la diversité maximale est obtenue quand le nombre de mot différents est égal au nombre de mots exprimés. \\[ CTTR = \\frac{V}{\\sqrt{2N}} \\] le Mass supposé être moins sensible à la longueur des textes. (voir Torruella et Capsada 2013 ou ) \\[ M = \\frac{log(n) - log(t)}{log² (n)} \\] Le problème de la longueur de text a confuit à des approches segmentées, où la mesure de diversité est une moyenne des moyenne pour chacun des segments. On emploie ici le MATTR, dont le MA signifie moyenne mobile (moving average), et le TTR le token/type ratio. Pour la mise en oeuvre on passe par quanteda. ( attention un pb de log dans le calcul) #on retient les tweets de plus de 5 mots foo&lt;-foo %&gt;%filter(nb_mots&gt;5) #la fonction de calcul de diversité t1=Sys.time() lexdiv&lt;-tokens(foo$text)%&gt;% textstat_lexdiv(foo$text, measure = c(&quot;CTTR&quot;, &quot;Maas&quot;), log.base = 10, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_hyphens = TRUE) t2=Sys.time() t&lt;- t2-t1 t ## Time difference of 5.476105 secs #On combine les données et on aggrège sur l&#39;année foo&lt;-cbind(foo,lexdiv[,2:3]) foo1&lt;-foo %&gt;% group_by(Year) %&gt;% summarise(CTTR=mean(CTTR, na.rm=TRUE), Maas=mean(Maas, na.rm=TRUE)) %&gt;% gather(variable, value, -Year) ggplot(foo1,aes(x=Year, y=value, group=variable))+ geom_line(size=1.2, aes(color=variable), stat=&quot;identity&quot;)+ facet_wrap(vars(variable), scale=&quot;free&quot;, ncol=1)+ labs(title = &quot;Evolution de la diversité lexicale des tweets de Trump&quot;, x=NULL, y=NULL) Figure 7.5: Evolution de la diversité lexicale des tweets de Trump 7.4 Comptons les mots Il est temps de compter les mots, chacun dentre eux, de se faire une idée une idée de leurs fréquences, de leur distribution. Souvent on éliminera ceux qui apparaissent de manière occasionnelle, mais aussi ceux qui apparaissent systématiquement dans tous les textes. Une fois ces deux filtrages effectués, le lexique est généralement de lordre de 500 à 10000 mots. Deux outils sont disponibles: les nuages de mots et les lollyplots. Les premiers donnent une idée immédiates, les seconds se prêtent mieux à une analyse systématique 7.4.1 Les nuages de mots Ils sont devenus extrêmement populaires même si leffet esthétique est plus important que leur utilité analytique. ggwordcloud Pour lapplication on prépare les données avec quanteda : on tokenise et on construit le dfm ( pour le détail voir chapitre tokenization), ce qui nous permets notamment déliminer la ponctuation et les mots courants (articles, déterminant etc) qui apportent peu de signification. foo&lt;-df %&gt;% filter(isRetweet==FALSE) %&gt;% filter( Year %in% c(&quot;2016&quot;,&quot;2018&quot;,&quot;2020&quot;))# on ne prend pas en compte les RT toks&lt;- tokens(foo$text) %&gt;% dfm(remove_punct = TRUE, remove = stopwords(&quot;english&quot;)) docvars(toks,&quot;Year&quot;)&lt;-foo$Year toks ## Document-feature matrix of: 13,421 documents, 21,904 features (99.94% sparse) and 1 docvar. ## features ## docs thrilled back great city charlotte north carolina thousands hardworking ## text1 1 1 1 1 1 1 1 1 1 ## text2 0 0 0 0 0 0 0 0 0 ## text3 0 0 0 0 0 0 0 0 0 ## text4 0 0 0 0 0 0 0 0 0 ## text5 0 0 0 0 0 0 0 0 0 ## text6 0 0 0 0 0 0 0 0 0 ## features ## docs american ## text1 1 ## text2 0 ## text3 0 ## text4 0 ## text5 0 ## text6 0 ## [ reached max_ndoc ... 13,415 more documents, reached max_nfeat ... 21,894 more features ] #on se concentre du les termes utilisés 300 fois. foo&lt;-toks %&gt;% dfm_trim(min_termfreq = 300, verbose = FALSE) freq &lt;- textstat_frequency(foo) library(ggwordcloud) ggplot(freq, aes(label = feature)) + geom_text_wordcloud(aes(size=frequency, color=rank)) + theme_minimal() + scale_size_area(max_size = 10) + scale_color_gradient(low = &quot;darkblue&quot;, high = &quot;red&quot;) Figure 7.6: Nuage de mots - Trump 2016, 2018, 2020 ggsave(&quot;./Images/g0.jpg&quot;, plot=last_plot(), width = 27, height = 19, units = &quot;cm&quot;) Et pour faire des comparaisons, entre lannée 2016 qui le conduit à être élu, 2018 une année de mid term et 2020 année de sa défaite, on utilise la même procédure mais on distingue un comptage de fréquence de mot par période. Le ggplot est identique aux précédents mais comprend en plus une géométrie facet_wrap qui éclate le nuages de mot selon les 3 périodes étudiées. foo&lt;-toks %&gt;% dfm_group(groups = Year) %&gt;% dfm_trim(min_termfreq = 250, verbose = FALSE) #pour compter la fréquence des mots par année freq &lt;- textstat_frequency(foo, group =Year) set.seed(42) library(ggwordcloud) ggplot(freq, aes(label = feature)) + geom_text_wordcloud(aes(size=frequency, color=rank)) + theme_minimal()+facet_wrap(vars(group)) + scale_size_area(max_size = 10) + scale_color_gradient(low = &quot;darkblue&quot;, high = &quot;red&quot;) Figure 7.7: Nuage de mots - Trump 2016, 2018, 2020 ggsave(&quot;./Images/g1.jpg&quot;, plot=last_plot(), width = 27, height = 19, units = &quot;cm&quot;) 7.4.2 Des lollyplots Plus utile sont des diagrammes dun format loliplot qui quantifie de manière plus précise la fréquence des termes. la notion de densité, quelle échelle? 7.4.3 La mesure de la concentration des termes https://www.tidytextmining.com/tfidf.html Le langage dun point de vue quantitatif a été caractérisé depuis bien longtemps et zipf est un des premier à avoir proposé la loi de distribution qui porte son nom mais aussi celle de mandelbrot qui en a proposé un affinement. Celle-ci sexprime de la manière suivante : si on classe les termes par ordre de fréquence r \\[f(r)=\\frac{K}{r} \\] Mandelbrot la généralise les études empiriques On peut encore débattre de sa signification. Une économie cognitive? Une conséquence de la théorie de linformation ? Un signature pour identifier un discours Zipf distribution 1949 On reprend la procédure de comptage des mots par groupe, mais sans filtrer sur la fréquence de ces mots. On en obtient n_w mots distincts. En examinant avec plus de détail il y a 5 élément dinformation : a ) le trait ( feature), cest à dire ici le mot étudié, 2) le nombre de fois où il apparait dans le corpus, 3)le rank qui lui correspond, 4)le nombre de document dans lesquel il apparait, et enfin 5) la période du corpus. foo&lt;-toks freq &lt;- textstat_frequency(foo) freq &lt;- textstat_frequency(foo, group =Year) head(freq) ## feature frequency rank docfreq group ## 1 thank 632 1 630 2016 ## 2 great 597 2 564 2016 ## 3 hillary 450 3 430 2016 ## 4 trump 389 4 365 2016 ## 5 amp 354 5 329 2016 ## 6 @realdonaldtrump 334 6 331 2016 n_w&lt;-dim(freq) #ft&lt;-flextable(head(freq)) ## pb affichage ft .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-51b90f56{border-collapse:collapse;}.cl-51ae8f2c{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-51ae8f2d{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-51ae8f2e{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-51aedd1a{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-51aedd1b{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-51aedd1c{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-51aedd1d{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-51aedd1e{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-51aedd1f{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} featurefrequencyrankdocfreqgroup freq&lt;-freq %&gt;% group_by(group)%&gt;% mutate(rank_rel=rank/max(rank), #on normalise le classement de 0 , mauvais classement à 1, bon classement. freq_prop=frequency/sum(frequency), freq_density=docfreq/sum(docfreq)) ggplot(freq, aes(x=rank_rel, fill = group)) + geom_histogram(show.legend = FALSE, binwidth = 0.05) + facet_wrap(~group, ncol = 3, scales = &quot;fixed&quot;) ggplot(freq, aes(x=freq_prop, fill = group)) + geom_histogram(show.legend = FALSE, binwidth = 0.0001) + facet_wrap(~group, ncol = 3, scales = &quot;fixed&quot;)+ scale_y_log10() g&lt;-freq %&gt;% ggplot(aes(rank_rel, freq_prop,color = group)) + geom_line(size = 1, alpha = 1, show.legend = FALSE) + scale_y_log10()+ geom_smooth(method=&quot;lm&quot;, alpha=0.5, color=&quot;grey80&quot;)+ scale_x_continuous(labels = scales::percent_format(accuracy = 1)) g On essaye dajuster avec deux fonctions linéaires, et on saperçoit de suite que lajustement est largement meilleure. Une première loi régit les 3000 mots les plus fréquents, une autre, beaucoup plus pentue régit les mots les moins fréquents, essentiellement ce qui napparaissent quune seul fois. #on coup à .1 car c&#39;est là où se trouve le &quot; coude&quot; foo1&lt;-freq %&gt;% filter(rank_rel&lt;0.1) fit1&lt;- lm(log10(freq_prop+0.0001)~log10(rank_rel+0.0000001),data= foo1) a&lt;-fit1$coefficients g&lt;-g+ geom_abline(intercept=a[1], slope=a[2], color=&quot;grey80&quot;) g foo2&lt;-freq %&gt;% filter(rank_rel&gt;0.09999999) fit2&lt;- lm(log10(freq_prop+0.0000001)~log10(rank_rel+0.0000001),data= foo2) b&lt;-fit2$coefficients g&lt;-g+ geom_abline(intercept=b[1], slope=b[2], color=&quot;grey80&quot;) g 7.5 Conclusion Nous aurons appris à Compter le nombre de documents et leurs longueurs Mesurer la complexité du langage Mesurer la diversité de son vocabulaire. A évaluer la concentration des sources A se donner une première idée de la lexicographie Ces mesures nont ne sens que si elles peuvent être lobjet de comparaison : De manière interne la comparaison se fait dans dans le temps et à travers des segments. On sintéresse moins au niveau, quaux différences entre les niveaux. De manière externe elle requiert un étalonnage. Comparer par rapport au français courant, à un niveau de langue soutenu, ou relâché. Létalonnage revient à caractériser des types de corpus : presses, écriture savante, réseaux sociaux, publications officielles etc. On ne peut que souhaiter que des comparaisons systèmatiques soient engagées et compilées pour donner des points de repère précis quand on étudie un corpus particulier. Elles participent à un premier niveau danalyse du texte, en surface, visant à apprécier la dynamique de sa production, à établir les échelles danalyse, à repérer les éléments structurels. Le texte est une matière qui a un poids (le nombre de mot), une variété (le nombre dexpressions), une complexité (les règles qui lorganisent). nous venons de nous doter des premiers outils danalyse, il est temps de passer à la suite. References "],["analyse-du-sentiment.html", "Chapitre 8 Analyse du sentiment 8.1 Un exemple avec syuzhet 8.2 La généralisation par le Liwc 8.3 Encore dautres généralisations 8.4 construire son propre dictionnaire", " Chapitre 8 Analyse du sentiment A vrai dire lanalyse du sentiment telle quon va la développer dans ce chapitre est une méthode dannotation comme celle quon a examinée précèdemment. Son principe est simple : on compte la fréquence de mots qui appartiennent à un dictionnaire de réfrences. Les différentes méthodes dépendent de la constitution de ces dictionnaires. Dans certains cas on peut nuancer en prenant en compte les modificateurs (notamment les négation et double négation) qui pourront pondérer le sentiment primaire des mots. liu ping est le fondateur de lanalyse du sentiment et dès 2012 en donne une synthèse complète (Liu 2012) . Depuis des développement considérables ont été apportée par des méthodes de deep learning, et notamment les modèles transformer qui renouvelent consirablement le domaine. On restera ici à un niveau classique ou compositionnel et On travaillera sur un corpus davis trip advisor, sur la période avant covid. Un premier exemple se concentrera sur le comptage des termes positifs, négatifs et neutres. Dans un second on montre une généralisation avec le LIWC qui propose 80 catégories. df&lt;-readRDS(&quot;./data/AvisTripadvisor.rds&quot;) 8.1 Un exemple avec syuzhet On utilise le package syuzhet et en particulier le dictionnaire nrc developpé et traduit par Mohammad and Turney (2013) ( Index Feel) Le même outil fournit un autre systéme dannotation qui compte les mentions déléments positifs ou négatifs, ainsi que démotions définies sur la base de linventaire de Plutchik (1982) on utilise simplement la fonction get_nrc_sentiment, en précisant le dictionnaire adéquat. Léchelle comprend en fait deux éléments : les 8 émotion de base *au sens de pluchik, et deux indicateurs de polarité. Lopérationnalisation réalisée par Mohammad and Turney (2013) sinscrit dans une tradition de la recherche en marketing, se souvenir de (???) et de (???). library(syuzhet) #analyse du sentimeent #paramétres method &lt;- &quot;nrc&quot; lang &lt;- &quot;french&quot; phrase&lt;-as.character(paste0(df$Titre,&quot; &quot;,df$Commetaire)) #extraction emotions &lt;- get_nrc_sentiment(phrase,language = &quot;french&quot;) polarity&lt;-subset(emotions,select=c(positive, negative)) df&lt;-cbind(df,polarity) On sintéresse surtout aux mentions positives et négatives (les émotions cest pour plus tard. (la mesure permet ainsi une dissymétrie des deux polarités, il y a le bien, le mal, le mal et le bien, mais aussi si qui nest ni mal ni bien). Les textes étant inégaux en taille on va ramener lindicateur de polarité au nombre de caractéres (sur une base de 500 c) de chaque contribution. En effet lalgo compte les valence et leur intensité est proportionnel à la longueur du texte. Ce qui est clairement démontré par la seconde figure. A partir de ces deux mesures, 4 indicateurs peuvent étre construits * Positivité : nombre de termes positifs pour 500 signes. * Négativitivé : nombre de termes négatifs pour 500 signes. * Valence : rapport du nombre de termes positifs sur les négatifs. * Expressivité : nombre de termes positifs et négatifs. le dernier graphe nous apprend que les jugements plutôt positifs sont aussi les plus expressifs. La froideur des avis les plus négatifs refléte-t-elle une crainte de la désaprobation sociale. Cest une piste de recherche à poursuivre, on pourrait sattendre à ce que les avis les plus négatifs surgissent plus facilement si la densité des négatives est plus importante et observer une sorte dautocorrélation. G1&lt;-ggplot(df, aes(x=positive))+geom_histogram(binwidth = 1, fill=&quot;darkred&quot;)+theme_minimal() G1 G2&lt;-ggplot(df, aes(x=negative))+geom_histogram(binwidth = 1,fill=&quot;Royalblue&quot;)+theme_minimal() G2 8.1.1 Valence et expression la linguistique donne aux mots une valence : elle peut être positive (bonheur), négative (malheur) ou neutre ( tranquité). Cest un régime ternaire. Chaque mot dune phrase est neutre, positif ou négatif. On peut doser les effets On a des dictionnaires df$text&lt;-as.character(paste0(df$Titre,&quot; &quot;,df$Commetaire)) df$WC&lt;-str_count(df$text, &quot;\\\\S+&quot;) df$positivity&lt;-(df$positive)/(df$WC) df$negativity&lt;-(df$negative)/(df$WC) df$valence&lt;-df$positivity-df$negativity df$expressivity&lt;-df$positivity+df$negativity G11&lt;-ggplot(df, aes(x=valence,y=expressivity ))+geom_point(color=&quot;grey&quot;)+geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x, bs = &quot;cs&quot;))+theme_minimal() G12&lt;-ggplot(df, aes(x=negativity,y=positivity ))+geom_point(color=&quot;grey&quot;)+geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x, bs = &quot;cs&quot;))+theme_minimal() G13&lt;-ggplot(df, aes(x=negativity,y= expressivity))+geom_point(color=&quot;grey&quot;)+geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x, bs = &quot;cs&quot;))+theme_minimal() G14&lt;-ggplot(df, aes(x=positivity,y= expressivity))+geom_point(color=&quot;grey&quot;)+geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x, bs = &quot;cs&quot;))+theme_minimal() plot_grid(G11, G12, G13,G14, labels = c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;,&#39;D&#39;)) 8.2 La généralisation par le Liwc Le liwc vient de lidée simple dun psychiatre qui a souhaité faire des diagnoistics de trauma craniens à partir des entretiens menés avec lmes patients atteints. (Tausczik and Pennebaker 2010) Le LIWC permet dobtenir dautres indicateurs du sentiment, une partie des 80 indicateurs proposés est relatif à des dimensions topicales dont trois groupes vont retenir notre attention dans la mesure où ils décrivent une partie de lexpérience relatée dans les commentaires. * La sensorialité ( voir, entendre, sentir) * Lorientation temporelle ( passé, présent, futur) * les émotions négatives (tristesse, colére, ) La procédure pour extraire ces notions est fort simple . On utilise linfrasturure de quanteda et une version francophone du dictionnaire (Piolat et al. 2011) # the devtools package needs to be installed for this to work #devtools::install_github(&quot;kbenoit/quanteda.dictionaries&quot;) library(cleanNLP) library(&quot;quanteda.dictionaries&quot;) dict_liwc_french &lt;- dictionary(file = &quot;FrenchLIWCDictionary.dic&quot;, format = &quot;LIWC&quot;) test&lt;-liwcalike(df$Commetaire,dictionary = dict_liwc_french) df&lt;-cbind(df,test) 8.3 Encore dautres généralisations Lapproche par dictionnaire sest déplacée vers lidentification dautre catégorie les valeurs morales 8.4 construire son propre dictionnaire faire des listes de lmots References "],["token.html", "Chapitre 9 Tokenisation 9.1 Objectifs du chapitre 9.2 Les outils 9.3 Introduction 9.4 Tokeniser un corpus 9.5 N-grammes 9.6 Choisir des n-grammes pertinents 9.7 Conclusion", " Chapitre 9 Tokenisation 9.1 Objectifs du chapitre Découper un texte en tokens Visualiser les n-grammes du texte Identifier les n-grammes pertinents et les transformer en tokens 9.2 Les outils Jeu de données : une citation de Max Weber et le corpus des commentaires laissés sur TripAdvisor concernant les hôtels polynésiens. Packages utilisés : tokenizer ; quanteda ; stopwords 9.3 Introduction Létape intiale de toute analyse textuelle est de découper le texte en unités danalyse, les tokens, ce qui transforme le texte écrit pour la compréhension humaine en données interprétables par lordinateur. Les tokens utilisés peuvent varier selon les objectifs de lanalyse et la nature du corpus, la granularité peut être plus ou moins fine. Les tokens peuvent ainsi être : des lettres : cest lunité insécable. des syllabes : ça permet de sintéresser aux phonèmes.mais aussi dextraire dun mot les suffixes et préfixes, ainsi que les radicaux ( la racine du mot, ex : dés-espéré-ment). des mots : il sagit du niveau le plus évident et le plus courant, que lon privilégiera tout au long de ce livre des phrases : cest lunité de langage, lui correpond un argument, une proposition ; lusage du point suivi dun espace et dune majuscule est assez général pour les identifier. des paragraphes : cest une unité plus générale, qui souvent développe une idée. des sections, des chapitres, ou des livres : selon la nature des documents, cela permet de découper le corpus en sous-unités. Les tokenizers sont les outils indispensables à cette tâche. Dans cet ouvrage, nous nous concentrons sur létude des mots. Lors de cette étude, un certain nombre de mots apparaissent de nombreuses fois, pour permettre de donner du sens au langage humain, mais ils ne portent pas en eux dinformations particulièrement pertinentes pour lanalyse : ce sont les stopwords, quil conviendra souvent déliminer. Les n-grammes, quant à eux, représentent des suites de n tokens. Un unigramme est donc équivalent à un token, un bigramme est une suite de deux tokens, etc. Lidentification des n-grammes permet de détecter des suites de tokens qui reviennent plus souvent que leur probabilité doccurrences. Si lon se concentre sur les mots, nous sommes alors face à une unité sémantique, comme on le comprend facilement avec le bigramme Assemblée Nationale. 9.4 Tokeniser un corpus 9.4.1 Les lettres Commençons par un exemple simple, à laide dune courte citation de Max Weber. On choisit les lettres pour unité de découpe, et lon utilise le package tokenizer. Automatiquement, tokenizer met le texte en minuscule et élimine la ponctuation #Les données MaxWeber &lt;- paste0(&quot;Bureaucratie: le moyen le plus rationnel que lon connaisse pour exercer un contrôle impératif sur des êtres humains.&quot;) #On tokenise, plus on transforme en dataframe le résultat. toc_maxweber&lt;-tokenize_characters(MaxWeber)%&gt;% as.data.frame()%&gt;% rename(tokens=1) #On compte pour chaque token sa fréquence d&#39;apparition foo&lt;-toc_maxweber %&gt;%mutate(n=1) %&gt;% group_by(tokens)%&gt;% summarise(n=sum(n))%&gt;% filter(n&gt;0) #On représente par un diagramme en barre cette distribution des occuences d&#39;apparition, en classant les tokens par fréquence ggplot(foo, aes(x=reorder(tokens,n), y=n))+ geom_bar(stat=&quot;identity&quot;, fill=&quot;royalblue&quot;)+ annotate(&quot;text&quot;, x=10,y=10, label=paste(&quot;nombre de tokens =&quot;, nrow(toc_maxweber)))+ coord_flip()+ labs(title = &quot;Fréquence des tokens, unité = lettres&quot;, x=&quot;tokens&quot;, y=&quot;nombre d&#39;occurences&quot;, caption =&quot; &#39;Bureaucratie: le moyen le plus rationnel que lon connaisse pour exercer un contrôle impératif sur des êtres humains.&#39; &quot;) Figure 9.1: Distribution du nombre de lettres 9.4.2 Les mots On refait la même opération, mais avec un texte complété. Il y a bien moins de mots que de lettres ! #Les données MaxWeber &lt;- paste0(&quot;Bureaucratie: le moyen le plus rationnel que lon connaisse pour exercer un contrôle impératif sur des êtres humains. La bureaucratie est une forme d&#39;organisation générale caractérisée par la prépondérance des règles et de procédures qui sont appliquées de façon impersonnelle par des agents spécialisés. Ces agents appliquent les règles sans discuter des objectifs ou des raisons qui les fondent. Ils doivent faire preuve de neutralité et oublier leurs propres intérêts personnels au profit de lintérêt général.&quot;) #On tokenise, plus on transforme en dataframe le résultat. toc_maxweber&lt;-tokenize_words(MaxWeber)%&gt;% as.data.frame()%&gt;% rename(tokens=1) #On compte pour chaque token sa fréquence d&#39;apparition foo&lt;-toc_maxweber %&gt;%mutate(n=1) %&gt;% group_by(tokens)%&gt;% summarise(n=sum(n)) #On représente par un diagramme en barre cette distribution des occurrences, en classant les tokens par fréquence ggplot(foo, aes(x=reorder(tokens,n), y=n))+ geom_bar(stat=&quot;identity&quot;, fill=&quot;royalblue&quot;)+ annotate(&quot;text&quot;, x=10,y=4, label=paste(&quot;nombre de tokens =&quot;, nrow(toc_maxweber)))+ coord_flip()+labs(title = &quot;Fréquence des tokens, unité = mots&quot;, x=&quot;tokens&quot;, y=&quot;nombre d&#39;occurences&quot;, caption =&quot; &#39;Bureaucratie: le moyen le plus rationnel que lon connaisse pour exercer un contrôle impératif sur des êtres humains.&#39; &quot;) Figure 9.2: Distribution du nombre de mots On constate que les deux mots les plus fréquents de cette citation sont un article indéfini et une préposition. Ces mots sont souvent superflus pour les analyses menées, il convient alors de les supprimer. Cest ce quon fait par la suite, en utilisant le package stopwords qui comprend des listes de stopwords dans différentes langues. #On tokenise et on enlève les stopwords, puis on transforme en dataframe le résultat. toc_maxweber&lt;-tokenize_words(MaxWeber, stopwords = stopwords(&quot;fr&quot;))%&gt;% as.data.frame()%&gt;% rename(tokens=1) #On compte pour chaque token sa fréquence d&#39;apparition foo&lt;-toc_maxweber %&gt;%mutate(n=1) %&gt;% group_by(tokens)%&gt;% summarise(n=sum(n))%&gt;%filter(n&gt;0) #On représente par un diagramme en barre cette distribution des occurrences, en classant les tokens par fréquence ggplot(foo, aes(x=reorder(tokens,n), y=n))+ geom_bar(stat=&quot;identity&quot;, fill=&quot;royalblue&quot;)+ annotate(&quot;text&quot;, x=10,y=1.5, label=paste(&quot;nombre de tokens =&quot;, nrow(toc_maxweber)))+ coord_flip()+labs(title = &quot;Fréquence des tokens, unité = mots, stopwords éliminés&quot;, x=&quot;tokens&quot;, y=&quot;nombre d&#39;occurences&quot;, caption =&quot; &#39;Bureaucratie: le moyen le plus rationnel que lon connaisse pour exercer un contrôle impératif sur des êtres humains.&#39; &quot;) Figure 9.3: Distribution du nombre de mots, sans les stopwords On peut également constater que certains mots sont proches, par exemple les deux derniers sur le graphiques précédents qui sont des déclinaisons du verbe appliquer. Il peut alors être pertinent de regrouper ces différentes formes verbales (comme un mot au singulier et au pluriel, au féminin et au masculin, ou conjugué sous différentes formes), pour faciliter lanalyse. Cest ce quon fait avec les opérations de stemming ou de lemmatisation, présentées au chapitre ??. 9.4.3 Les phrases On reproduit les mêmes opérations, mais cette fois sur les phrases de lexemple précédent. tokenize_sentences(MaxWeber)%&gt;%as.data.frame()%&gt;%rename(tokens=1)%&gt;%flextable(cwidth = 5) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-8fe0a758{border-collapse:collapse;}.cl-8fd42cd0{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-8fd453c2{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-8fd4c8a2{width:360pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8fd4c8a3{width:360pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8fd4c8a4{width:360pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} tokensBureaucratie: le moyen le plus rationnel que lon connaisse pour exercer un contrôle impératif sur des êtres humains.La bureaucratie est une forme d'organisation générale caractérisée par la prépondérance des règles et de procédures qui sont appliquées de façon impersonnelle par des agents spécialisés.Ces agents appliquent les règles sans discuter des objectifs ou des raisons qui les fondent.Ils doivent faire preuve de neutralité et oublier leurs propres intérêts personnels au profit de lintérêt général. 9.5 N-grammes Les n-grammes sont des séquences de n tokens, généralement consécutifs. Voici tout de suite un exemple sur les lettres2, allant de bigramme au trigramme : toc_maxweber&lt;-tokenize_character_shingles(MaxWeber,n=3, n_min=2) %&gt;% as.data.frame()%&gt;%rename(tokens=1) flextable(head(toc_maxweber, n=20)) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-90027018{border-collapse:collapse;}.cl-8ff88bde{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-8ff88bdf{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-8ff8b2c6{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8ff8b2c7{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8ff8b2c8{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} tokensbuburururerereaeaeauauaucucucrcrcrararatatatititie foo&lt;-toc_maxweber %&gt;%mutate(n=1) %&gt;% group_by(tokens)%&gt;% summarise(n=sum(n))%&gt;%filter(n&gt;3) ggplot(foo, aes(x=reorder(tokens,n), y=n))+ geom_bar(stat=&quot;identity&quot;, fill=&quot;royalblue&quot;)+annotate(&quot;text&quot;, x=5,y=11, label=paste(&quot;nombre de tokens =&quot;, nrow(toc_maxweber)))+ coord_flip()+labs(title = &quot;Bigrammes et trigrammes des lettres&quot;, x=&quot;n-gramme&quot;, y=&quot;nombre d&#39;occurences&quot;) Figure 9.4: Bigrammes et trigrammes de lettres On peut faire la même chose sur les mots, en éliminant les stopwords : toc_maxweber&lt;-tokenize_ngrams(MaxWeber,n=3, n_min=2, stopwords = stopwords(&#39;fr&#39;)) %&gt;% as.data.frame()%&gt;%rename(tokens=1) qflextable(head(toc_maxweber, n=19)) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-906913ae{border-collapse:collapse;}.cl-906029ba{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-906029bb{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-906077b2{width:152pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-906077b3{width:152pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-906077b4{width:152pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-906077b5{width:152pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-906077b6{width:152pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-906077b7{width:152pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-906077b8{width:152pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-906077b9{width:152pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-906077ba{width:152pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} tokensbureaucratie moyenbureaucratie moyen plusmoyen plusmoyen plus rationnelplus rationnelplus rationnel lonrationnel lonrationnel lon connaisselon connaisselon connaisse exercerconnaisse exercerconnaisse exercer contrôleexercer contrôleexercer contrôle impératifcontrôle impératifcontrôle impératif êtresimpératif êtresimpératif êtres humainsêtres humains On peut également sintéresser aux n-grammes non directement consécutifs mais séparés par k tokens : toc_maxweber&lt;-tokenize_skip_ngrams(MaxWeber,n=3, n_min=2, k=2, stopwords = stopwords(&#39;fr&#39;)) %&gt;% as.data.frame()%&gt;%rename(tokens=1) qflextable(head(toc_maxweber, n=19)) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-908f6b4e{border-collapse:collapse;}.cl-908254ae{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-90827ba0{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9082f076{width:179.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9082f077{width:179.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9082f078{width:179.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9082f079{width:179.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9082f07a{width:179.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9082f07b{width:179.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9082f07c{width:179.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9082f07d{width:179.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} tokensbureaucratie moyenbureaucratie plusbureaucratie rationnelbureaucratie moyen plusbureaucratie moyen rationnelbureaucratie moyen lonbureaucratie plus rationnelbureaucratie plus lonbureaucratie plus connaissebureaucratie rationnel lonbureaucratie rationnel connaissebureaucratie rationnel exercermoyen plusmoyen rationnelmoyen lonmoyen plus rationnelmoyen plus lonmoyen plus connaissemoyen rationnel lon Dans cet exemple, aucun n-gramme nest répété, mais cest rarement le cas avec des corpus plus importants. Dans ce cas, une forte répétition de n-grammes est un indice dune unité sémantique composée de plusieurs tokens que lon peut alors regrouper en un seul et même token. Cest ce que lon verra dans la section suivante, avec lutilisation de quanteda. 9.5.1 Propriétés statistiques des n-grammes Sur la dun base dun corpus important on peut calculer les probabilité dapparitions dun n-gramme. Cest une ressource que fournit Google avec son Books Ngram Viewer. Processus de markov application à la correction 9.6 Choisir des n-grammes pertinents Dans ce livre lunité principales danalyse restera le mot. Mais nous savons, au moins intuitivement que certaines combinaisons de mots représentent des expressions qui ont la valeur dun mot, une valeur sémantique, par exemple, lexpression Assemblée Nationale. Ces deux mots réunis constituent un syntagme, une unité de sens. La question qui se pose est alors de savoir comment les identifier dans le flot des n-grammes ? La technique est simple : si deux mots se retrouvent dans un ordre donné plus fréquemment que ce que le produit de leurs probabilités dapparition laisse espérer, cest quils constituent une expression. On peut imaginer faire un test du chi² pour décider si un couple de mots constitue une unité sémantique ou non. Le package quanteda propose une bonne solution à ce problème avec la fonction collocation. 9.6.1 Créer les tokens avec quanteda À partir du corpus des commentaires de TripAdvisor concernant les hôtels de Polynésie Française, on crée un objet de format token, dans lequel on a enlevé les stopwords. Mais pour que les n-grammes très fréquents restent des syntagmes signifiants, on laisse apparent les positions des stopwords, avec loption padding= TRUE. #les données AvisTripadvisor&lt;-read_rds(&quot;data/AvisTripadvisor.rds&quot;) #création du corpus corpus&lt;-corpus(AvisTripadvisor,docid_field = &quot;ID&quot;,text_field = &quot;Commetaire&quot;) head(corpus) ## Corpus consisting of 6 documents and 21 docvars. ## 1 : ## &quot;Tout est magnifique au Vahine island. Séjour de rêve avec un...&quot; ## ## 2 : ## &quot;Tout était parfait, notre meilleure expérience et plus belle...&quot; ## ## 3 : ## &quot;Un séjour magnifique, 3 jours époustouflants, accueillis cha...&quot; ## ## 4 : ## &quot;Vraiment beau, cadre idyllique personnel très attentionné. N...&quot; ## ## 5 : ## &quot;Au Vahiné Island on entre dans un monde parallèle où on ne s...&quot; ## ## 6 : ## &quot;Nous avons adoré notre séjour au Vahiné Island avec nos 3 en...&quot; #transformation en objet token tok&lt;-tokens(corpus,remove_punct = TRUE, remove_symbols=TRUE, remove_numbers=TRUE) head(tok) ## Tokens consisting of 6 documents and 21 docvars. ## 1 : ## [1] &quot;Tout&quot; &quot;est&quot; &quot;magnifique&quot; &quot;au&quot; &quot;Vahine&quot; ## [6] &quot;island&quot; &quot;Séjour&quot; &quot;de&quot; &quot;rêve&quot; &quot;avec&quot; ## [11] &quot;une&quot; &quot;équipe&quot; ## [ ... and 22 more ] ## ## 2 : ## [1] &quot;Tout&quot; &quot;était&quot; &quot;parfait&quot; &quot;notre&quot; &quot;meilleure&quot; ## [6] &quot;expérience&quot; &quot;et&quot; &quot;plus&quot; &quot;belle&quot; &quot;découverte&quot; ## [11] &quot;en&quot; &quot;Polynésie.Un&quot; ## [ ... and 37 more ] ## ## 3 : ## [1] &quot;Un&quot; &quot;séjour&quot; &quot;magnifique&quot; &quot;jours&quot; ## [5] &quot;époustouflants&quot; &quot;accueillis&quot; &quot;chaleureusement&quot; &quot;par&quot; ## [9] &quot;toute&quot; &quot;l&#39;équipe&quot; &quot;du&quot; &quot;Vahine&quot; ## [ ... and 27 more ] ## ## 4 : ## [1] &quot;Vraiment&quot; &quot;beau&quot; &quot;cadre&quot; &quot;idyllique&quot; &quot;personnel&quot; ## [6] &quot;très&quot; &quot;attentionné&quot; &quot;Nous&quot; &quot;avons&quot; &quot;adoré&quot; ## [11] &quot;notre&quot; &quot;séjour&quot; ## [ ... and 27 more ] ## ## 5 : ## [1] &quot;Au&quot; &quot;Vahiné&quot; &quot;Island&quot; &quot;on&quot; &quot;entre&quot; &quot;dans&quot; ## [7] &quot;un&quot; &quot;monde&quot; &quot;parallèle&quot; &quot;où&quot; &quot;on&quot; &quot;ne&quot; ## [ ... and 126 more ] ## ## 6 : ## [1] &quot;Nous&quot; &quot;avons&quot; &quot;adoré&quot; &quot;notre&quot; &quot;séjour&quot; &quot;au&quot; ## [7] &quot;Vahiné&quot; &quot;Island&quot; &quot;avec&quot; &quot;nos&quot; &quot;enfants&quot; &quot;bungalow&quot; ## [ ... and 77 more ] #enlever les stopwords tok&lt;-tokens_remove(tok,stopwords(&#39;fr&#39;),padding=TRUE) head(tok) ## Tokens consisting of 6 documents and 21 docvars. ## 1 : ## [1] &quot;Tout&quot; &quot;&quot; &quot;magnifique&quot; &quot;&quot; &quot;Vahine&quot; ## [6] &quot;island&quot; &quot;Séjour&quot; &quot;&quot; &quot;rêve&quot; &quot;&quot; ## [11] &quot;&quot; &quot;équipe&quot; ## [ ... and 22 more ] ## ## 2 : ## [1] &quot;Tout&quot; &quot;&quot; &quot;parfait&quot; &quot;&quot; &quot;meilleure&quot; ## [6] &quot;expérience&quot; &quot;&quot; &quot;plus&quot; &quot;belle&quot; &quot;découverte&quot; ## [11] &quot;&quot; &quot;Polynésie.Un&quot; ## [ ... and 37 more ] ## ## 3 : ## [1] &quot;&quot; &quot;séjour&quot; &quot;magnifique&quot; &quot;jours&quot; ## [5] &quot;époustouflants&quot; &quot;accueillis&quot; &quot;chaleureusement&quot; &quot;&quot; ## [9] &quot;toute&quot; &quot;l&#39;équipe&quot; &quot;&quot; &quot;Vahine&quot; ## [ ... and 27 more ] ## ## 4 : ## [1] &quot;Vraiment&quot; &quot;beau&quot; &quot;cadre&quot; &quot;idyllique&quot; &quot;personnel&quot; ## [6] &quot;très&quot; &quot;attentionné&quot; &quot;&quot; &quot;&quot; &quot;adoré&quot; ## [11] &quot;&quot; &quot;séjour&quot; ## [ ... and 27 more ] ## ## 5 : ## [1] &quot;&quot; &quot;Vahiné&quot; &quot;Island&quot; &quot;&quot; &quot;entre&quot; &quot;&quot; ## [7] &quot;&quot; &quot;monde&quot; &quot;parallèle&quot; &quot;où&quot; &quot;&quot; &quot;&quot; ## [ ... and 126 more ] ## ## 6 : ## [1] &quot;&quot; &quot;&quot; &quot;adoré&quot; &quot;&quot; &quot;séjour&quot; &quot;&quot; ## [7] &quot;Vahiné&quot; &quot;Island&quot; &quot;&quot; &quot;&quot; &quot;enfants&quot; &quot;bungalow&quot; ## [ ... and 77 more ] #on transforme en document-features matrix pour des représentations graphiques dfm&lt;-dfm(tok,remove_padding=TRUE) head(dfm) ## Document-feature matrix of: 6 documents, 23,045 features (99.85% sparse) and 21 docvars. ## features ## docs tout magnifique vahine island séjour rêve équipe très chaleureuse cadre ## 1 1 1 1 1 1 1 2 1 1 1 ## 2 2 0 0 0 0 0 1 2 0 0 ## 3 1 1 1 1 1 0 0 0 0 0 ## 4 1 1 0 0 1 0 0 1 0 1 ## 5 2 1 0 1 0 1 0 4 0 0 ## 6 0 1 0 1 2 1 0 0 0 0 ## [ reached max_nfeat ... 23,035 more features ] #un nuage de mots rapide textplot_wordcloud(dfm, max_words = 200, color = rev(RColorBrewer::brewer.pal(6, &quot;RdBu&quot;))) Figure 9.5: Mots les plus fréquents du corpus 9.6.2 Identifier les noms propres On cherche ici à identifier les noms propres présents dans le corpus. #on sélectionne les mots commençant par une majuscule toks_cap &lt;- tokens_select(tok, pattern = &quot;^[A-Z]&quot;, valuetype = &quot;regex&quot;, case_insensitive = FALSE, padding = TRUE) #on cherche les collocations tstat_col_cap &lt;- textstat_collocations(toks_cap, min_count = 3, tolower = FALSE) flextable(head(as.data.frame(tstat_col_cap))) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-9225cf48{border-collapse:collapse;}.cl-921cadc8{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-921cadc9{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-921cadca{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-921cfbb6{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-921cfbb7{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-921cfbb8{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-921cfbb9{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-921cfbba{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-921cfbbb{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} collocationcountcount_nestedlengthlambdazBora Bora155027.15907457.75427Pearl Beach13028.59205523.25054Jean Claude13028.98072922.41189Tiputa Lodge13027.12910322.06886Taha'a Island10028.27292120.98451Nuku Hiva230211.61916520.66761 9.6.3 Composer des tokens à partir dexpressions multi-mots Dans ce corpus, les noms propres correspondent aux noms des îles et des hôtels, et aux prénoms composés. La valeur du lambda montre la force de lassociation entre les mots, on retiendra dune manière générale un lambda au moins supérieur à 3 pour remplacer les tokens dorigine par leurs n-grammes. toks_comp &lt;- tokens_compound(tok, pattern = tstat_col_cap[tstat_col_cap$z &gt; 3], case_insensitive = FALSE) head(toks_comp) ## Tokens consisting of 6 documents and 21 docvars. ## 1 : ## [1] &quot;Tout&quot; &quot;&quot; &quot;magnifique&quot; &quot;&quot; &quot;Vahine&quot; ## [6] &quot;island&quot; &quot;Séjour&quot; &quot;&quot; &quot;rêve&quot; &quot;&quot; ## [11] &quot;&quot; &quot;équipe&quot; ## [ ... and 22 more ] ## ## 2 : ## [1] &quot;Tout&quot; &quot;&quot; &quot;parfait&quot; &quot;&quot; &quot;meilleure&quot; ## [6] &quot;expérience&quot; &quot;&quot; &quot;plus&quot; &quot;belle&quot; &quot;découverte&quot; ## [11] &quot;&quot; &quot;Polynésie.Un&quot; ## [ ... and 37 more ] ## ## 3 : ## [1] &quot;&quot; &quot;séjour&quot; &quot;magnifique&quot; &quot;jours&quot; ## [5] &quot;époustouflants&quot; &quot;accueillis&quot; &quot;chaleureusement&quot; &quot;&quot; ## [9] &quot;toute&quot; &quot;l&#39;équipe&quot; &quot;&quot; &quot;Vahine_Island&quot; ## [ ... and 26 more ] ## ## 4 : ## [1] &quot;Vraiment&quot; &quot;beau&quot; &quot;cadre&quot; &quot;idyllique&quot; &quot;personnel&quot; ## [6] &quot;très&quot; &quot;attentionné&quot; &quot;&quot; &quot;&quot; &quot;adoré&quot; ## [11] &quot;&quot; &quot;séjour&quot; ## [ ... and 27 more ] ## ## 5 : ## [1] &quot;&quot; &quot;Vahiné_Island&quot; &quot;&quot; &quot;entre&quot; ## [5] &quot;&quot; &quot;&quot; &quot;monde&quot; &quot;parallèle&quot; ## [9] &quot;où&quot; &quot;&quot; &quot;&quot; &quot;sait&quot; ## [ ... and 125 more ] ## ## 6 : ## [1] &quot;&quot; &quot;&quot; &quot;adoré&quot; &quot;&quot; ## [5] &quot;séjour&quot; &quot;&quot; &quot;Vahiné_Island&quot; &quot;&quot; ## [9] &quot;&quot; &quot;enfants&quot; &quot;bungalow&quot; &quot;&quot; ## [ ... and 76 more ] dfm&lt;-dfm(toks_comp, remove_padding=TRUE) textplot_wordcloud(dfm, max_words = 200, color = rev(RColorBrewer::brewer.pal(6, &quot;RdBu&quot;))) Figure 9.6: Mots les plus fréquents du corpus 9.6.4 Identifier les autres concepts Dans ce corpus, on peut aussi sattendre à voir apparaître dautres expressions multi-mots qui représentent des concepts, telles que petit déjeuner. col&lt;-textstat_collocations(toks_comp, min_count = 10) flextable(head(as.data.frame(col))) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-939a0e0c{border-collapse:collapse;}.cl-938d1e9a{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-938d1e9b{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-938d1e9c{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-938d6c7e{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-938d6c7f{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-938d6c80{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-938d6c81{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-938d6c82{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-938d6c83{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} collocationcountcount_nestedlengthlambdazpetit déjeuner769027.51048585.88487très bien714023.59164776.20326très bon422024.07259561.79011très agréable374024.20013358.42280grand merci167025.65065055.78169passé nuits151025.51790653.58480 Au vue de la diversité des collocations, on choisit un lambda supérieur à 7 pour retenir les concepts les plus pertinents. toks_comp &lt;- tokens_compound(tok, pattern = col[col$z &gt; 7]) head(toks_comp) ## Tokens consisting of 6 documents and 21 docvars. ## 1 : ## [1] &quot;Tout&quot; &quot;&quot; &quot;magnifique&quot; &quot;&quot; &quot;Vahine&quot; ## [6] &quot;island&quot; &quot;Séjour&quot; &quot;&quot; &quot;rêve&quot; &quot;&quot; ## [11] &quot;&quot; &quot;équipe&quot; ## [ ... and 22 more ] ## ## 2 : ## [1] &quot;Tout&quot; &quot;&quot; &quot;parfait&quot; &quot;&quot; &quot;meilleure&quot; ## [6] &quot;expérience&quot; &quot;&quot; &quot;plus_belle&quot; &quot;découverte&quot; &quot;&quot; ## [11] &quot;Polynésie.Un&quot; &quot;grand_merci&quot; ## [ ... and 33 more ] ## ## 3 : ## [1] &quot;&quot; &quot;séjour&quot; &quot;magnifique&quot; &quot;jours&quot; ## [5] &quot;époustouflants&quot; &quot;accueillis&quot; &quot;chaleureusement&quot; &quot;&quot; ## [9] &quot;toute_l&#39;équipe&quot; &quot;&quot; &quot;Vahine&quot; &quot;Island&quot; ## [ ... and 26 more ] ## ## 4 : ## [1] &quot;Vraiment&quot; &quot;beau&quot; ## [3] &quot;cadre_idyllique&quot; &quot;personnel_très_attentionné&quot; ## [5] &quot;&quot; &quot;&quot; ## [7] &quot;adoré&quot; &quot;&quot; ## [9] &quot;séjour&quot; &quot;attention&quot; ## [11] &quot;spéciale&quot; &quot;&quot; ## [ ... and 22 more ] ## ## 5 : ## [1] &quot;&quot; &quot;Vahiné&quot; &quot;Island&quot; &quot;&quot; &quot;entre&quot; &quot;&quot; ## [7] &quot;&quot; &quot;monde&quot; &quot;parallèle&quot; &quot;où&quot; &quot;&quot; &quot;&quot; ## [ ... and 117 more ] ## ## 6 : ## [1] &quot;&quot; &quot;&quot; &quot;adoré&quot; &quot;&quot; &quot;séjour&quot; &quot;&quot; ## [7] &quot;Vahiné&quot; &quot;Island&quot; &quot;&quot; &quot;&quot; &quot;enfants&quot; &quot;bungalow&quot; ## [ ... and 74 more ] dfm&lt;-dfm(toks_comp, remove_padding=TRUE) textplot_wordcloud(dfm, max_words = 200, color = rev(RColorBrewer::brewer.pal(6, &quot;RdBu&quot;))) Figure 9.7: Mots les plus fréquents du corpus 9.7 Conclusion Dans ce chapitre, nous avons vu comment découper un corpus en unités, les tokens. Nous avons abordé le sujet des n-grammes, et vu comment composer des tokens à partir de concepts multi-mots, identifiés par des n-grammes adjacents. Le principe de textcat est fondée sur ces n-grammes de lettre. Chaque langue se caractérise par une distribution particulière des n-grammes. Pour décider de lappartenance dun texte à une langue, si on dispose des profils de distribution, on compare la distribution des n-grammes du texte à ces références. On peut ainsi calculer une distance et attribuer le texte à la langue dont il est le plus proche. "],["annotations-lexicales-et-syntaxiques.html", "Chapitre 10 Annotations lexicales et syntaxiques 10.1 Stemmatisation, lemmatisation et synonymisation 10.2 Part of Speech (POS) 10.3 Dépendances syntaxiques 10.4 reconnaissance dentités nommées 10.5 co-reférence", " Chapitre 10 Annotations lexicales et syntaxiques Pour aller au-delà de lanalyse du seul lexique et de lanalyse de la cooccurence des termes à travers les textes, comme le font les méthodes de typologie et danalyse factorielle des correspondance depuis longtemps, il est néçessaire danalyser le texte en tenant compte de ses propriétés syntaxiques. Depuis une dizaine dannées, des outils puissants, les annotateurs, sont proposés de manière accessible. Les plus connus sont Spacy, Stanford NLP et désormais UDpipe. Dans lenvironnement r différentes ressources sont disponibles : Quanteda, clean_nlp, Udpipe,  Ils sont disponibles désormais dans de nombreuses langues même si la richesse et la précision obtenues varient dune langue à lautres Ils sappuient sur des corpus plus ou moins étendus et spécialisés dannotations manuelle : les Treebanks. Ils réalisent souvent plusieurs tâches dont les principales sont les suivantes : Tokeniser Lemmatiser Identifier les parts of speech Identifier les dépendances syntaxiques Identifier les entités nommées. identifier les co-reférences. 10.1 Stemmatisation, lemmatisation et synonymisation Les mots prennent des formes variées, il peut être intéressant dans certains cas de réduire cette variété et ne considérer que lidée des mots. Deux techniques sont disponibles 10.1.1 la stemmatisation et la lemmatisation cest le fait de ne conserver que le radical des mots, pour regrouper sous le même radical toutes les variétés morphologique dun même mot. Il saffit dont denlever les syllabes qui correspondent aux suffixes et aux flexions du mot (mode singulier ou pluriel, genre, desinences : conjugaison et déclinaison etc..). On parle aussi de racinisation. Un lemme est un mot racine (ne pas confondre avec le radical), san inflexions de genre, de mode, de conjugaison ou de déclinaison. Cest généralement celui quon trouve dans les dictionnaires. Il sagit de ramener un terme, à sa forme la plus simple qui en français est linfinitif/masculin-singulier). 10.1.2 Synonymisation le cas de wordnet et linvention des synset synonymes, antonymes, hipponyne, hyperonymes.. https://cran.r-project.org/web/packages/wordnet/vignettes/wordnet.pdf 10.2 Part of Speech (POS) Dans une phrase les mots non pas la même valeur. Certains sont des nombres propres, ils se réfèrent à ce que nous venons de voir, cest à dire des entitées nommées, dautres désignent des catégories dobjet. Ce sont les noms communs qui se rapportent à des catégories de choses. Un marteau - si jen avais un - peut être nimporte quel marteau, la masse qui casse la pierre, ou ce petit marteau qui me permet denfoncer un clou dans le cadre du tableau. Des typologies universelles ont été construites, elles recouvrent des typologies plus spécifiques à certaines langues. Les désinences du latin ont par exemple disparu du français. Cette forme est spécifiques au latin, on la retrouvera en allemand. La notion de morphosntaxique désigne présisément que les variations de formes des mots dépendent dune règle syntaxique. Prenons le verbe, et sa forme, être, dont la forme au passé simple est était. La forme des mots change, mais lidée reste. Une catégorisation en 17 éléments est proposée. En voici les éléments et les définitions Un petit exemple avec le package UDpipe. library(udpipe) fr &lt;- udpipe_download_model(language = &quot;french&quot;) udmodel_french &lt;- udpipe_load_model(file = &quot;french-gsd-ud-2.5-191206.udpipe&quot;) Citations &lt;- read_csv(&quot;./data/Citations.csv&quot;) Flaubert&lt;-Citations %&gt;% filter(doc==1) UD &lt;- udpipe_annotate(udmodel_french, x=Flaubert$text) x &lt;- as.data.frame(UD) foo&lt;-x %&gt;% select(doc_id,paragraph_id, sentence_id, token_id,token,lemma,head_token_id, upos,feats)%&gt;%filter(sentence_id==1) flextable(foo) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-977123e4{border-collapse:collapse;}.cl-9766078e{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-97662e8a{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-97662e8b{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9766a37e{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9766a37f{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9766a380{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9766a381{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9766a382{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9766a383{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} doc_idparagraph_idsentence_idtoken_idtokenlemmahead_token_iduposfeatsdoc1111Lele2DETDefinite=Def|Gender=Masc|Number=Sing|PronType=Artdoc1112lendemainlendemain9NOUNGender=Masc|Number=Singdoc1113futêtre9AUXMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Findoc1114,,6PUNCTdoc1115pourpour6ADPdoc1116EmmaEmma9PROPNdoc1117,,6PUNCTdoc1118uneun9DETDefinite=Ind|Gender=Fem|Number=Sing|PronType=Artdoc1119journéejournée0NOUNGender=Fem|Number=Singdoc11110funèbrefunèbre9ADJGender=Fem|Number=Singdoc11111..9PUNCT Les trois première colonnes identifient le document, les phrases et les mots. Des lemmes sont proposées. La colonne UPOS donne les part of Speech universel. 10.3 Dépendances syntaxiques Cest à Lucien Tesnière que lon doit lidée de la grammaire de la dépendance qui est au coeur du NLP moderne. Lidée est de déterminer au niveau de la phrase les relations entre ses termes de manière hierarchisée selon un principe de gouvernant à subordonné. Verdelhan-Bourgade (2020) résume son analyse de manière précise et concise : Tous les mots nont pas le même statut. Les mots pleins, qui « expriment directement la pensée » (p. 59), relèvent de quatre catégories structurales : les substantifs (notés par O), les adjectifs (A), les verbes (I), les adverbes (E). Les mots dits vides (souvent désigné de manière pratique par les stopwords aujourdhui) précisent le sens des autres, ou servent à marquer des relations.La connexion établit la relation entre mot régissant et mot subordonné. Lorsquun régissant commande un subordonné, cela constitue un nud, qui peut se faire à partir dune des quatre espèces de mots pleins. Il en donne lexemple suivant : « Très souvent mon vieil ami chante cette fort jolie chanson à ma fille » où lon peut repèrer: un nud verbal, central, qui commande des actants (ami, chanson, fille) et des circonstants (souvent). La valence est « le nombre de crochets par lesquels un verbe peut attraper des actants », à peu près équivalente à « voix ». les nud substantivaux (ami, chanson, fille), qui commandent des compléments (mon, vieil, cette, jolie, ma) le nud adjectival (jolie) qui commande ici le subordonné fort le nud adverbial, très étant subordonné à souvent. \" 10.3.1 Arbre syntaxique Larbre syntaxique est obtenu en analysant les relations entre les termes. Nous poursuivons avec UDpipe, lannotation précédente a déjà fait le travail. A chaque mot deux informations sont associées : la première est lindex du mot auxquel il se rapporte, la seconde est la nature de la relation. Onn utilise ici une fonction écrite par (bnosac](http://www.bnosac.be/index.php/blog/93-dependency-parsing-with-udpipe) pour donner une représentation graphique de larbre. plot_annotation &lt;- function(x, size = 3){ stopifnot(is.data.frame(x) &amp; all(c(&quot;doc_id&quot;,&quot;paragraph_id&quot;, &quot;sentence_id&quot;, &quot;token_id&quot;,&quot;token&quot;,&quot;lemma&quot;,&quot;head_token_id&quot;, &quot;upos&quot;,&quot;feats&quot;, &quot;dep_rel&quot;) %in% colnames(x))) x &lt;- x[!is.na(x$head_token_id), ] x &lt;- x[x$sentence_id %in% min(x$sentence_id), ] edges &lt;- x[x$head_token_id != 0, c(&quot;token_id&quot;, &quot;head_token_id&quot;, &quot;dep_rel&quot;)] edges$label &lt;- edges$dep_rel g &lt;- graph_from_data_frame(edges, vertices = x[, c(&quot;token_id&quot;, &quot;token&quot;, &quot;lemma&quot;, &quot;upos&quot;, &quot;xpos&quot;, &quot;feats&quot;)], directed = TRUE) ggraph(g, layout = &quot;linear&quot;) + geom_edge_arc(ggplot2::aes(label = dep_rel, vjust = -0.20), arrow = grid::arrow(length = unit(4, &#39;mm&#39;), ends = &quot;last&quot;, type = &quot;closed&quot;), end_cap = ggraph::label_rect(&quot;wordswordswords&quot;), label_colour = &quot;red&quot;, check_overlap = TRUE, label_size = size) + geom_node_label(ggplot2::aes(label = token), col = &quot;darkgreen&quot;, size = size, fontface = &quot;bold&quot;) + geom_node_text(ggplot2::aes(label = upos), nudge_y = -0.35, size = size) + labs(title = &quot;Tokenisation, PoS &amp; dependency relations&quot;) } plot_annotation(x, size = 3) Figure 10.1: arbre de dépendance 10.3.2 Vers des application plus générale Dans la phrase precédente on note que funèbre est ladjectif de journée. On peut être tenté de retrouver ces relations qui caractérisent des choses (les nouns ou noms choses) à des adjectifs. On souhaite faire une liste de ces paires. Lexemple va être court : un poème de Maupassant Maupassant&lt;-Citations %&gt;% dplyr::filter(doc==2) UD &lt;- udpipe_annotate(udmodel_french, x=Maupassant$text) foo &lt;- as.data.frame(UD) foo&lt;- foo %&gt;% select(paragraph_id,sentence_id, token_id,lemma,upos,head_token_id,dep_rel)%&gt;% mutate(key1=paste0(paragraph_id,sentence_id, token_id),key2=paste0(paragraph_id,sentence_id, head_token_id) ) On va donc construires un tableau lemme_cible x lemmes_associés, les premiers risqueront dêtres les noms communs, les seconds leurs adjectifs. # selection de la relation. res &lt;- foo %&gt;% filter(dep_rel == &quot;amod&quot;) #on y joint les dependences dep&lt;-res %&gt;% left_join(foo, by = c(&quot;key2&quot; = &quot;key1&quot;)) #on construit la tables des relations lemmes cibles -adjectifs table&lt;-as.data.frame.matrix(table(dep$lemma.x, dep$lemma.y)) #table$n&lt;-rowSums(table) #table$adj&lt;-rownames(table) #row.names(table) &lt;- table$adj le tableau obtenu est en fait la structure dun graphe bipartite. la représentation passe par un de igraph avec pour paramètres importants : * Taille des arcs (edge) : est proportionnelle à la force du lien ( nombre de relations) * Taille des noeud : proportiennel au rangs du noeud. * Couleur et forme des noeuds : lemme et lemme cible. * Un algorithme de force de Fruchterman and Reingold (1991) est employé pour représenter les positions relatives des mots et minimiser les superpositions. Dessiner le réseau bg &lt;-graph_from_incidence_matrix(table, weighted=TRUE) summary(bg) ## IGRAPH 982e3a6 UNWB 36 20 -- ## + attr: type (v/l), name (v/c), weight (e/n) #E(bg)$weight# See the vertex attributes #V(bg)$type #V(bg)$name # Plot the network shape = ifelse(V(bg)$type, &quot;circle&quot;, &quot;square&quot;) # assign shape by node type col = ifelse(V(bg)$type, &quot;peachpuff&quot;, &quot;darkolivegreen1&quot;) # assign color by node type plot(bg, vertex.shape = shape, vertex.label.cex=.9,vertex.label.color=&quot;black&quot;,vertex.color = col,edge.color=&quot;azure2&quot;,vertex.frame.color=col, vertex.size=0.5*igraph::degree(bg),layout=layout_with_fr,edge.width=1*E(bg)$weight,edge.curved=0.5) 10.4 reconnaissance dentités nommées En français courant les entités nommées correspondent largement à lidée de noms propres. Un nom propre à une entité. Une chose qui est est indépendemment des catégories qui peuvent létiqueter. John Dupont, né le 19 février 1898 à Glasgow et abattu à Verdun le 8 août 1917, est un personnage unique. John Dupont ne désigne par une catégorie, mais bien une personne singulière. La designation peut cependant être ambigüe, il y a un Paris, Texas., et un Paris sur Seine. La morphologie ne ressout pas lambiguité. les entités nommées appartiennent à différentes catégories dobjets : des noms de lieux, des noms de personnes, des noms de marques, des acronymes dorganisation, Elles ne représentent jamais une catégorie mais une unité singulière. https://cran.r-project.org/web/packages/nametagger/nametagger.pdf 10.5 co-reférence References "],["gestion-des-données-textuelles.html", "Chapitre 11 Gestion des données textuelles 11.1 Jouer avec les tokens 11.2 Les DTM 11.3 Les ctm", " Chapitre 11 Gestion des données textuelles 11.1 Jouer avec les tokens Dans le chapitre 8 nous avons détaillé la manière dont un texte qui se présente en terme de donnée sous la forme dune chaine de caractère peut être décomposée en unités élémentaires, les termes (ou tokens). Dans le Chapitre 9, on a appris à étiqueter ces termes (ou les annoter, ou les taguer - ce sont des synonymes). de la chaine de caractère au fichier des termes Cette première transformation, permet de jouer avec les données et loutil de base pourra être DPLYR dans tidyverse dont voici quelques petites applications : Comparer la fréquence des verbes sur plusieurs périodes Une petite matrice de markov pour compléter une phrase  11.2 Les DTM Généralement une seconde opération est conduites en représentant le corps sous la forme dun tableau rectangulaire où les lignes représentent les textes, et les colonnes les termes ou réciproquement et lon partela de tdm. 11.2.1 codage Le codage de ce tableau peut prendre deux formes : présence / absence et cest alors un tableau disjonbctif complet parfait pour une AFC (chapitre 11) fréquence : nombre de fois ou le mot apparait Revenir aux nuages de mots Le nombre de fois où un mot est cité, le nombre de document dans lesquels il est présent. 11.2.2 TF - IDF La distinction sur laquelle nous avons conclu la section précédente entre fréquence des termes (Term frequency) et \"fréquence des documents (document frequency) peut être approfondie. Un terme peut être très fréquents, mais sil est présent dans tous les documents, cette fréquence est peu informative, le mot ne caractérise aucun document ou groupe de documents. Sil est rare, si rare quil napparait quun unique fois ( hapax), il risque ne ne pas apporter plus dinformation. Un mot devient caractéristique sil est fréquent et concentré dans peu de documents. Pour obtenir un indicateur de cette qualité, et pouvoir la mesurer, il faut donc pondérer la fréquence par une certaine quantité qui la minore quand le terme se distribiton dans lensemble des document. Ce poids est obtenu en prenant linverse de la fréquence des documents. \\[ idf=\\frac{n}{n_j} \\] jkhkjh \\[ x=\\sum_{i=1}^n X_i \\] du texte la formule 11.3 Les ctm 11.3.1 cooccurence Une troisième transformation résulte dun calcul de coocurrences. Les coocuurences sont linformation fondamentale, on peut en faire des matrices de coorélation, ou de distance. 11.3.2 une application à lanalyse des similarités On en profite pour introduire tsne avec un exemple où les mots de sont pas trop fréquents. Par exemple les données apps, qui sont un texte réduit à sa plus petite dimension : lélicitation dun nombre de mots. 11.3.3 une application au clustering De la même manière cette même opération peut être faite sur le document et se prêtéra aisement à un travail de typologie (ou clustering) un exemple "],["réseaux-sémantiques.html", "Chapitre 12 Réseaux sémantiques 12.1 Des analyses de proximités 12.2 igraph et les cartes sémantiques 12.3 propriétés des graphes et détection de communauté 12.4 la question du temps", " Chapitre 12 Réseaux sémantiques 12.1 Des analyses de proximités 12.1.1 les classiques le bon vieux mds 12.1.2 modèles de force des modèles inspirés par la physique ont démontré leur efficacité, ils consiste à attribuer à chaque objet une force attractive et une force repulsive. Lalgo est iteratif et converge à une situation déquilibre. kk fr 12.1.3 tsne tsne est un nouvelle génération de représentation spatiale réduite, son asctuce est de considérer un espace réduit déformé par la masse de la concentration des objets représentés. 12.2 igraph et les cartes sémantiques 12.2.1 gérer les liens 12.2.2 gérer les noeuds 12.2.3 gérer les labels 12.3 propriétés des graphes et détection de communauté 12.4 la question du temps vers un graphisme dynamique chercher quelques liens "],["vectorisation-du-corpus.html", "Chapitre 13 Vectorisation du corpus 13.1 Application avec Word2vec 13.2 Exploiter le modèle 13.3 Un clustering et une projection tsne 13.4 les perspectives", " Chapitre 13 Vectorisation du corpus Cest sans doute lidée la plus novatrice que lapproche computationnelle du langage a apporté ces 10 dernières années. Le modèle word2vec de Mikolov(2013) en est une première version, dautres ont apporté des amélioration comme le modèle Glove. Lidée fondamentale est quon peut représenter des mots dans un espace de grande dimension par des vecteurs. Ce qui importe cest de conserver la relation entre mots dans cet espace. Deux mots très corrélés, au sens de leur cooccurences, doivent lêtre avec la même intensité dans cet espace. Admettant que le cosinus de langle entre deux vecteurs est équivalent à leur corrélation, on comprend aisément que la vectorisation consiste à identifier un jeu de coordonnées, les paramètres des vecteurs mots, en connaissant les angles quis forment entre eux. De manière imaginée, il sagit de représenter le vocabulaire dun corpus ( et si ce corpus est celui de tous les corpus, dune langue) sous la forme dun oursin. Mais dans un espace à de 100 à 1000 dimensions. Si les oursins pointent leurs aiguilles dans toutes les directions, celles-ci sont contraintes à trois dimensions. Oursin Pour estimer les coordonnée des vecteurs deux méthodes peuvent être employée simultanéement. Les mots observés, dont on peut prédire le contexte (Skip-gram) Les éléments du contexte observés, dont on peut prédire le mot (CBOW) Lidée de plongement lexical tient alors dans cette dynamique double didentification et de rattachament des éléments textuels ensembles, selon différentes méthodes de vraisemblance/mesure. Le caractère remarquable de la méthode cest quil est posible dopérer des opérations algébriques, lexemple canonique est celui de reine = Roi+Homme - Femme vecteur 13.1 Application avec Word2vec pour la mise en oeuvre on emploie le package WordVec de BenJamin Schmidt sur le corpus Trump. Dans un premier temps on exploite les éléments présentés dans les chapitre précédents pour pré-processer le texte et le transformer en une séquence de termes bien tempérés : on va transformer les tweets en une séquences de lemmes, en ne gardant que les unités pleinement signifiantes : noms communs, adjectifs et adverbes, verbes. En quelque sorte un excercice de condensation du langage sur ses unités les plus signifiantes. Cest aussi loccasion de rappeler que ce qui importe dans le traitement du langage est de réduire les variations de formes pour mieux capturer les significations. 13.1.1 Préparer et annoter grammaticalement les données On prépare les données en résumant les tweets à leur plus simple expression dabord on tokenize, et on réduit les tokens en suprrimant les symbole, les nombre, en mettant en minuscules etc df_trump&lt;- read_csv(&quot;./data/TrumpTwitterArchive01-08-2021.csv&quot;) #lecture de l&#39;ensemble de nos tweets obj&lt;-df_trump$text foo&lt;-tokens(obj, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE, remove_url = TRUE, remove_separators = TRUE, split_hyphens = FALSE, padding = FALSE) %&gt;% tokens_remove(pattern = c(&quot;*.tt&quot;, &quot;*.uk&quot;, &quot;*.com&quot;, &quot;rt&quot;, &quot;#*&quot;, &quot;@*&quot;,&quot;amp&quot;, &quot;RT&quot;)) %&gt;% tokens_select(pattern=&quot;&lt;U+.*&quot;, selection = &quot;remove&quot;, valuetype = &quot;regex&quot;)%&gt;% tokens_tolower() #on reconstitue la chaine de caractère à partir des tokens transformés foo1&lt;-data.frame( id = seq_along(foo), text = sapply(foo, paste, collapse = &quot; &quot;), row.names = NULL ) et on fait de lannotations POS, comme étudié dans le chapitre X. attention ç a peut prendre du temps. 35 mn sur notre machine. cest pourquoi nous ajoutons un petit dispositif de calcul de temps pour se donner une maitrise des ajustements. le traitement du texte consomme parfois beaucoups de ressources, et il est utile den contrôler lusage. library(cleanNLP) #pour les POS et Dépendences syntaxiques # initialisation du modèle , ici udpipe, mais aussi spacy corenlp ou stringi #(un travail devrait être de comprer ces méthodes par le taux de couvrement!!!!) cnlp_init_udpipe(model_name = &quot;english&quot;) #Annotation des tweets afin de pouvoir identifier les stopwords t0&lt;-Sys.time() #date de départ Vocab&lt;-cnlp_annotate(foo1$text,verbose=10000) #le verbose fixe la notification du nombre d&#39;éléments traités, c&#39;est utiles pour savoir si on va juste prendre un café ou aller déjeuner t1&lt;-Sys.time() #date de fin.... juste pour controler une opération qui peut prendre 40 mn sur un processeur 4 coeurs à 3.6ghz et 32go de ram. #on conseille d&#39;échantillonner d&#39;abord t&lt;-t1-t0 t write_rds(Vocab,&quot;./data/Vocab.rds&quot;) donc lopération aura pris t minutes. Et un peu de filtrage sur les POS afin de se contreer sur les unités signifiantes. Vocab &lt;-readRDS (&quot;./data/Vocab.rds&quot;) foo&lt;-as.data.frame(Vocab[c(&quot;token&quot;)]) # il faut changer de format #on filtre adverbes adjectifs verb et non communs updated_vocab &lt;- foo %&gt;% filter(token.upos %in% c(&#39;ADV&#39;,&#39;ADJ&#39;,&#39;VERB&#39;, &#39;NOUN&#39;)) #on crée une chaine de caractère qui concatène les lemmes filtrés all_tweets &lt;- paste(updated_vocab[&#39;token.lemma&#39;], sep= &quot; &quot;) #on génère le fichier de ces tweets &quot;purifiés&quot; write.table(all_tweets, file=&quot;./data/tweets.txt&quot;) 13.1.2 WordVectors au travail deux étapes un preprocessing, qui permet de prendre en compte les ngrams lentrainement du modèle #install.packages(&quot;remotes&quot;) #remotes::install_github(&quot;bmschmidt/wordVectors&quot;) library(wordVectors) #Nettoyage des tweets et identification des n-grammes en vue d&#39;entraîner le modèle prep_word2vec(origin=&quot;./data/tweets.txt&quot;,destination=&quot;./data/trump_vec.txt&quot;,lowercase=T,bundle_ngrams=4) #Création et entraînement du modèle vectoriel model = train_word2vec(&quot;./data/trump_vec.txt&quot;, &quot;./data/trump.bin&quot;, vectors=200,threads=2, window=5, iter=10,negative_samples=0, force=TRUE, min_count=30) mod&lt;-model@.Data La taille du vocabulaire est de 2306 pour 401445 mots dans le fichier dentraînement. Ils se présente sous la forme dun tableau de 2306 termes, et de 200 colonnes. 13.2 Exploiter le modèle Le résultat de ce traitement est un tableau comprenant \\(m\\) termes, et \\(k\\) dimensions. lespace du langage qui était un tableau de coocurrence de taille \\(m.m\\) a été réduit à un tableau de \\(m.k\\) dimension. Si nous avions 1000 mots dans le vocabulaire et que nous le représentant en 100 dimensions, alors quil fallait \\(m*(m-1)/2)\\) paramètres, soit presque 500k, linformation est réduite à \\(m*k\\) paramètres soit 100k. dune certaine manière la vectorisation compresse les données. pour exploiter cette représentation, une première manière de faire est de rechercher dans le corpus les termes les plus associé à un terme cible. La cible de trump, on nen doute pas est biden. Pour exploiter ce tableau des fonctions pratiques sont proposées dans le package. la principale closest_to foo&lt;-model %&gt;% closest_to(~&quot;biden&quot;,30)%&gt;% filter(word!=&quot;biden&quot;) #on choisit les 30 termes les plus proches, sauf biden foo$Similarity&lt;-foo[,2] #juste pour renommer la variable g1&lt;-ggplot(foo, aes(x=reorder(word,Similarity),y=Similarity))+ geom_point(col=&quot;black&quot;,size=3)+ coord_flip()+ ggtitle(&quot;N-grammes similaires à Biden&quot;) g1 même chose pour Trump foo&lt;-model %&gt;% closest_to(~&quot;trump&quot;,30)%&gt;% filter(word!=&quot;trump&quot;) #on choisit les 30 termes les plus proches, sauf biden foo$Similarity&lt;-foo[,2] #juste pour renommer la variable g1&lt;-ggplot(foo, aes(x=reorder(word,Similarity),y=Similarity))+ geom_point(col=&quot;black&quot;,size=3)+ coord_flip()+ ggtitle(&quot;N-grammes similaires à trump&quot;) g1 On peut affiner le concept de trumpe en faisant la somme de ses noms. On laisse le lecteur faire son interprétation. foo&lt;-model %&gt;% wordVectors::closest_to(~(&quot;trump&quot;+&quot;donald_trump&quot;+&quot;mr.trump&quot;+&quot;donald&quot;),30) foo$Similarity&lt;-foo[,2] g1&lt;-ggplot(foo, aes(x=reorder(word,Similarity),y=Similarity))+ geom_point(col=&quot;black&quot;,size=3)+coord_flip()+theme_minimal()+ ggtitle(&quot;N-grammes proches de la trump+biden&quot;) g1 Quand on soustrait Biden du concept de president trump, cest Melania qui apparait. foo&lt;-model %&gt;% wordVectors::closest_to(~(&quot;trump&quot;+ &quot;president&quot;-&quot;biden&quot;),30) foo$Similarity&lt;-foo[,2] g1&lt;-ggplot(foo, aes(x=reorder(word,Similarity),y=Similarity))+geom_point(col=&quot;black&quot;,size=3)+ coord_flip()+ theme_minimal()+ scale_y_log10()+ggtitle(&quot;N-grammes proches de trump-Biden&quot;) g1 13.3 Un clustering et une projection tsne q_words = c(&quot;trump&quot;, &quot;biden&quot;) term_set = lapply(q_words, function(q_word) { nearest_words = model %&gt;% closest_to(model[[q_word]],80) nearest_words$word }) %&gt;% unlist subset = model[[term_set,average=F]] subset1&lt;-as.data.frame(subset@.Data) calculer tous les cosinus # un calcul de dissimilarité sur la base des cosinus #la fonction habituel dist ne le permetpas Matrix &lt;- as.matrix(subset1) sim &lt;- Matrix / sqrt(rowSums(Matrix * Matrix)) sim &lt;- sim %*% t(sim) #on transforme en distance la similarité cosinus, celle ci varie de 0 à 2. D_sim &lt;- as.dist(1 - sim) clustering #un clustering hiérarchique avec 10 groupes clus&lt;-hclust(D_sim) groupes&lt;- cutree(clus,k=10) library(ggdendro) ggdendrogram(clus, rotate=TRUE ,type = &quot;triangle&quot;) ddata &lt;- dendro_data(clus, type = &quot;triangle&quot;) ggplot(segment(ddata)) + geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) + coord_flip()+ geom_text(data = ddata$labels, aes(x = x, y = y, label = label), size = 2, vjust = 0) un tsne library(Rtsne) library(RColorBrewer) # run Rtsne with default parameters set.seed(57) rtsne_out &lt;- Rtsne(as.matrix(subset), perplexity=25) # plot the output of Rtsne #jpeg(&quot;fig.jpg&quot;, width=2400, height=1800) color.vec = c(&quot;#556270&quot;, &quot;#4ECDC4&quot;, &quot;#1B676B&quot;, &quot;#FF6B6B&quot;, &quot;#C44D58&quot;, &quot;seagreen1&quot;, &quot;seagreen4&quot;, &quot;slateblue4&quot;, &quot;firebrick&quot;, &quot;Royalblue&quot;) #des manip pour associer les groupe du clustering aux termes et à la leur coordonnée dans tsne. groupes&lt;-as.data.frame(groupes) groupes$word&lt;-rownames(groupes) terms&lt;-as.data.frame(rownames(subset)) terms$word&lt;-terms[,1] terms&lt;-terms %&gt;% left_join(groupes, by = &quot;word&quot;) plot(rtsne_out$Y, t=&#39;n&#39;) #count(terms, clus)$n[2] text(rtsne_out$Y, labels=rownames(subset),cex=0.8,col=color.vec[terms$groupes]) 13.4 les perspectives des cas dapplications paragraph2vec pas que les termes mais des position s ou les pos. lavenir des modèles pré-entrainés quon examiné dans le chapitre deep-learning "],["topic-analysis.html", "Chapitre 14 Topic Analysis 14.1 LDA 14.2 STM", " Chapitre 14 Topic Analysis Lanalyse thématique sattache à résumer de grands ensembles plus ou moins structurés de données textuelles en principaux thèmes probables. Les données dentrées sont des éléments textuels constituant une collection de documents, eux-mêmes composés de mots, le tout est alors à considérer comme une mélange, ou un mélange de thèmes à identifier. Ces algorithmes entraînés reposent sur des méthodes de calculs empruntent aux domaines du machine learning et de lintelligence artificielle. Les différentes variantes de sa mise en oeuvre tiennent compte : Du plongement lexical Du rôle de potentielles métadonnées à intégrer Les packages, [Word2Vec] et [stm] seront utilisés ci-dessous afin de mener les analyses. 14.1 LDA 14.1.1 Le modèle original de Blei Le rôle de cet algorithme est donc, à un corpus donné, établir un modèle possible de mélange thématique, en lisant et reliant successivement les mots entre eux. Cela est rendu possible quand les variables sont dîtes interchangeables.(???) Nous avons donc, au sens de Blei : Un vocabulaire \\(V\\) indexant tous les mots, De documents composés dune séquence de \\(n\\) mots Un Corpus \\(D\\), collection \\(m\\) de documents Un ensemble \\(Z\\) de topics potentiels Définir un réel \\(k\\) égal au nombre de topic souhaité Chaque mot \\(w\\) se voit donc associer des coefficients \\(\\beta_{i,j}\\) et \\(\\alpha_{i}\\) dans un espace \\(\\theta_{i}\\) de distribution au sein des documents, obtenu par une allocation de Dirichlet. Ainsi, on a successivement : \\[ p(\\theta|\\alpha) = \\frac{\\Gamma(\\sum_{i=1}^k \\alpha_{i}}{\\prod_{i = 1}^{k}\\Gamma(\\alpha_{i})}\\theta_{1}^{\\alpha_{1}-1}...\\theta_{k}^{\\alpha_{k}-1} \\] et lensemble des éléments prédéfinis reliés de la manière suivante : \\[ p(\\theta,z,w|\\alpha,\\beta)=p(\\theta\\|\\alpha)\\prod_{n=1}^{N}p(z_{n}|\\theta)p(w_{n}|z_{n},{\\beta}) \\] Un schéma explicatif est proposé par H. Naushan, en 2020. Daprès le théorème de Finetti, lorsque les variables sont échangeables, il est possible de les visualiser selon une infinité de mélanges. La probabilité dune séquence de mots/topics peut donc sexprimer de la sorte : \\[ p(w|z)=\\int{} p(\\theta) (\\prod_{n=1}^{N}p(z_{n}|\\theta)p(w_{n}|z_{n})d\\theta \\] Cela se considère comme une mélang continu dunigrammes ou la probabilité de rencontrer un mot se résume à sa distribution \\(p(w|\\theta,\\beta)\\) : \\[ p(w|\\theta,\\beta)=\\sum_{z}p(w|z,\\beta)p(z|\\theta) \\] La distribution marginale \\(p(w|\\alpha,\\beta)\\) de chaque document, est donc intrinsèque à cette idée de mélange de thématique, et sobtient ainsi : \\[ p(w|\\alpha,\\beta)= \\int p(\\theta,\\alpha)(\\prod_{n=1}^{N}p(w_{n}|\\theta,\\beta))d\\theta \\] 14.1.2 Une application aux commentaires trip advisor Le cas quon analyse est celui des commentaires déposés par les clients des hôtels de polynésie au cours des années 2019 et 2020. Avant dappliquer une analyse LDA, la stratégie va être le filtrer le vocabulaire pour se concenter sur les objets dont on parle et les qualités quon leurs accorde. df&lt;-readRDS(&quot;./data/Polynesie2020.rds&quot;) #library(cleanNLP) text&lt;-paste0(df$Titre,&quot;. &quot;,df$Commetaire) #initialisation du modèle de langue cnlp_init_udpipe(model_name = &quot;french&quot;) #annotation obj &lt;- cnlp_annotate(text, verbose=1000) ## Processed document 1000 of 4317 ## Processed document 2000 of 4317 ## Processed document 3000 of 4317 ## Processed document 4000 of 4317 #on extrait la table des tokens tok &lt;- obj$token Table &lt;- table(tok$upos) %&gt;% as.data.frame() g1&lt;-ggplot(Table,aes(x=reorder(Var1,Freq),y=Freq))+ geom_bar(stat=&quot;identity&quot;,fill=&quot;darkgreen&quot;)+ coord_flip()+ labs(title = &quot;Fréquence des catégories morpho-syntaxiques&quot;,x=&quot;UPOS&quot;,y=&quot;nombre de tokens&quot;) g1 De multiple solutions sont proposée dans r pour le modèle LDA, nous choississons ici le package tex2vec qui offre dautres modèles ( glove, lsa) mais dans un premier temps on filtre les tokens en fonction des UPOS, puis en écartant les termes fréquents dans plus de 95% des documents ou dans moins de 5% des documents. Le sens est toujours dans les termes médians, pas trop rares et pas trop génériques. tf &lt;- obj$token %&gt;% filter(upos %in% c(&quot;ADJ&quot;, &quot;NOUN&quot;,&quot;VERB&quot;)) %&gt;% cnlp_utils_tfidf(min_df = 0.05, max_df = 0.95, tf_weight = &quot;raw&quot;) #on commence par définir le modèle avec trois hyperparamètres ## le nombre de topics ## la probabilité a priori qu&#39;un document appartienne à un document ## la probabilité qu&#39;un mot d&#39;un document appartienne à un topic donné library(text2vec) lda_model = LDA$new(n_topics = 8, doc_topic_prior = 0.1, topic_word_prior = 0.01) set.seed(67) # on définit les paramètres du p^rocessus d&#39;estimation : le nombre d&#39;itérations, le seuil de convergence doc_topic_distr = lda_model$fit_transform(x = tf, n_iter = 1000, convergence_tol = 0.001, n_check_convergence = 25, progressbar = FALSE) ## INFO [20:44:44.052] early stopping at 450 iteration ## INFO [20:44:45.430] early stopping at 75 iteration description #description des topic en fonction d&#39;un degré de pertinence de lamba ( lambda =1 probabilités) lda_res&lt;-as.data.frame(lda_model$get_top_words()) lda_res&lt;-as.data.frame(lda_model$get_top_words(n = 15, lambda = 0.30)) lda_res$rank&lt;-as.numeric(row.names(lda_res)) lda_res&lt;-lda_res%&gt;% gather(variable, value, -rank) ggplot(lda_res, aes(x=variable, y= rank, group = value , label = value)) + scale_y_reverse() + geom_text(aes(color=variable,size=sqrt(26-rank)))+scale_color_hue()+ guides(color=FALSE,size=FALSE)+ labs(x=&quot;topics&quot;, y=&quot;par ordre de pertinence&quot;) Lanalyse nest pâs toujours évidente et le travail interprétatif est faciliter par des graphes interactif. LDAvis est un bon compagnon. Il fouunit deux outils précieux. dabord il projette les topic dans un espace à deux dimensions qui permet de représenter les distances des porfils de topic. Ensuite il nutilise pas que la probabailité quun terme soit associée à un sujet donnée, il prend on compte le caractère distinctifs du terme. Même sil est peut fréquent et peu probable quil appartiennent au topic K, mais quil ne se retrpouve qua dans le topic . Cest la potion de saillance La pertienence pondète la probabilité et la saillance : library(LDAvis) lda_model$plot() On reintègre les topics dans le fichier géneral et on en profite pour leur donner des noms explicites. topic&lt;- as.data.frame(doc_topic_distr) df&lt;-cbind(df,topic) df$MerPiscine&lt;-df$V1 df$Interaction&lt;-df$V2 df$Paradis&lt;-df$V3 df$Motulife&lt;-df$V4 df$Ballade&lt;-df$V5 df$Transit&lt;-df$V6 df$Pension&lt;-df$V7 df$Prix&lt;-df$V8 14.1.3 la determination du nombre optimal de topics perplexité (text2vec) dautres indices Etudier la plage 14.2 STM La Modélisation Thématique Structurelle est un prolongement du modèle LDA développé ci-dessus. Permettant de parvenir aux mêmes types de résultats de regroupements thématiques par plongement lexical, cette dernière se distingue dans le sens où elle permet dassocier dautres variables, ou méta-données, au corpus traité afin de prendre en compte les relations de leurs modalités au contenu. Ainsi, elle crée la notion de prévalence dun topic, qui permet de prendre en compte sa fluctuation en fonction de la propre évolution de la covariance des éléments dune même mélange. (???) "],["machine-learning-supervisé-et-nlp.html", "Chapitre 15 Machine learning supervisé et NLP 15.1 Le principe et ses applications 15.2 Une première application 15.3 Passons à un réseau de neurones et un RF 15.4 Comparons les modèles 15.5 Mais il faut expliquer 15.6 Pour finir un exercice de fine tuning", " Chapitre 15 Machine learning supervisé et NLP les techniques de machine learning supervisées ont pour but de trouver une fonction f qui associe une variable y, à un ensemble de variables x. Leur évaluation se fait en examinant la qualité de prédiction sur un échantillon nouveau. 15.1 Le principe et ses applications Les méthodes de machine learning, notamment celles quon appele classificateurs, peuvent être utile pour résoudre des problèmes de prédiction dune variable à partir du contenu textuel. Lexemple évident est quand on veut prédire la note à partir de commentaire du consommateur. Dans le même domaine on peut désirer connaitre ce qui détermine lutilité dun tel commentaire. Pour des données sociales on sintéressera par exemple à la popularité dun tweet. Et dans des usages plus sophistiqués on peut imaginer construire des catégories spécifiques , par exemple le caractère phatique ou nom dun contenu, le ton employé. Dans ce cas, il faudra au préalable annoter manuellement un échantillon de ces contenus pour développer le classificateur et lemployer opérationnnement pour le codage de lensemble du corpus. e processus va se dérouler selon les étapes suivantes 1 - on constitue les corpus de donnée. Un fichier dentrainement et un de test. 2 - Le fichier dentrainement est labellisé, etiqueté, annoté. Ce sont des termes équivalents qui désignent un processus plus ou moins objectif qui qualifie une chaine de caractère. Cette annotation peut être exogène ou endogène. Une annotation endogène est un jugement sur le contenu qui doit être annoté. Contient-il des élément toxiques ou non ? Tel tweet est positif, tel autre ne lest pas, tel tweets proteste, tel autre informe. Telle chaine de caractère est associée à un nombre de retweet élevé, tel autre à un nombre bas. 2 - on nettoie le texte, on limite les signes candidats inspiré de https://www.hvitfeldt.me/blog/binary-text-classification-with-tidytext-and-caret/ 15.2 Une première application Dans cette analyse, lobjectif va être de comprendre quel sont les mots, ou les expressions, qui sont determinent la popularité des tweets de Trump. Lobjectif va être de prédire à partir des termes, les chances quun tweet soit largement retwitté, et de comprendre quels sont les éléments qui contribuent le plus à expliquer la qualité des prédictions. On va employer caret, qui est un cadre général pour mener des projets de machine-learning, et de tidytext pour manipuler le corpus de tweets. #library(caret) df&lt;-read.csv(&quot;./data/TrumpTwitterArchive01-08-2021.csv&quot;) df$Year&lt;-str_sub(df$date,1,4) 15.2.1 Constituer le dataset On prend les données du mandat (après 2016) pour ne pas mettre de confusion et rester dans un domaine homogène. (On peut déjà envisager une extension en jouant du temps, et en répétant lanalyse selon une fenêtre de temps - on reste dans le code statique) On classe les tweets en deux catégories : ceux avec beaucoups de retweets et ceux qui en on moins. On prend le critère de la médiane pour avoir des annotations équilibrées. On envisagera plus tard ue approche en terme de régression. La dichotomisation est ici une première approche, un première approximation. On choisit de se concentrer sur la période présidentielle pour que le nombre de followers soit à peu près constant. #sélection du corpus data&lt;-df %&gt;% select(text, retweets,Year) %&gt;% filter(Year&gt;2018) #calcul de la mediane du nombre de retweets median&lt;-round(median(data$retweets),1) # codage des tweets au dessus et en dessous de la médiane data&lt;-data %&gt;% mutate(retweets_cl=ifelse(retweets&gt;median, &quot;yes&quot;,&quot;no&quot;)) #représentatio de la distribution ggplot(data, aes(retweets))+ geom_histogram(binwidth = 0.1)+ theme_minimal()+ labs(title=paste0(&quot;médiane = &quot;,median))+ scale_x_log10() data$id&lt;-row.names(data) ggplot(data, aes(retweets_cl))+geom_bar() 15.2.2 On va nettoyer le texte supprimer les liens identifier les ngrams éliminer des stopword éliminer les termes utilisés moins de k fois (ce sera 20) on crée un tableau dfm en calculant les tf-idf ( on pondère la préquence dun terme par linverse de la fréquence des documents dans lesquels il apparait) #nettoyage des données ##les liens data_clean &lt;- data %&gt;% mutate(top = retweets_cl, text = str_replace_all(text, &quot; ?(f|ht)tp(s?)://(.*)[.][a-z]+&quot;, &quot;&quot;)) %&gt;% select(id, top, text) ## les stop_word data_counts &lt;- map_df(1:3, ~ unnest_tokens(data_clean, word, text, token = &quot;ngrams&quot;, n = .x)) %&gt;% anti_join(stop_words, by = &quot;word&quot;) %&gt;% count(id, word, sort = TRUE) ## mots assez frequents words_10 &lt;- data_counts %&gt;% group_by(word) %&gt;% summarise(n = n()) %&gt;% filter(n &gt;= 30) %&gt;% select(word)%&gt;%drop_na() #we will right-join this to our data.frame before we will calculate the tf_idf and cast it to a document term matrix. data_dtm &lt;- data_counts %&gt;% right_join(words_10, by = &quot;word&quot;) %&gt;% bind_tf_idf(word, id, n) %&gt;% cast_dtm(id, word, tf_idf) #We create this meta data.frame which acts as a intermediate from our first data set since some tweets might have disappeared completely after the reduction. meta &lt;- tibble(id = as.character(dimnames(data_dtm)[[1]])) %&gt;% left_join(data_clean[!duplicated(data_clean$id), ], by = &quot;id&quot;) 15.2.3 caret sur la scène On commence à employer les fonction de caret qui permettent de distinguer les sets dentrainement et de test, mais aussi, de tester lefficacité des combinaisons dhyper-paramètres propres à chaque modèle. Caret donne accès à plusieurs dizaienes de modèles différents. Voir : xxx Nous allons tester trois types de modèles classiques : un naive bayes qui est le classique de la classification textuelle, un simple réseau de neurones. nnet. et un outil très populaire de Random forest. les données du processus dapprentissage. Le corpus est partitionné en un corpus dentrainement et un corpus de texte.Le processus dapprentissage est controllé par une méthode de crossvalidation en trois blocs. trainIndex &lt;- createDataPartition(meta$top, p = 0.7, list = FALSE, times = 1) #on partitionne data_df_train &lt;- data_dtm[trainIndex, ] %&gt;% as.matrix() %&gt;% as.data.frame() #on definit le training set data_df_test &lt;- data_dtm[-trainIndex, ] %&gt;% as.matrix() %&gt;% as.data.frame() #on définit le test set response_train &lt;- meta$top[trainIndex] #on définit l&#39;annotation trctrl &lt;- trainControl(method = &quot;cv&quot;,5, classProbs=TRUE, savePredictions = TRUE) # on définit la stratégie d&#39;entrainement. Ici on utilise une méthode de cross validation fondée sur un découpage en 5 échantillons. 15.2.4 Un modèle naive bayes Le modèle naive bayes est populaire car simple. Il calcule une probabilité conditionnelle : celle davoir un succès (s) en fonction dune série de n termes \\(t_{i}\\) . \\[p(S\\mid t_{1},...,t_{n}) =p(S)\\frac{p(t_{1},...,t_{n}\\mid S)}{p(t_{1},...,t_{n})} \\] On retrouve la très classique équation de Bayes. En faisant lhypothèse de lindépendance des probabilités conditionnelles, et en négligeant le numérateur, elle se simplifie sous la forme suivante. \\(c\\) est une constante déchelle, et la probabilité a priori dêtre un succès est pondérée par le produit des probabilité conditionnelle dobserver le terme \\(t_{i}\\) sachant que le tweet est un succès. \\[p(S\\mid t_{1},...,t_{n}) =\\frac{1}{c}p(S) \\prod_{x = i}^{n} p(t_{i}\\mid S)\\] On entraine le modèle avec un lissage de Laplace[https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece) pour éviter le problème des probabilités nulles qui résultent du fait quun mot, peut ne pas appartenir à lensemble dentrainement. nb_mod &lt;- train(x = data_df_train, y = as.factor(response_train), method = &quot;naive_bayes&quot;, trControl = trctrl, tuneGrid = data.frame(laplace = 1, usekernel = FALSE, adjust = FALSE)) nb_mod ## Naive Bayes ## ## 13173 samples ## 2861 predictor ## 2 classes: &#39;no&#39;, &#39;yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 10538, 10538, 10539, 10538, 10539 ## Resampling results: ## ## Accuracy Kappa ## 0.5943987 0.1894914 ## ## Tuning parameter &#39;laplace&#39; was held constant at a value of 1 ## Tuning ## parameter &#39;usekernel&#39; was held constant at a value of FALSE ## Tuning ## parameter &#39;adjust&#39; was held constant at a value of FALSE Examinons les résultats. La précision est de 0.59, autrement dit on prédit correctement les succès et les échecs dans 59% des cas, une prédiction aléatoire est de 50%, compte-tenu du caractère bien balancé de notre échantillon dapprentissage. Lindice kappa est un indice de concordance qui compare la répartition des classes par rapport à une répartition aléatoire. Sa valeur ici est plutôt faible. la qualité interne nest pas suffisante, on va donc évaluer le modèle ( ou clafficateur) en prédisant les valeurs sur de léchantillon test. library(MLeval) nb_mod$results ## laplace usekernel adjust Accuracy Kappa AccuracySD KappaSD ## 1 1 FALSE FALSE 0.5943987 0.1894914 0.01020197 0.02034707 nb_pred &lt;- predict(nb_mod, newdata = data_df_test, prob=TRUE) nb_cm &lt;- confusionMatrix(nb_pred, as.factor(meta[-trainIndex, ]$top)) nb_cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction no yes ## no 1546 934 ## yes 1301 1864 ## ## Accuracy : 0.6041 ## 95% CI : (0.5912, 0.6169) ## No Information Rate : 0.5043 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.209 ## ## Mcnemar&#39;s Test P-Value : 9.801e-15 ## ## Sensitivity : 0.5430 ## Specificity : 0.6662 ## Pos Pred Value : 0.6234 ## Neg Pred Value : 0.5889 ## Prevalence : 0.5043 ## Detection Rate : 0.2739 ## Detection Prevalence : 0.4393 ## Balanced Accuracy : 0.6046 ## ## &#39;Positive&#39; Class : no ## Pour être concrêt pensons aux tests covid. Ils peuvent rendre compte de la présence du virus à raison, ce sont les vrais positifs et symétriquement rendre compte de labsence de trace virale: les vrais négatifs. Ce sont les prédictions correctes. Les prédictions incorrecte sont de deux types : dans le premier on annonce une contamination alors quil ny en a pas : ce sont les faux positifs. Ils alertent sans raison et peuvent obliger un sujet sain à sisoler. Leurs symétriques sont les faux négatifs : ils annoncent que le virus est absent alors quil est tapi dans nos cellules. Les deux erreurs nont pas le même poids : la première génère de fausses peurs, la seconde la maladie. Les indicateurs de qualité du modèle sont calculés à partir du tableau de contingence suivant. Réalité Echec Succès Prédiction Echec TN FP Succès FN TP la précision (accuracy) représentent le % de bonnes prédictions : Accuracy=TP+TN/TP+TN+FP+FN. le recall ( ousensibility) : TPR=TP/TP+FN (sensitivity/ recall) TNR=TN/TN+FP (specificity) PPV=TP/TP+FP (precision/ positive predictive value) NPV=TN/TN+FN (negative predictive value) Informedness=TPR+TNR-1 Markedness=PPV+NPV-1 15.3 Passons à un réseau de neurones et un RF 15.3.1 neuralnetwook ##rdf trctrl &lt;- trainControl(method = &quot;cv&quot;, 3,classProbs=TRUE, savePredictions = TRUE) model_grid &lt;- expand.grid( mtry = 20 # mtry specified here ,splitrule = &quot;gini&quot; ,min.node.size = 20 ) ranger_mod &lt;- train(x = data_df_train, y = as.factor(response_train), method = &quot;ranger&quot;, trControl = trctrl,tuneGrid = model_grid, importance=&quot;impurity&quot;) ## Growing trees.. Progress: 63%. Estimated remaining time: 18 seconds. ## Growing trees.. Progress: 60%. Estimated remaining time: 21 seconds. ## Growing trees.. Progress: 74%. Estimated remaining time: 11 seconds. ## Growing trees.. Progress: 34%. Estimated remaining time: 1 minute, 1 seconds. ## Growing trees.. Progress: 71%. Estimated remaining time: 25 seconds. ranger_mod$results ## mtry splitrule min.node.size Accuracy Kappa AccuracySD KappaSD ## 1 20 gini 20 0.7757535 0.5516061 0.003262941 0.006579914 ranger_pred &lt;- predict(ranger_mod, newdata = data_df_test, prob=TRUE) ranger_cm &lt;- confusionMatrix(ranger_pred, as.factor(meta[-trainIndex, ]$top)) ranger_cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction no yes ## no 2151 601 ## yes 696 2197 ## ## Accuracy : 0.7702 ## 95% CI : (0.759, 0.7812) ## No Information Rate : 0.5043 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.5406 ## ## Mcnemar&#39;s Test P-Value : 0.009051 ## ## Sensitivity : 0.7555 ## Specificity : 0.7852 ## Pos Pred Value : 0.7816 ## Neg Pred Value : 0.7594 ## Prevalence : 0.5043 ## Detection Rate : 0.3810 ## Detection Prevalence : 0.4875 ## Balanced Accuracy : 0.7704 ## ## &#39;Positive&#39; Class : no ## ##nn trctrl &lt;- trainControl(method = &quot;cv&quot;, 3,classProbs=TRUE, savePredictions = TRUE) nnet_mod &lt;- train(x = data_df_train, y = as.factor(response_train), method = &quot;nnet&quot;, trControl = trctrl, tuneGrid = data.frame(size = 2, decay = 0.001), MaxNWts = 15000, importance=&quot;impurity&quot;) ## # weights: 5727 ## initial value 6165.435393 ## iter 10 value 5131.113987 ## iter 20 value 2693.268888 ## iter 30 value 1939.584540 ## iter 40 value 1508.231554 ## iter 50 value 1161.409980 ## iter 60 value 1020.246587 ## iter 70 value 955.921032 ## iter 80 value 911.245645 ## iter 90 value 872.454683 ## iter 100 value 830.462504 ## final value 830.462504 ## stopped after 100 iterations ## # weights: 5727 ## initial value 6119.111868 ## iter 10 value 4337.881229 ## iter 20 value 3049.081188 ## iter 30 value 2533.236617 ## iter 40 value 2458.913455 ## iter 50 value 2406.382560 ## iter 60 value 2391.367400 ## final value 2391.356622 ## converged ## # weights: 5727 ## initial value 6340.949667 ## iter 10 value 5563.911843 ## iter 20 value 3748.059645 ## iter 30 value 2166.637385 ## iter 40 value 1404.020770 ## iter 50 value 1116.013663 ## iter 60 value 976.942131 ## iter 70 value 901.926408 ## iter 80 value 857.298132 ## iter 90 value 822.190715 ## iter 100 value 793.690967 ## final value 793.690967 ## stopped after 100 iterations ## # weights: 5727 ## initial value 9139.423061 ## iter 10 value 7006.336540 ## iter 20 value 4525.373503 ## iter 30 value 3439.018382 ## iter 40 value 2882.863029 ## iter 50 value 2697.324188 ## iter 60 value 2621.979404 ## iter 70 value 2567.097468 ## iter 80 value 2532.072104 ## iter 90 value 2494.953088 ## iter 100 value 2471.215709 ## final value 2471.215709 ## stopped after 100 iterations nnet_pred &lt;- predict(nnet_mod, newdata = data_df_test, prob=TRUE) nnet_cm &lt;- confusionMatrix(nnet_pred, as.factor(meta[-trainIndex, ]$top)) nnet_cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction no yes ## no 1852 649 ## yes 995 2149 ## ## Accuracy : 0.7088 ## 95% CI : (0.6967, 0.7206) ## No Information Rate : 0.5043 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.4181 ## ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.6505 ## Specificity : 0.7680 ## Pos Pred Value : 0.7405 ## Neg Pred Value : 0.6835 ## Prevalence : 0.5043 ## Detection Rate : 0.3281 ## Detection Prevalence : 0.4430 ## Balanced Accuracy : 0.7093 ## ## &#39;Positive&#39; Class : no ## #nnet_mod$finalModel 15.3.2 multilayer trctrl &lt;- trainControl(method = &quot;cv&quot;, 3 ,classProbs = TRUE) neuralGrid &lt;-expand.grid( layer1 = 3, layer2 = 3, layer3 = 0, decay=0.0005 ) neural_mod &lt;- train(x=data_df_train,y = response_train, method = &quot;mlpWeightDecayML&quot;, tuneGrid = neuralGrid, # cannot pass parameter hidden directly!! trControl = trainControl(method = &quot;none&quot;)) neural_pred &lt;- predict(neural_mod, newdata = data_df_test, prob=TRUE) neural_cm &lt;- confusionMatrix(neural_pred, as.factor(meta[-trainIndex, ]$top)) neural_cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction no yes ## no 2847 2798 ## yes 0 0 ## ## Accuracy : 0.5043 ## 95% CI : (0.4912, 0.5175) ## No Information Rate : 0.5043 ## P-Value [Acc &gt; NIR] : 0.5053 ## ## Kappa : 0 ## ## Mcnemar&#39;s Test P-Value : &lt;2e-16 ## ## Sensitivity : 1.0000 ## Specificity : 0.0000 ## Pos Pred Value : 0.5043 ## Neg Pred Value : NaN ## Prevalence : 0.5043 ## Detection Rate : 0.5043 ## Detection Prevalence : 1.0000 ## Balanced Accuracy : 0.5000 ## ## &#39;Positive&#39; Class : no ## 15.4 Comparons les modèles #comparaison des modèles mod_results &lt;- rbind( nb_cm$overall, nnet_cm$overall, ranger_cm$overall ) %&gt;% as.data.frame() %&gt;% mutate(model = c(&quot;Naive-Bayes&quot;, &quot;Neural network&quot;,&quot;RF&quot;)) mod_results %&gt;% ggplot(aes(model, Accuracy)) + geom_bar(stat=&quot;identity&quot;) + ylim(0, 1) + geom_hline(yintercept = mod_results$AccuracyNull[1], color = &quot;red&quot;) 15.4.1 Une analyse plus précise de la performance https://rvprasad.medium.com/informedness-and-markedness-20e3f54d63bc avec MLeval qui fournit 4 graphes library(plotROC) library(MLeval) res &lt;- evalm(list(nnet_mod,nb_mod, ranger_mod),gnames=c(&#39;nn&#39;,&#39;nb&#39;, &#39;rf&#39;)) 15.5 Mais il faut expliquer Prédire est une chose, encore faut-il pouvoir expliquer. Les spécialistes du machines learning ont développé des outils à cette fin Lun entre eux est vip Un principe général est dexaminer comment le modèle se comporte si on enlève la variable, quel sera laccroissement de lerreur. Une variable qui contribue fortement risque daffecter fortement ce paramètre. En prenant les variables une à une ont peut alors établir leur importance relative. #explication library(vip) vip1&lt;-vip(nnet_mod, num_features = 40, geom = &quot;point&quot;, horizontal = TRUE, aesthetics = list(color = &quot;firebrick&quot;, shape = 1, size = 3)) + theme_minimal() vip1 vip3&lt;-vip(ranger_mod, num_features = 40, geom = &quot;point&quot;, horizontal = TRUE, aesthetics = list(color = &quot;firebrick&quot;, shape = 1, size = 3)) + theme_minimal() vip3 15.6 Pour finir un exercice de fine tuning Si un modèle est bon, peut il être encore meilleur ? Oui si on en contrôle les hyperparamètres par une grille de valeurs. Avec le modèle nnet on peut jouer sur le nombre dunités cachées, et de taux de decay. Attention ca peut être time intensive (plus dune dizaine dheures pour le code suivant sans parallélisation). On essaye daccélerer le processus avec le package doParallel qui va répartir les calculs sur 3 des 4 processeurs de la machine (On en garde un pour lOS). t1&lt;-Sys.time() t1 library(doParallel) cl &lt;- makePSOCKcluster(3) registerDoParallel(cl) #tuning trctrl &lt;- trainControl(method = &quot;cv&quot;, 5,classProbs = TRUE) #grille de paramètres nnetGrid &lt;- expand.grid(size = seq(from = 1, to = 4, by = 1), decay = seq(from = 0.003, to = 0.030, by = 0.002)) #train nnet_mod_t &lt;- train(x=data_df_train, y = response_train, method = &quot;nnet&quot;, trControl = trctrl, tuneGrid=nnetGrid, MaxNWts = 10000) plot(nnet_mod_t) stopCluster(cl) t2&lt;-Sys.time() t2-t1 "],["references.html", "References", " References "],["annexes-quelques-problèmes-très-techniques.html", "Chapitre 16 Annexes : quelques problèmes très techniques 16.1 La question de lencodage 16.2 Jouer avec les formats de données 16.3 Adopter des formats propres (tidy) 16.4 Les limites du calcul", " Chapitre 16 Annexes : quelques problèmes très techniques 16.1 La question de lencodage Le lecteur sera soumis très rapidement au problème compliqué et agaçant de lencodage des données. 16.2 Jouer avec les formats de données Le modèle excell appartient au passé ### json Le jason et la logique des liste ### xml lxml triomphant 16.2.1 des formats exotiques ris et bib pour la biblio 16.3 Adopter des formats propres (tidy) 16.4 Les limites du calcul Certaines opérations sont couteuse en calcul, notamment lannotation. calculer les temps de calculs parralélisation optimisation "]]
