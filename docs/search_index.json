[["index.html", "NLP avec r et en français - un Manuel synthétique Chapitre 1 Préface 1.1 Cours et séminaires 1.2 La structure du livre 1.3 Les jeux de données 1.4 Les ressources 1.5 Disponibilité 1.6 conventions 1.7 A faire", " NLP avec r et en français - un Manuel synthétique Sophie Balech et Christophe Benavent et al 2021-07-26 Chapitre 1 Préface 1 Leco système r sest enrichi ces dernière années à grande vitesse dans le domaine du traitement du langage naturel, lobjet de ce manuel a pour but den donner une synthèse. Sa vocation est pratique même si on y laissera germer quelques considérations plus méthodologiques, voire épistémologiques. On ouvrira cependant chaque fois que cest possibles aux questions théoriques et éthiques de ces méthodes. Leur réalisation computationnelle est le fruit souvent dune longue histoire, au cours de laquelle les linguistes ont semé des idées essentielles quont systématisé les informaticiens. On soignera la bibliographie de manière synthétique pour en faire un état de lart essentielet actualisée. La rédaction de louvrage est mené avec une règle de reproductibilité et de transparence, cest le pourquoi le choix de ce support et des jeux de données associés. Il sera dynamique, modifié à mesure de nos cours, séminaires, ateliers et observations des lecteurs. 1.1 Cours et séminaires La liste des cours et séminaires où il sera présenté et utilisé. Colloque Marketing digital 1-2 septembre 2021 AFM decembre 2021 Ed Sorbonne - février 2022 Dauphine master 204 - octobre 2021 Master Siren - Dauphine - mai 2022 Toulouse Lille master Data Science 1.2 La structure du livre L analyse NLP peut être analysée comme un processus qui va de la production jusqu à la diffusion des analyses. Elle est aussi traversée par des évolutions profondes de méthodes qui ont complexifié au sens formel les modèles initiaux. Lapprentissage submerge le comptage,et les catégorisations. Rappelons nous que les modèles de langages désormais distribués par les grands acteurs, comprennent des dizaines, voir des centaines de milliards de paramètres. Le plan suit une logique qui va du simple au très compliqué, et de lacquisition des données, de leur traitement et leur modélisation, jusquà la propagation. acquisition des données : directe, api et scrapping corpus dtm et cooccurence AFC et typologie lannotation syntaxique et lexicale analyse du sentiment et sa généralisation word embedness factorial models Topic analysis ML deep learning translation : parsceque quil faut traiter des corpus multi lingual et que la communication peut aussi etre multilinguales. génératives : parce que la prochaine étape cest quand on appliquera ces méthode sur la productions textuelles des bots. 1.3 Les jeux de données Au cours du développement, plusieurs cas pratiques - souvent réduit en volume pour rester exemplaires, seront employés. Les donées seront partagées. En voici la présentation systématique. Trump Twitter Archive : Lintégralité des tweets de Trump jusquà son banissement le 8 Janvier 2021. Confinement Jour J Citations : un recueil de citations littéraires pour de petits exemples et ponctuer le texte aride dun peu de littérature et de poésie. Trip advisor Polynésie, un extrait dun corpus établi par Pierre Ghewy et Sebastien de lUPF Airbnb Covid PMP David Bowie : comp disponibles dans le repositery avec le code du book. Les amendements et améliorations sont souhaitées et attendues. 1.4 Les ressources Ce livre est écrit en Markdown (Allaire et al. 2021) et avec le package Bookdown (Xie 2021) Le code sappuie très largement sur tidyverse et emploie largement les ressources de ggplot et dplyr . On recommande au lecteur de consulter donc les ouvrages suivants quand il sinterroge sur la construction des graphiques. On part du parti-pris que les lecteurs ont une connaissance satisfaisantes de ces outils génériques. Une mention particulère doit être faite sur la question du traitement du texte, stringr est aussi un des outils fondamentaux, rmardown ggplot dplyr stringer 1.4.1 Les packages Les packages seront introduits au fur et à mesure. En voici la liste complète. knitr::opts_chunk$set(echo = TRUE, message=FALSE,warning=FALSE) #boite à outils et viz library(tidyverse) # inclut ggplot pour la viz, readr et library(cowplot) #pour créer des graphiques composés library(ggridges) # le joy division touch library(citr) #networks library(igraph) library(ggraph) # Accéder aux données library(rtweet) # une interface efficace pour interroger l&#39;api de Twitter # NLP library(tokenizers) library(quanteda) library(quanteda.textstats) library(udpipe) #annotation syntaxique library(tidytext) library(cleanNLP) #annotation syntaxique #sentiment library(syuzhet) #analyse du sentimeent #mise en page des tableaux library(flextable) #statistiques et modèles library(lme4) library(jtools) library(interactions) #ML library(caret) #graphismes theme_set(theme_bw()) #palettes library(colorspace) #pour les couleurs # chapitre II library(revtools) library(rvest) library(readr) 1.5 Disponibilité Lensemble du code est disponible sur github. A ce stade, cest encore embryonnaire. Les proches pourrons cependant y voir lévolution du projet et de la progression 1.6 conventions Quelques conventions décriture du code r : On appele les dataframes de manière générale df, les tableaux intermédiaires sont appelé systématiquement foo Gestion des palettes de couleurs ** une couleur :\" royalblue\" ** deux couleurs ** 3 à 7 couleurs On emploie autant que possible le dialecte tidy. Les chunks sont notés X, le chapitre, 01 à n, les jeux. 502 est le second chunk du chapitre 4. On commente au maximum les lignes de code pour épargner le corps du texte et le rendre lisible 1.7 A faire todo list : insérer un compteur google analytics ( voir https://stackoverflow.com/questions/41376989/how-to-include-google-analytics-in-an-rmarkdown-generated-github-page) modifier le titre en haut à gauche vérifier le système de références voir ( https://doc.isara.fr/tuto-zothero-5-bibtex-rmarkdown-zotero/) Vérifier la publication en pdf restructurer le plan References "],["intro.html", "Chapitre 2 Introduction 2.1 Une réflexion ancienne et un nouveau champ méthodologique 2.2 Les facteurs de développement 2.3 De nouvelles méthodologies pour les sciences sociales 2.4 La matière du langage et la transparence de la langues 2.5 Conclusion", " Chapitre 2 Introduction Le texte connaît une double révolution. la première est celle de son système de production, il se produit désormais tant de textes que personne ne peut plus tous les lire, même en réduisant son effort à sa propre sphère dintérêt et de compétence, la seconde est celle de sa lecture, cest une lecture conditionnée et recommandée.. La production primaire de textes voit son volume croître exponentiellement. Prenons quelques exemples : La production se soumet ensuite à ceux qui en contrôlent les flux et en exploitent les contenus, qui les mettent en avant ou les écartent, définissant la composition de ce que chacun va lire. La diffusion de cette production suit des loi puissances, cest ainsi que la révolution de la lecture est venue avec les moteurs de recherche, et les pratiques de curations (ref), cest une lecture sélectionnée et digérée par les moteurs de recommandation. (ref). Sil ne fallait quun exemple on pourrait évoquer la transformation radicale de la littérature dite scientifique sur le plan technique. La recherche par mots clés est complétée de plus en plus par des outils de veille, lindexation a donné naissance à limmatriculation de la moindre note, les fichiers ont adopté des standards, linteropérabilité est de mise, le réseau des co-citations est maintenu en temps réel. Les scores qualifient autant les articles que leurs auteurs et les revues qui les accueillent. Elle risque de ce poursuivre par la production de résumés, la transcription automatique (speech2tex) etc. Le NLP est au coeur de ces technologies, il se nourrit de plus en plus dintelligence artificielle. Nous en verrons de nombreux exemple à tout les stades du traitement : identifier la langue, mesurer le sentiment, isoler des sujets. Le NLP est aussi une nouvelle ressource pour les chercheurs en sciences sociales à la fois par les matériaux empiriques et les méthodes danalyse. Cest une mouvemenjt qui affecte toutes les shs. Lemballement de la production de texte génère une nouvelle matière détude pour le sociologues, le gestionnaire, léconomiste, le psychologue pour névoquer que quelques disciplines. 2.1 Une réflexion ancienne et un nouveau champ méthodologique On se doit pas se faire aveugler par léclat de la nouveauté, les techniques daujourdhui dépendent didées semées depuis longtemps dans au moins deux champs disciplinaire la linguistique et linformatique Les pratiques et techniques que nous allons étudier ne tombent pas de mars mais résultent de plusieurs flux de pensées qui se croisent se confortent et amène lenergie pour créer un nouveau bras dans le champs immensement étendu de létude de la langue et du langage. Et cets sans doute par celà quil faut commencer. La langue cest lensemble des règle formelles et moins formelle qui constitue une parole, ce quon se dit de lun à lautre ou de lun à aux autres. Le langage est la production de cette parole. Linscription de cette parole par lécriture constitue le texte. Le miracle du passage de la parole au signe est celui du symbole. Si dans ce manuel, on choisit de présenter les différentes facettes de ce qui sappelle TAl, NPL, Text Mining, dans une approche procédurale qui suit les principales étapes du traitement des données. On rendra compte à chaque étape des techniques disponibles, et on illustre dexemples. Nous suivrons ici une approche plus fidèle au processus de traitement des données, lequel peut connaître une stratégie inférentielle et exploratoire - quelles informations sont utiles au sein dun corpus de texte -, tout aussi bien quune stratégie hypothético-déductive. Nous resterons agnostique sur cette question, restant délibérément à un niveau technique et procédural. 2.1.1 Lhéritage linguistique la convergence de deux grands mouvements. Sans en faire lhistoire minutieuse que ce domaine réclame, nous pouvons au moins rappeler un certains nombre détapes et de contributeurs clés. La langue et le langage sont lobjet dune interrogation millénaire mais quelques auteurs clés ont mis à jour les idées essentielles qui justifient lusage des méthodes actuelles. Donnons en un aperçu rapide de manière historique, en bornant un champs de connaissance que nous ne pouvons quaffleurer. les sophistes : plier le langage à ses intérêy est une première sciences du langege qui produit une connaissances des dispositifs les plus efficaces. Par sur que cette displines aient trouvé un chemine de vérité,; mais elle ereste commune, cest loeuvre de la publicité.L Saussure : il apporte une idée fondamentale que dans le symbole, le signe et le signifiant sont les deux face dune même monaiie, quil existe une relation entre lartefact et lidée. Quun signe particulier puisse signifier une idée. cest un penseur de la correspondance. Frith et lidée distributionnelle. un mot trouve son sens dans ceux qui lui sont le plus associés Zipf Tesnière et les arbres syntaxiques. Chomsky et sa grammaire génératitive. enracinant le phénomène linguistique dans la cortalisation du langage, il apporte une idée forte et structuraliste dun équivalence des langues. Genette et lintertextualité, le palimpseste. cest une question de sens, le sens dun texte vient de ses prédecesssurs de ceux à qui ils se réfèrent. Les textes se parlent lun lautre, et ce nest pas dans leur contenu quon trouvera une vérité dans dans le rapport quils établissent avec leur prédecesseur par lappareil des notes et des bibliographies. Austin et lidée que le langage nest ^pas que communication mais performation . ce quon dirt agit sur le monde La narrativité 2.1.2 la tradition lexicologique le lexique est affaire ancienne, le français est aidé par des expériences les fondamentales : le littré, lacadémie française et les dictionnaires des éditeurs. pour étudier un lanage il faut se rapporter à des formes stables, les dictionnaires les fournissent et fournisse les normes pour les coder. Lidée de quantifier le langage nest pas nouvelle. Encore moins sil faut compter les occurences et les cooccurences des mots.Un vaste mouvement sest formé dans les années soixante autour de la lexicologiue stimulée par lécole française danalyse de données. Le descendant de ce mouvement se retrouve dans lexcellent iramutek de léquipe de toulouse, il a été précedé par le fameur Alceste. Nous y consacrerons un chapitre plein sur le plan technique. Mais il est important de souligner que cette école française de lanalyse textuelle ne se limite pas au comptage. Un logiciel comme trope qui dailleurs ne connait aucun équivalent dans lécosystème que nous allons explorer manifeste aussi cette inventivité. Sy exprime pleinement la logique distributionnelle. 2.1.3 la linguistique computationnelle le frottement de la linguistique et de linformatique se produit à propos de questions pratiques. 2.1.3.1 la question de la fouille de données les nomenclatures une convergence nécessaire 2.1.3.2 la question du classement des documents Le monde des bibliothèques et celui de la GED. 2.2 Les facteurs de développement Ces développements sont favorisés par un environnement fertile dont trois facteurs se renforcent mutuellement. Ils conduisent à lélaboration de nouvelles méthodes. la naissance de langue universelle lemergence vaste ensemble de données textuelle la naissance dune communauté épistémique, de pratique et 2.2.1 Une lingua franca Le premier est lexpansion de deux langages, proprement statistique pour r et plus généraliste pour Python. Le propre de ces langages est, prenons le cas de r, de permettre délaborer des fonctions, dont un ensemble cohérent pour réaliser certaines tâches peut être rassemblé dans une bibliothèque appelée package (et chargé par library(nomdupackage)). On dispose désormais de milliers de packages (17 788 sur le CRAN) destinés à résoudre un nombre incalculable de tâches. hornik Coder une analyse revient ainsi à jouer avec un immense jeu de lego, dont de nombreuses pièces sont déjà pré-assemblées. Dun point de vue pratique, les lignes décriture sont fortement simplifiées permettant à un chercheur sans grande compétence de codage deffectuer simplement des opérations complexes. En retour, cette facilitation de lanalyse abonde le stock de solutions. 2.2.2 La multiplication des sources de données. Le troisième est la multiplication des sources de données et leur facilité daccès. le contenu écrit des réseaux sociaux les rapports dactivités des entreprises, les compte-rendu archivé de réunion Les avis des consommateurs sur les catalogues de produit Les articles et les revues scientifiques Même les livres Les plus évidentes sont proposées par les bases darticles de presse telles qu presseurop ou factiva. Les bases de données bibliographiques sont dans la même veine particuièrement intéressante et pensée pour ces usages. Les données privées, et en particulier celles des réseaux sociaux, même si un péage doit être payé pour accéder aux APIs, popularisent le traitement de données massives. Les forums et sites davis de consommateurs sont pour les sociologues de la consommation et les specialiste du comportement de consommation une ressource directe et précieuses. Le mouvement des données ouvertes (open data) proposent et facilitent laccès à des milliers de corps de données : grand débat. 2.2.3 Une communauté Le second facteur de développement , intimement lié au premier, est la constitution dune large communauté de développeurs et dutilisateurs qui se retrouvent aujourdhui dans des plateformes diverses. Le savoir, autrement dit des codes commentés se trouvent dans une varété importante de lieux : Des plateformes de dépots telle que Github qui rassemblent une trentaine de millions de developpeurs et datascientits. Des plateformes de Q&amp;A (question et réponses) telles que Stalk Over Flow, Des tutoriaux de toute sortes Des blogs ou des fédération de blog de blogs (BloggeR), Des revues (Journal of Statistical Software) et de bookdown. Des ressources abondantes sont ainsi disponibles et facilitent la formation des chercheurs et des data scientists et la résolution de leurs problèmes pratiques, quiconque narrive pas à résoudre un problème a une bonne chance de trouver la solution dun autre, à un degré de circonstance près. Elles sont dautant plus utiles que certaines règles ou conventions simposent pour fluidifier léchange. La principale est celle de lexemple reproductible. La seconde est le maintien dune éthique du partage qui encourage à partager le code, et dont une littérature importante étudie leffet positif sur les performances économiques et la durabilité [rauter]. Les externalités de réseaux y sont fortes Toutes les conditions sont réunies pour engendrer une effervescence créative. Python ou r, sont dans cet univers en rapide expansion, les langues véhiculaires qui favorise une innovation constante. Les statistiques de github en témoigne : près de 50 millions dutiliseurs, 128 millions de \" repositories\" et 23 millions de propriétaires. source voir aussi https://towardsdatascience.com/githubs-path-to-128m-public-repositories-f6f656ab56b1 2.3 De nouvelles méthodologies pour les sciences sociales Pour les chercheurs en sciences sociales (et en premier lieu pour les chercheurs en gestion où toutes les sciences sociales se croisent) cette révolution textuelle offre de nouvelles opportunités dobtenir et danalyser des données pour vérifier ses hypothèses et mener son enquête. Ce sont de nouveaux terrains, de nouvelles méthodes et un nouvel objet de recherche. Nouveaux terrains : La multiplication des sources de données, associée à leur normalisation rencontre une multiplication de techniques provenant de mulpliples discplinaire et qui convergent dans un langage commun. . production abondante davis de consommateurs, de discours de dirigeants, de compte-rendus de conseils, darticles techniques,la linguistique computationnelle, de la fouille de données, des moteurs de recommandation, de la traduction automatique, et des ressources nouvelles et précieuses pour traiter labondance des données Nouvelles méthodes : Un nouveau paradigme méthodologique se construit à la croisée de données abondantes et de techniques de traitement intelligentes. Il permet daller plus loin que lanalyse lexicale traditionnelle en incorporant des éléments syntaxiques, sémantiques, et pragmatiques, proposés par lensemble des outils des techniques de traitement du langage naturel. Il se dessine surtout une nouvelle approche méthodologique qui prend place entre lanalyse qualitative, et les traditionnelles enquêtes par questionnaires capables de traiter des corpus dune taille inédite. Le travail de (Humphreys and Wang 2018) en donne une première synthèse dans le cadre dun processus qui sarticule autour des différentes phases dune recherche : la formulation de la question de recherche, la définition des construits, la récolte des données, lopérationnalisation des construits, et enfin linterprétation, lanalyse et la validation des résultats obtenus. Un nouvel objet : on pourrait croire quavec des données massives et des techniques intelligente on assiste à un retour du positivisme qui bénéficierait enfin des instruments de mesure et de calculs qui ont permis aux les chercheur plus proches de la matière des succès majeurs. Sans doute, ladministration de la preuve va être faciliter par ces techniques et encourager levidence based policy (REF) et résoudre en partie la crise de la réplication et de la reproductibilité. Mais à mesure que ce développe lappareillage de méthode et de données, moins on peut supposer que lobservateur est neutre. Les téléscopes géants, les synchrotron, naffecte ni les galaxies lointaines ni les atomes proches. Le propre des données que lont est amené à étudier est de résulter de la confrontation dun système dobservation (certains préfèrent de parler de surveillance), et dun agent qui a des buts, une connaissance, et des ressources. Le dispositif de mesure est en lui-même performatif. Lexemple le plus évident est celui des systèmes de notation, qui sous prétexte de transparence donne la distribution des répondants précédents. Lagent qui va noter choisit la valeur en fonction dune norme apparente - la note majoritaire- et de sa propre intention - se manifester ou se confondre à la foule. Pour se donner une idée plus précise de ce mouvement, examinons quelques publications récentes dans les champs qui nous concernent. 2.3.1 Sociologie et histoire classes sociales avec word to vec en sociologie (Kozlowski, Taddy, and Evans 2019). Larticle révolution française [PNAS) On citera cependant jean-baptiste Coulmont et son obstination à étudier les entités nommées, prénoms et autres marqueurs culturels de lidentité et des classes. et au luxembourg 2.3.2 Psychologie Très tôt la psychogie sest intéressée au langage, pas seulement comme produit des processus psychologiques, mais comme expression de ceux-ci. Dans le champs de la psychologie de léducation et avec une forte motivation scientiste, dès les années 60 sest posée la question de la mesure de la difficulté dun texte pour un niveau déducation donné. La mesure de la lisibilité des texte sest développée profitant à dautre secteurs tels que ceux de la propagande. Dans cette même perspective, la richesse lexicographique comme représentant les compétences a a son tour développé de nouvelles instrumentations. James W. Pennebaker a développé son approche à partir de létude des traumas; donnant une grande importance à la production discursive des patients. Sa contribution majeure est létablissement dun ensemble de dictionnaires destinés à mesurer des caractéristiques du discours. Un instrument quon présentera dans le chapitre 7 (à vérifier). Auteur en 2011 dun livre sintéressant à lusage des petits mots. Son approche se poursuit en psychiatrie avec lanalyse des troubles du langage, et a connu un coup déclat avec la demonstration que lanalyse des messages sur les réseaux sociaux comme facebook permet de détecter des risques de dépression.(!ref) 2.3.3 Management La finance et lanalyse du sentiment Dans le champ du management, on trouvera des synthèses pour la recherche en éthique (Lock and Seele 2015), en comportement du consommateur (Humphreys and Wang 2018) en management public (Anastasopoulos, Moldogaziev, and Scott 2017) ou en organisation (Kobayashi et al. 2018) , 2.3.4 Economie economie des brevets intervention des institutions mesure de linnovation 2.4 La matière du langage et la transparence de la langues La situation nouvelle qui est la notre est que lorsque la parole disparaissait avec le vent, elle laiise des traces et senregistre. Cette matière ne sorganise plus dans les papyrus et autres manuscrits, ni même dans des livres sués, elle sincruste dans un édifice de plus en plus complexe. Le langage a acqui une dimension matérielle quil na presque jamais eu. lhistoire se définit pas une écriture, ce qui était des sociétés sans écriture sont devenues des sociétés de pure parole dont des scribes machiniques remplissent les silos de leur prise de note. Une société de procès-verbal quifacilite le travail du sociologue et de léconomiste. Des siècles durant les philologues, Nitecteches en premier e*ou en dernier se sont acharnés à trouver dans la langues des règles, à travers leur histoire. aujourdhui cest lintelligence articielle qui est à loeuvre prenant en compte que le cumul des règles est propice aux lois de probabilité. 2.5 Conclusion des techniques des méthodes References "],["constitution-du-corpus.html", "Chapitre 3 Constitution du corpus 3.1 Lexploitation de base de données textuelles 3.2 Scrapping 3.3 les API 3.4 La gestion des documents 3.5 Les contenus vocaux (speech-to-text) 3.6 Echantillonner les textes 3.7 Conclusion", " Chapitre 3 Constitution du corpus Objectifs du chapitre : ** explorer différente techniques de collectes de données : exploitation de bases textuelles, méthodes de scrapping, APIS, extraction de document pdf, extration de texte dans des images, et une perspective orale avec les techniques de speech2 tex.** La constitution du corpus est la première étape dun projet NLP. Il se définit dabord par la constitution dune collection de textes dont la provenance est la nature peut être diverse. Dans ce chapitre on va examiner plusieurs techniques de collecte : Lexploitation de bases textuelles Les méthodes de scrapping Le recours aux APIs La collection de document pas que textuels Les sources orales On conclue avec quelques réflexions sur la question de la constitution de léchantillon. 3.1 Lexploitation de base de données textuelles On commence par un exemple simple en utilisant la base Europresse. lobjectif est de constituer un fichier de références bibliographiques, exploitable via r. Dans Europresse , nous avons fait une recherche sur les articles comprenant le terme \" vaccination\" dans la presse nationale françaises, constituées de 14 titres. On retient les 150 derniers articles au 16 Juillet 2021. On utilise revtools pour sa fonction dimportation des fichiers *.RIS et de sa transformation en dataframe. #library(revtools) df &lt;- read_bibliography(iconv(&quot;./data/20210719013820.ris&quot;)) flextable(head(df,3)) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-2554bbd8{border-collapse:collapse;}.cl-2547df4e{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-25482d32{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-2548c90e{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2548c90f{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2548c910{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} labeltypetitleauthorabstractjournalpagesyearlanguageurlDAOPBy.Cecilia.Kang_2021_TheNewYorTimNEWSFacebook Says Biden Is Scapegoating Over Vaccine FalsehoodsBy Cecilia KangThe social network and the Biden administration have engaged in an increasingly rancorous war of words over the issue of vaccine misinformation. WASHINGTON -- Facebook and the Biden administration engaged in an ...The New York TimesB 32021Englishhttps://nouveau.europresse.com/Link/PARIS10T_1/news%c2%b720210719%c2%b7NY%c2%b772161119/07/202119/07/2021By.Matt.Stevens_2021_TheNewYorTimNEWSRules for Audiences Can Spin HeadsBy Matt StevensVaccination and mask requirements vary by venue. It's a weird pandemic summer for the performing arts. During its preview performances in June, New York Classical Theater was allowed to put ...The New York TimesC 12021Englishhttps://nouveau.europresse.com/Link/PARIS10T_1/news%c2%b720210719%c2%b7NY%c2%b745642019/07/202119/07/2021By.Lisa.Lerer_2021_TheNewYorTimNEWSThe Republican Path From Warp Speed Praise To Vaccine OppositionBy Lisa Lerer... 'll try to answer it. Have a comment? We're all ears. Email us at &lt;occ.email&gt; onpolitics@nytimes.com&lt;/occ.email&gt; or message me on Twitter at @llerer . By the numbers: $15 billion ... That's roughly the ...The New York TimesA 172021Englishhttps://nouveau.europresse.com/Link/PARIS10T_1/news%c2%b720210719%c2%b7NY%c2%b771554919/07/202119/07/2021 df&lt;-df%&gt;% mutate(jour=substring(DA,1,2)) g22&lt;-ggplot(df, aes(x=jour))+ geom_bar()+labs(x=NULL,y=&quot;Fréquence&quot;)+ geom_vline(xintercept=12, linetype=&quot;dashed&quot;, color = &quot;red&quot;)+ facet_grid(vars(journal)) g22 #screen_topics() #fonction de digramme interactif revtools nest pas fait que pour importer des données au format bibliographique .ris, ou au format .bib, et de les transformer un tableau observations/variables. Il a des fonctions de visualisations rapides fort efficaces. La plus spectaculaire est un outil de visualisation qui sappuie sur deux modèles de détections de topics (ce sujet sera lobjet du chapitre 8), paramétrables de manière interactive en quelques minutes, et conçu avec shiny, le package star des graphes interactifs. Cest un super outil pour avoir un premier coup doeil sur les données de manière interactive. On lapplique sur nos données. Lallure de linterface est donnée dans la copie décran suivante. screen_topic Linterface nétant pas programmatique, on exporte quelques images en jpeg (un bouton dans linterface permet de faire celà sans effort) et on les récupère avec cowplot, le package qui permet dassembler des graphes et que nous utiliserons systématiquement dans ce cours. p1 &lt;- ggdraw() + draw_image(&quot;./images/topic_espace.png&quot;) p2 &lt;- ggdraw() + draw_image(&quot;./images/topic_topic.png&quot;) p3 &lt;- ggdraw() + draw_image(&quot;./images/topic1.png&quot;) p4 &lt;- ggdraw() + draw_image(&quot;./images/topic5.png&quot;) plot_grid(p1, p2 , ncol=2) Et pour illustrer les graphiques des termes les plus proches du topic 1 et du topic 5. Lun est relatif à lactualité australienne, lautre à lactualité anglaise. plot_grid(p3, p4 , ncol=1) La méthode est sympa, rapide, sur le pouce, mais pas suffisante pour aller audelà et noatmment comparer les lignes éditoriales des deux titres que nous avons choisis. A ce stade de lanalyse cest déjà beaucoup. On peut aller plus loin, et interroger les grandes bases bibliographiques avec fulltext pour en extraire les références, les résumés ou encore les fulltext. Il faudra cependant obtenir le plus souvent une clé dAPI, et surtout faire partie dune organisation qui en à laccès ( Universités, Centres de recherches ). ces bases ne sont pas complètement ouvertes. 3.2 Scrapping Le scrapping correspond à un internet sauvage où la collecte dinformation se traduit par une technique de chasseurs-cueilleurs, le glanage. cest lactivité qui consiste à moissonner les informations disponibles sur le net en simulant et en automatisant la lecture par un navigateur ( on préfère lexpression des quebecois : des butinuers). Elle consiste à construire un robot capable de lire et denregistrer les informations disponibles sous forme html puis à les distribuer (parsing) dans des tableaux structurés, selon une stratégie dexploration du web préalablement définie. En réalité le scrapping pose deux problèmes : celui de la structure de recherche. Cest le problème que relève les spiders, des robots qui recherchent dans les pages des liens, et vont de proche en proche, de lien en lien, pour explorer un domaine.Ils peuvent être plus systématique et prendre avantage de lorganisation dun site web pour enummérer les pages. celui de la collecte de linformation sur chacune des pages. Il sappuie sur le principe que le langage html est un langage à balise où le contenu et le contenant sont clairement séparés. Par exemple, dans le corps de texte dune page on définira un titre par la balise dont linstruction sachève par la balise . On sépare ainsi clairement le contenu de la forme. ` Un titre de niveau 1 (un gros titre) &lt;p&gt;Un paragraphe.&lt;/p&gt; &lt;h2&gt;Un titre de niveau 2 (un sous titre)&lt;/h2&gt; &lt;p&gt;Un paragraphe.&lt;/p&gt; &lt;h3&gt;Un titre de niveau 3 (un sous-sous titre)&lt;/h3&gt; &lt;p&gt;Etc.&lt;/p&gt; ` Ultérieurement on pourra définir les propriétés graphiques dune balise par des CSS. par exemple avec ceci les paragraphes seront publiés en caractère bleu. p{ color: blue; } Ce qui nous intéresse nest pas la décoration, mais le fait que les développeurs définissent des balises spécifiques pour chacun des éléments de leurs page web, et que si nous savons les repérer, nous avons le moyen de mieux lire le texte. Les balises sont la cible du scrapping. 3.2.1 rvest avec r Pour scrapper avec r , le package de référence est rvest permet de réaliser des extractions simples mais suffisantes pour de nombreux usages. une application rvest https://www.r-bloggers.com/2018/10/first-release-and-update-dates-of-r-packages-statistics/ library(rvest) url = &#39;https://cran.r-project.org/web/packages/available_packages_by_date.html&#39; CRANpage &lt;- read_html(url) tbls &lt;- html_nodes(CRANpage, &quot;table&quot;) # since HTML is in table; no need to scrape td/tr elements table1 &lt;- html_table(tbls[1], fill = TRUE) dd &lt;- data.frame(table1[1]) #house cleaning dd$Date &lt;- as.Date(dd$Date) library(lubridate) # updates by year dd_y &lt;- dd %&gt;% mutate( PYear= year(Date)) %&gt;% select (PYear) %&gt;% group_by(PYear) %&gt;% summarise( nof = n() ) ### simple graph ggplot(dd_y, aes(x=PYear, y=nof)) +geom_bar(stat=&quot;identity&quot;) + labs(x = NULL, y = &quot;Nombre de packages par année de dernière mise à jour&quot;) +geom_smooth() Voici un deuxième petit exemple qui cible un forum : Uberzone Ici pour identifier les balises on emploie un utilitaire selectorgadget qui permet de manière interactive, de selectionner les balises cibles. library(rvest) # Scrape thread titles, thread links, authors and number of views start &lt;- &quot;https://uberzone.fr/threads/si-la-vaccination-devient-obligatoire-vous-feriez-vous-vacciner-ou-changeriez-vous-de-corps-de-metier.17425&quot; x&lt;-c(&quot;/page-2&quot;, &quot;/page-3&quot;, &quot;/page-4&quot;) for (val in x){ url&lt;-paste0(start,val) h &lt;- read_html(url) post &lt;- h %&gt;% html_nodes(&quot;.bbWrapper&quot;) %&gt;% html_text()%&gt;% str_replace_all(pattern = &quot;\\t|\\r|\\n&quot;, replacement = &quot;&quot;) post #authors &lt;- h %&gt;% # html_nodes(&quot;.username--style2 &quot;) %&gt;% # html_text() %&gt;% # str_replace_all(pattern = &quot;\\t|\\r|\\n&quot;, replacement = &quot;&quot;) # Create master dataset (and scrape messages in each thread in process) master_data &lt;- tibble(post) rds_name&lt;-paste0(&quot;./data/df_&quot;,substr(val,2,6),&quot;.rds&quot;) saveRDS(master_data,rds_name) } flextable(head(master_data,2)) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-2c2f44be{border-collapse:collapse;}.cl-2c25d55a{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-2c25fc4c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-2c264a30{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2c264a31{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2c264a32{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} postJe comprends pas pourquoi persistez-vous à vouloir convaincre alors que vous-même avait dit que ça ne sert à rienEt comme la cité .avatar--xss{width:21px;height:21px;line-height:21px!important;margin-right:2px} @Quinctilis en exemple même si cest pas un virus le tabac lalcool la malbouffe et autres tue tous les jours et peut-être même à plus grande échelle car jaimerais bien quon me compare le nombre de morts de lannée 2018 à celui de lannée 2020.Si vous êtes vacciné quest-ce qui vous inquiète tant répondez juste à cette question.Messages Fusionnés 15 Juillet 2021Et inutile de crier au scandale \"oh il sait pas cest quoi une dictature il connaît pas\" si je sais cest quoi une dictature et je sais très bien quen France ce nest pas le cas enfin jusquà quand, quand je vois quavec le temps qui avance il y a de plus en plus de restrictions, en tout cas obliger les gens à une idéologie cest un acte digne dune dictature mais je confirme on est pas une dictature en France car je sais que cest trop facile après de trouver ça comme excuse.Shibani a dit:Je comprends pas pourquoi persistez-vous à vouloir convaincre alors que vous-même avait dit que ça ne sert à rienEt comme la cité .avatar--xss{width:21px;height:21px;line-height:21px!important;margin-right:2px} @Quinctilis en exemple même si cest pas un virus le tabac lalcool la malbouffe et autres tue tous les jours et peut-être même à plus grande échelle car jaimerais bien quon me compare le nombre de morts de lannée 2018 à celui de lannée 2020.Si vous êtes vacciné quest-ce qui vous inquiète tant répondez juste à cette question.Messages Fusionnés 15 Juillet 2021Et inutile de crier au scandale \"oh il sait pas cest quoi une dictature il connaît pas\" si je sais cest quoi une dictature et je sais très bien quen France ce nest pas le cas enfin jusquà quand, quand je vois quavec le temps qui avance il y a de plus en plus de restrictions, en tout cas obliger les gens à une idéologie cest un acte digne dune dictature mais je confirme on est pas une dictature en France car je sais que cest trop facile après de trouver ça comme excuse.Cliquez pour agrandir...Ta cirrhose et ton Cancer du poumon (je te les souhaite pas faut pas déconner) ne sont pas transmissibles, c'est pas le cas du Covid. Tu penses quoi de la Rougeole et de la poliomyélite ? Tu vas pas vacciner t'es enfants contre? Assurément oui... Faut savoir que si la majorité ne se vaccine plus ben ces deux maladies dangereuses feraient leur grand retour. Ici avec le Covid c'est encore plus hard car plus il se propage plus il va muter et plus c'est risqué alors que propose tu de laisser courir et chacun ça chance?Malgré qu'on est dans une société capitaliste qui promeut l'individualisme des concessions pour le bien de tous sont nécessaires.Rouspéter comme vous voulez dans la très grande majorité vous allez vous vacciner bon gré mal gré parce que l'état l'a décidé pour vous. C'est malheureux mais le bâton finalement y a rien de mieux.Bonne journée et bonne recherche de créneaux libre pour la vaccination sur Doctolib.Messages Fusionnés 15 Juillet 2021Sinon pour rigoler un peu bon courage aux antivac pour trouver cette article ci-dessous... 3.2.2 Des problèmes pratiques, juridiques et éthiques La pratique du scrapping se heurte dabord à une question technique. ce nest pas un excercice facile, et il doit être confier à des spécialistes. Il se heurte aussi à différents problèmes dordre éthique et juridique. Si la pratique nest pas interdite en tant que telle, elle se confronte à différents droits et principes éthiques En termes pratiques, le scrapping crée des risques pour les sites : Le risque de deny of service, cest à dire de saturer ou de parasiter un système et de sexposer à ses contre-mesures. Il contribue à la complexification du web, et implique une consommation excessive de ressources energétiques. Et des risques pour la qualité dU recueil de données Le risque dinformation parcellaires, tronquées, inexactes qui résultent de ces contre-mesures. Les producteurs développent des stratégies moins naives. Lexemple des pages numérotée par ordre de production auxquels on substitue un nombre au hasard pour annihilier linformation temporelle. le risque matériel de mal lire les informations, pour des raison dencodage approximatifs. En termes de droits même les conditions légales relèvent de différents droits : De la propriété intellectuelle, Du respect de la vie privée, Du droit de la concurrence qui sans linterdire, condamne la copie laissant espérer quune transformation des données fasse quil y échappeR. Cependant des facilités et tolérances sont souvent accordées quand cest dans un objectif de recherche et que des précautions minimales danonymisation ou de pseudonymisation sont prises, et que les règles de conservation et de destruction des données sont précisées. En termes éthiques Un principe éthique essentiel dans la recherche, et ailleurs, et de ne pas nuire à la soci2té dans son ensemble, hors cette technique participe à la robotisation du web (plus de 50% du trafic résulterait de la circulation des spi.ders , scrapers, sniffers et autres bots, comme dans la forêt une éthique écologique revient à préveler le minimal nécessaire pour létude entreprise 3.3 les API Les API doivent être considérées comme la voie normale daccès à linformation, du moins en droit. Elles relèvent du contrat. Le recours aux APIs est civilisé, ne serait-ce parce quon introduit une sorte détiquette, des règles de courtoisie, un système de reconnaissance réciproque et dattribution de droits. Sur le plan méthodologique elles présentent davantage de donner aux requêtes un caractère reproductible , mêmes si les bases visées peuvent varier. Elles asurent une grande fiabilité des données. Lutilisation dAPI lève lambiguïté légale qui accompagne le scraping et peut ainsi paraître comme plus civilisée. Elle nécessite naturellement que le gestionnaire de la base de données fournisse les moyens de sidentifier et de requêter, elle peut avoir linconvénient dêtre coûteuse quand laccès est payant, ce qui sera de plus en plus le cas. 3.3.1 Un tour dhorizon des API La plus part des grandes plateformes offrent des API plus ou moins ouvertes, examinons-en quelques une pour comprendre plus clairement leur intérêt méthodologique. On va se concentrer sur trois exemples : le firehose de tweeter, lapi de google maps, la Crunchbase. Twitter nest pas quun réseau social, cest une gigantesque base de données qui enregistre les engagements et les humeurs de 500 millions dhumains à travers la planète et les centres dintérêt. Elle permet potentiellement de saisir les opinions à différentes échelles géeographique et temporelle, y compris les plus locales et les plus courtes. Elle a le défaut de souffrir fortement de biais de sélection, le premier étant le biais dengagement. Les passionnés dun sujets parlent plus que les autres, une parôle mieux contrôlée. Le cas de Google maps est passionnant à plus dun égard. le premier dentre eux est que dans leffort dindicer chaque objet de la planête, la base de données devient un référentiel universel, plus quune représentation intéressée du monde. Quand lutilisateur communs cherche un chemin optimal, lanalyste de donnée trouve un socle pour ordonner le monde. intégrité des bases de données, universalité des élément, interopérabilité, disponibilité Les problèmes posés : justesse , précision et représentativité. leur constitution nest pas aléatoire, leurs couverture reste partielle. accessibilité, la privatisation du commun. Si pour le chercheur les APIS sont sur un plan de principe une merveille sur un plan plus social elle instaure des inégalités daccès énormes aux données qui permettent de valoriser la connaissance. Ce mécanisme opère via deux canaux. Le premier est celui de la tarification qui ségrège les chercheurs en fonctions des ressources dont ils disposent. Le second passe par la couverture du champs, les données les plus précises et les plus denses se trouvent dans les régions les plus riches. des catégorisations peu délibérées 3.3.2 un point de vue plus technique https://www.dataquest.io/blog/r-api-tutorial/ 3.3.3 Un exemple avec Rtweet https://cran.r-project.org/web/packages/rtweet/vignettes/intro.html Plusieurs packages de r permettent dinterroger le firehose ( la bouche dincendie!) de twitter. https://www.rdocumentation.org/packages/rtweet/versions/0.7.0 Lauthentification ne nécesssite par de clé API, il suffit davoir son compte twitter ouvert. Cependant la fonction lookup_coords requiert davoir une clé dapi ou google cloud map. Elle permet de selectionner sur un critère géographique. https://developer.twitter.com/en/docs/tutorials/getting-started-with-r-and-v2-of-the-twitter-api #une boucle pour multiplier les hashtag x&lt;-c(&quot;#getaround&quot;,&quot;#Uber&quot;, &quot;#heetch&quot;) for (val in x) { tweets &lt;- search_tweets(val,n=20000,retryonratelimit = TRUE)%&gt;% #geocode = lookup_coords(&quot;france&quot;) mutate(search=val) write_rds(tweets,paste0(&quot;tweets_&quot;,substring(val,2),&quot;.rds&quot;)) } df_blablacar&lt;-readRDS(&quot;./data/tweets_blablacar.rds&quot;) df_uber&lt;-readRDS(&quot;./data/tweets_uber.rds&quot;) df_heetch&lt;-readRDS(&quot;./data/tweets_heetch.rds&quot;) df&lt;-rbind(df_blablacar,df_uber ) ls(df_blablacar) foo&lt;-df %&gt;% select(account_lang, geo_coords,country_code, country, account_lang,place_name) On laisse le lecteur explorer les différentes fonctionnalités du package. On aime cependant celle-ci qui sample le flux courant au taux annoncé de 1%. Voici lextraction de ce qui se dit en france pendant 10 mn (600s). La procédure peut donner une sorte de benchmark auquel on peut comparer une recherche plus spécifique. rt &lt;- stream_tweets(lookup_coords(&quot;france&quot;), timeout = 600) 3.3.4 Quelques apis et packages intéressants : le très récent genius pour accéder aux lyrics de Genius est à tester pour des études de culture populaires. Google maps La Crunchbase construite par le média Techcrunch repertorie les créations de start-up et les levées de fonds quelles ont obtenues. Elle recence les dirigeants, les acquisitions, décrit les business model. 3.4 La gestion des documents Les documents qui constituent le corpus peuvent se présenter de plusieurs manière. La question de la gestion des format est souvent importante mais laborieuse 3.4.1 tidy data, json Lapproche tidy data est générique mais est mise en oeuvre parfaitement avec luniversel tidyverse. Tidy, signifie en anglais, propre ou rangé, celà donne lesprit. Des données propres répondent à des caractéristiques précises voir : https://r4ds.had.co.nz/tidy-data.html Il existe trois règles interdépendantes qui permettent de mettre en ordre un ensemble de données : Chaque variable doit avoir sa propre colonne. Chaque observation doit avoir sa propre ligne. Chaque valeur doit avoir sa propre cellule. Pourquoi sassurer que vos données sont ordonnées ? Il y a deux avantages principaux : Il y a un avantage général à choisir une façon cohérente de stocker les données. Si vous disposez dune structure de données cohérente, il est plus facile dapprendre les outils qui fonctionnent avec elle, car ils ont une uniformité sous-jacente. Il y a un avantage spécifique à placer les variables dans des colonnes car cela permet à la nature vectorielle de R de briller. Comme vous lavez appris dans les fonctions mutate et summary, la plupart des fonctions R intégrées fonctionnent avec des vecteurs de valeurs. Cela rend la transformation de données ordonnées particulièrement naturelle. #en attendant Mettez chaque ensemble de données dans un tibble. Mettez chaque variable dans une colonne. Dans cet exemple, seul le tableau 1 est ordonné. Cest la seule représentation où chaque colonne est une variable. cest le langage général des apis, il répond à une contrainte : dans un champs cest une liste de valeur qui peut être contenue, On peut avoir ainsi des listes de liste. Prenons un livre, il a un ou plusieurs auteurs, un seul titre, le titre de plusieurs chapitres. Et on souhaite mettre en oeuvre dans un seul format. permet de construire des structures aborescentes. Jsonlite https://cran.r-project.org/web/packages/jsonlite/vignettes/json-aaquickstart.html voir aussi https://paldhous.github.io/NICAR/2019/r-text-analysis.html http://zevross.com/blog/2015/02/12/using-r-to-download-and-parse-json-an-example-using-data-from-an-open-data-portal/ https://www.r-bloggers.com/2020/09/reading-json-file-from-web-and-preparing-data-for-analysis/ https://cran.r-project.org/web/packages/fulltext/fulltext.pdf 3.4.2 Extraire du texte des pdf Le package pdftools est parfaitement adapté à la tâche. Des fonctions simples extraient différents éléments du pdf : les information relative au document pdf lui-même La liste des polices employées Les attachements La table des matières ( si elle a été encodée) et naturellement le texte dans un ordre de droite à gauche et de ligne à ligne, reconnaissant cependant les retrour chariot, et sauts de lignes. Chaque page est contenue dans une ligne. On comprendra que lextraction va nous donner un texte en vrac! library(pdftools) info &lt;- pdf_info(&quot;./pdf/2021neoliberalismegouverner_Meunier_Esprit.pdf&quot;) #les meta donnée du fichier fonts &lt;- pdf_fonts(&quot;./pdf/2021neoliberalismegouverner_Meunier_Esprit.pdf&quot;) # les polices utilisée dans le fichier files &lt;- pdf_attachments(&quot;./pdf/2021neoliberalismegouverner_Meunier_Esprit.pdf&quot;) # les attachements toc &lt;- pdf_toc(&quot;./pdf/2021neoliberalismegouverner_Meunier_Esprit.pdf&quot;) #il n&#39;y a pas de table des matières dans ce texte #mais ce pourrait être utile text &lt;- pdf_text(&quot;./pdf/2021neoliberalismegouverner_Meunier_Esprit.pdf&quot;) #extraire le texte, avec ses éléments de mise en page cat(text[[1]]) # pour afficher le texte de la première page ## Le néolibéralisme ## et lart de gouverner ## À propos de Naissance de la biopolitique ## de Michel Foucault ## François Meunier ## ## ## ## ## O n dit parfois du métier de lhistorien quil consiste avant tout ## à découper en périodes, à indiquer les ruptures dans le temps ## historique, à montrer les changements denvironnement et ## de paradigme. Cest à ce travail que se consacre Michel Foucault dans ## son célèbre cours de 1978-1979 au Collège de France connu sous le ## nom de Naissance de la biopolitique 1. Il devait porter initialement sur la ## « biopolitique », un mot chatoyant recouvrant les pratiques politiques ## contemporaines autour du vivant (santé, démographie, sexualité, etc.). ## Mais Foucault voulait montrer dabord à quel point la venue du libé- ## ralisme avait modifié en profondeur les pratiques gouvernementales. ## Première rupture, celle advenue à la fin du xviiie siècle avec le libéralisme ## économique classique, selon lequel le marché devient linstance clé dans ## lart de gouverner, donnant à laction publique un lieu de légitimation en ## même temps que des limites. Seconde rupture, celle qui sépare libéralisme ## et néolibéralisme, que Foucault situe dans laprès-guerre en Allemagne, ## avec ce quon appelle « lordolibéralisme ». ## ## ## Équivocité du néolibéralisme ## Reprenant, quelque quarante ans après, le fil de ce cours, nous remettons ## ici en cause le découpage historique. Dabord, il nous semble que ce ## ## 1 - Michel Foucault, Naissance de la biopolitique. Cours au Collège de France (1978-1979), Paris, ## EHESS/Seuil/Gallimard, 2004. ## ## ## ## ## · ESPRIT · Mai 2021 83/ Le texte extrait nest pas directement utilisable, Il va falloir le traiter en analysant précisément sa composition, et en définissant une séquence dopérations logiques qui permettent un premier nettoyage du texte. Dans lexemple, on va de plus essayer de respecter la structure en paragraphes du texte. Lordre des opérations est le suivant Suprimer haut et bas de pages Supprimer les sauts de ligne Identifier les sauts de paragraphe Enlever les notes de bas de page Corriger lhyphénation () Grouper les document en un seul bloc de texte le splitter en autant de paragraphes. On va utiliser des fonctions de traitement de chaines de caractère avec Stringret le recours à lart (ici simple, voire minimal) des regex auxquels on consacre un développement dans le chapitre X. (il va falloir gérer les réferences internes !!!!) tex&lt;- as.data.frame(text) tex[1,] ## [1] &quot;Le néolibéralisme\\net lart de gouverner\\nÀ propos de Naissance de la biopolitique\\nde Michel Foucault\\nFrançois Meunier\\n\\n\\n\\n\\nO n dit parfois du métier de lhistorien quil consiste avant tout\\n à découper en périodes, à indiquer les ruptures dans le temps\\n historique, à montrer les changements denvironnement et\\nde paradigme. Cest à ce travail que se consacre Michel Foucault dans\\nson célèbre cours de 1978-1979 au Collège de France connu sous le\\nnom de Naissance de la biopolitique 1. Il devait porter initialement sur la\\n« biopolitique », un mot chatoyant recouvrant les pratiques politiques\\ncontemporaines autour du vivant (santé, démographie, sexualité, etc.).\\nMais Foucault voulait montrer dabord à quel point la venue du libé-\\nralisme avait modifié en profondeur les pratiques gouvernementales.\\nPremière rupture, celle advenue à la fin du xviiie siècle avec le libéralisme\\néconomique classique, selon lequel le marché devient linstance clé dans\\nlart de gouverner, donnant à laction publique un lieu de légitimation en\\nmême temps que des limites. Seconde rupture, celle qui sépare libéralisme\\net néolibéralisme, que Foucault situe dans laprès-guerre en Allemagne,\\navec ce quon appelle « lordolibéralisme ».\\n\\n\\nÉquivocité du néolibéralisme\\nReprenant, quelque quarante ans après, le fil de ce cours, nous remettons\\nici en cause le découpage historique. Dabord, il nous semble que ce\\n\\n1 - Michel Foucault, Naissance de la biopolitique. Cours au Collège de France (1978-1979), Paris,\\nEHESS/Seuil/Gallimard, 2004.\\n\\n\\n\\n\\n· ESPRIT · Mai 2021 83/\\n&quot; t_reg&lt;-str_replace(tex$text,&quot;[\\\\s+].*Meunier[\\n]+&quot;, &quot; &quot;) # entete droite : on selectionne tout bloc de texte qui commence par un nombre indéterminée de blanc qui s&#39;achève par n&#39;importe quel caractère répétés mais terminé par la séquence Meunier suivie de sauts de ligne. t_reg&lt;-str_replace(t_reg,&quot;[\\\\s+].*gouverner[\\n]+&quot;, &quot; &quot;) # entete gauche t_reg&lt;-str_replace_all(t_reg,&quot;[\\\\s+].*2021[\\n]&quot;, &quot; &quot;) # bas de page gauche t_reg&lt;-str_replace_all(t_reg,&quot;ESPRIT.*[\\n]&quot;, &quot; &quot;) # bas de page droit #on marque les paragraphes avec la chaine XXX pour les splitter dans un second temps t_reg&lt;-str_replace_all(t_reg,&quot;\\n\\n\\n&quot;, &quot;XXX&quot;) # On supprime les saut de ligne en les remplaçant par un espace t_reg&lt;-str_replace_all(t_reg,&quot;[\\n]&quot;, &quot; &quot;) #on enlève les notes de bas de page t_reg&lt;-str_replace_all(t_reg,&quot;\\\\d\\\\s[\\\\-].*XXX&quot;, &quot;XXX&quot;) #on regroupe les pages t&lt;-paste(unlist(t(t_reg)), collapse=&quot; &quot;) #on enlève les notes dans le texte t&lt;-str_replace_all(t,&quot;[A-Z|a-z]+\\\\d\\\\s[\\\\-]&quot;, &quot; &quot;) t&lt;-str_replace_all(t,&quot;\\\\d\\\\d\\\\s[\\\\-]&quot;, &quot; &quot;) #hyphenation t&lt;-str_replace_all(t,&quot;[A-Z|a-z]+[\\\\-]\\\\s&quot;, &quot;&quot;) #pour enlever les espaces excedentaires t&lt;-str_squish(t) t ## [1] &quot;Le néolibéralisme À propos de Naissance de la biopolitique de Michel Foucault O n dit parfois du métier de lhistorien quil consiste avant tout à découper en périodes, à indiquer les ruptures dans le temps historique, à montrer les changements denvironnement et de paradigme. Cest à ce travail que se consacre Michel Foucault dans son célèbre cours de 1978-1979 au Collège de France connu sous le nom de Naissance de la biopolitique 1. Il devait porter initialement sur la « biopolitique », un mot chatoyant recouvrant les pratiques politiques contemporaines autour du vivant (santé, démographie, sexualité, etc.). Mais Foucault voulait montrer dabord à quel point la venue du libé- ralisme avait modifié en profondeur les pratiques gouvernementales. Première rupture, celle advenue à la fin du xviiie siècle avec le libéralisme économique classique, selon lequel le marché devient linstance clé dans lart de gouverner, donnant à laction publique un lieu de légitimation en même temps que des limites. Seconde rupture, celle qui sépare libéralisme et néolibéralisme, que Foucault situe dans laprès-guerre en Allemagne, avec ce quon appelle « lordolibéralisme ».XXXÉquivocité du néolibéralisme Reprenant, quelque quarante ans après, le fil de ce cours, nous remettons ici en cause le découpage historique. Dabord, il nous semble que ce XXX · nest pas autour de la notion de marché quil faut attacher la genèse du libéralisme classique, mais plutôt autour de lidée dune société capable de sorganiser en dehors du prince. Ensuite, la rupture constituante du néolibéralisme se situe postérieurement à larrivée de Reagan et Thatcher au pouvoir, lorsquon aura théorisé et mis en pratique la financiarisation de léconomie comme instance ordonnatrice (cela donc après le cours de Foucault). Sagissant de lordolibéralisme, il présente de fortes continuités avec le libéralisme classique, même sil garde les marques dun mer- cantilisme qui sest développé tardivement en Allemagne. Il nest guère étonnant que ce courant se soit fondu si aisément dans le modèle social de marché auquel on associe davantage la social-démocratie allemande que le libéralisme débridé. Cela devrait aider à mieux caractériser ce quil faut entendre par « néo- libéralisme », un mot devenu équivoque. Si lon peut créditer Foucault dêtre parmi ceux qui lont inventé, cest plus chez lui par commodité verbale2. Lorsquil rédigea le résumé du cours au terme de son année, cest significativement le seul mot de « libéralisme » quil a retenu. Le cours bénéficie toujours dune forte aura. En effet, il est la seule incursion de Foucault dans lhistoire contemporaine ; il introduit le concept de gouvernementalité, qui a acquis une certaine place dans la science politique. Son style attire, mélange décrit et doral où la pensée se construit par bonds successifs et inattendus, avançant « en écrevisse » comme il le dit. Le texte déroute aussi parce quil ne cherche pas à construire un contre-modèle quand il analyse le courant intellectuel libéral. La dissection des textes saccompagne à lévidence chez lui dune certaine fascination. Il rabroue même son public quand celui-ci voudrait le voir glisser vers des objections trop faciles au libéralisme3. Il nest pas étonnant que des milieux se réclamant du libéralisme, y compris poli- tique, sen réclament presque autant que ses adversaires.XXX XXX Enfin et surtout, le texte se concentre pour lessentiel sur lAllemagne, un pays que Foucault connaissait très bien pour y avoir enseigné : « Dans cette seconde moitié du xxe siècle, le libéralisme est un mot qui nous vient dAllemagne. » Cest lordolibéralisme quil désigne comme « néolibéral ». Il traite assez peu, de façon surprenante, de ce quon appelle lÉcole de Chicago, que Foucault appelle lanarcho-libéralisme, née dans les années 1930, dont linfluence a été majeure dans le renversement idéologique opéré à lépoque de Reagan aux États-Unis : « Je ne suis pas sûr davoir le temps de parler des Américains. »XXXLâge classique Léconomie politique, à partir de Turgot et Smith, sest bâtie sur une critique du régime mercantiliste. Le mercantilisme, cest lidéologie dun État en constitution, qui organise lhégémonie du prince, qui pour cela capte des richesses, sorganise administrativement, privilégie la bonne collecte des impôts et lexportation, et où un ordre juridique se substitue au droit divin du souverain. Cette critique avait commencé sur le plan des idées politiques. Chez Locke et Spinoza, le citoyen naissait et la liberté politique était réclamée. Mais on ne touchait pas encore à lorganisation sociale du royaume. Le pas en avant fait par léconomie politique a été de donner toute sa place à la nouvelle classe des marchands, consciente désormais de contribuer à lenrichissement du royaume. On quittait une vision assez prédatrice, où léchange était essentiellement un jeu à somme nulle, où ce que gagnait un pays était une perte pour lautre et où le talent du prince consistait à ce que son pays se sorte bien de cette confrontation. La vision classique est inverse : il y a possibilité dun échange équitable, qui se fait fina- lement au bénéfice  « à lintérêt »  des deux parties4. Il y a possibilité dune croissance endogène où le marchand réinvestit son profit dans des activités nouvelles, une morale que le puritain anglais avait parfaitement intériorisée. Il sintroduit à plein la notion de concurrence qui gomme les situations de rente. Naissait dans la foulée la notion dintérêt général et de bien commun, davantage au centre des intérêts individuels dans la XXX · tradition britannique que lexpression dune souveraineté première dans la tradition française. Foucault décrit cette transition mais force le trait quand il indique, dans une phrase significative, que cet âge classique est celui où le marché devient un principe de régulation politique en remplacement de lordre juridique qui prévalait auparavant. Selon lui, cest désormais la légi- timité marchande qui non seulement limite mais organise et structure la décision publique. Elle devient le « lieu de véridiction » de laction gou- vernementale : « Ce lieu de formation de la vérité [], il faut le laisser jouer avec le moins dinterventions possibles pour quil puisse et formuler sa vérité et la proposer comme règle et norme à la pratique gouvernementale. Ce lieu de vérité, cest bien entendu non pas la tête des économistes, mais le marché 5. » Mais Foucault anticipe de près de deux siècles. Dune part, la prévalence du droit est plus manifeste encore à lépoque classique quà âge préclassique ; les structures de marché sapprofondissent et sappuient sur des contrats établis, le mot étant dailleurs repris dans la notion de contrat social qui naît à cette période. Dautre part, il ny a pas pour les classiques une rupture dans la conception du marché. Les prix auraient été avant cette période, dit Foucault, des prix ordonnés selon des critères déquité ou de stabilité sociale, des prix « justes » et non, comme postérieurement, des prix régis par la loi de loffre et la demande. Cest ce que dément une bonne part de lhistoriographie moderne, à partir dauteurs comme de Roover ou Todeschini6, pour qui le juste prix nétait rien dautre que le prix de marché, mais dun marché mis en position de bien fonctionner, une idée qui fera apparaître progressivement le concept de concurrence, très net chez les auteurs de la seconde scolastique dans lEspagne de la fin du xvie siècle, celui de monopole étant déjà ancien. La rupture, majeure, avec le mercantilisme existe, mais elle est que la société peut se libérer du prince, que léconomie peut fonctionner malgré sa dispersion, sans coordination venue den haut. La fameuse « main invisible », dans le seul passage de La Richesse des nations où Smith la mentionne, y est comme une image pour résoudre ce paradoxe dune XXX économie qui évite le chaos alors que les centres de décision (de pouvoir) sont dispersés, chacun deux ne prenant nullement en compte la décision des autres. Il sagit là dune notion commune qui exprime la différence entre la cause finale dune action et lintention des agents7. Sur ce point, nos économistes restent même en retrait du libéralisme politique dun Locke ou dun Montesquieu pour qui la société fonctionne non pas malgré sa dispersion mais grâce à une dispersion des pouvoirs et des centres de décision, qui devient lélément régulateur par lequel on atteint le bien commun. Dans ce contexte, le gouvernement prend une place très différente. La société connaît des contraintes et interactions multiples et lart de gou- verner consiste à les prendre en compte. Il sintroduit une rationalité différente, une rationalité des fins, que Foucault décrit très bien. La question posée est celle de lutilité ou de lefficacité des effets induits dune mesure. Mais ce qui est important Ce qui est important et moins et moins bien vu par Foucault est que bien vu par Foucault est que cette rationalité nouvelle peut justifier cette rationalité nouvelle peut autant lintervention publique que son justifier autant lintervention retrait. On pourra soutenir dans un cas publique que son retrait. que lautonomie de la société signifie que le gouvernement est en trop, quil perturbe lordre économique ou quil interfère de façon coercitive sur la liberté individuelle. Mais on peut alternativement affirmer que le gouvernement a en main des guides et préceptes, une rationalité économique, qui lautorisent et qui même le poussent à intervenir dans lordre économique. On voit poindre ici une bifurcation profonde, toujours actuelle, qui verra tout à la fois un Hayek libertarien ou un Keynes interventionniste se réclamer de la tradition économique libérale. À vrai dire, pour ce second courant libéral, on peut corriger la phrase de Foucault citée plus haut : autant que dans le marché, le lieu de vérité est dans la tête des économistes. Ils affichent déjà leur prétention en venant XXX · comme des ingénieurs sociaux, comme les Locke et Montesquieu lont été en matière dinstitutions politiques, devisant le bon mécanisme ou le bon montage. On le voit déjà chez Quesnay, un physiocrate qui précède Smith, dans ses réflexions sur ce que doit être un « bon impôt », cest-à-dire un impôt efficace, dans le sens de lintérêt bien compris du royaume. Turgot le sera plus encore. Et au siècle suivant, des économistes comme Cournot ou Bertrand introduiront lidée dun calcul économique aux fins dune utilité sociale maximale, et cela dans des domaines comme la décision de construire un pont, où le marché est absent et na rien à dire.XXX Lordolibéralisme en Allemagne Cest un sujet détonnement, mal couvert par les historiens, que le libé- ralisme ait eu une place si réduite dans la riche tradition intellectuelle allemande, Kant faisant bien sûr une immense exception. Remontant hardiment dans le temps, lune des explications peut tenir au luthéria- nisme dans sa version allemande, qui a été tout autant une école dindi- vidualisme face au divin que de soumission face au pouvoir temporel, un terrain peu propice à lélaboration dun pacte collectif donnant sa légitimité au pouvoir, comme dans la tradition anglaise ou française. Un autre facteur déterminant tient au retard allemand à construire son État national. Il la fait tout du long du xviiie siècle (sagissant de la Prusse) et du xixe siècle sous la houlette prussienne, avec un vif ressen- timent vis-à-vis des autres États européens déjà fortement constitués, et particulièrement de la Grande-Bretagne qui imposait son libéralisme à coups de libre-échange. Le romantisme, au début de ce siècle, en a été le pendant culturel, avec une forte dimension nationaliste, contredisant lidéal libéral des Lumières. Le Zollverein (une union douanière entre États allemands mise sur pied dans les années 1830) exprimait bien le vu des élites libérales à louest de lAllemagne, mais était dans la réalité un projet dessence protectionniste actionné par la Prusse et théorisé par Friedrich List comme laffirmation dune souveraineté propre. Il a échoué en tant que projet politique. Au fond, lAllemagne a connu son moment mercantiliste, mais avec un siècle et demi de retard. Ce sont les notions de richesse de lÉtat, de levées dimpôt, de contrôle de la monnaie, de commerce extérieurXXX devenu lenjeu dâpres batailles avec les pays voisins qui prévalaient. Un épisode intellectuel intéressant, qui allait préfigurer les tenants de llibéralisme et les distinguer des libéraux tant classiques quétats-uniens, a été celui des « sciences camérales ». Il sagissait décoles administratives mises en place par le roi de Prusse pour former le personnel capable dassurer la puissance du souverain, non par des moyens militaires (cela allait venir plus tard), mais par une gestion réglée du pays, par voie de « police », selon le mot retenu à lépoque et commenté longuement par Foucault. Cette tradition sest poursuivie à lépoque de Bismarck puis sous Weimar avec ce grand pré-keynésien qua été Rathenau. Les penseurs de lordolibéralisme ont tout à la fois bousculé cette tradition et en ont subi linfluence. Les deux grands noms sont Wilhelm Röpke et Walter Eucken, suivis par Ludwig Erhard, futur chancelier, qui a donné au mouvement sa consistance politique. Ils ont bâti toute lossature idéologique du Parti chrétien-démocrate allemand, sous le terme d« éco- nomie sociale de marché », un terme que le SPD allait adopter lui aussi, soucieux, devant la popularité de la notion, de ne pas être durablement exclu du pouvoir. Lordolibéralisme se caractérise dabord par un rejet de lintervention directe de lÉtat dans le jeu de léconomie : ni rôle stabilisateur, ni rôle redistributeur. Second principe, proche du premier, le gouvernant doit écouter ce que dit le système des prix, cest-à-dire linformation quapporte le fonctionnement dun marché concurrentiel. Mais il sagit ici dun choix presque autant forcé quidéologique. Tony Judt, dans son his- toire de laprès-guerre8, signale cet épisode déterminant qua été le début de la guerre froide. Les États-Unis ont imposé à la Grande-Bretagne et à la France de se réarmer. Mais bien sûr pas à lAllemagne. Or celle-ci gardait largement intact son appareil industriel de guerre, qui sest mis naturellement à tourner pour nourrir le reste des pays européens en produits venus de la mécanique ou de la chimie. Le modèle extraverti propre à lAllemagne était lancé, une voie quont trouvée plus tard le Japon et les autres pays asiatiques. Sappuyer sur la logique du marché international devenait alors lélément clé dune stratégie imposée par lenvironnement autant que par la doctrine. Notons le contraste avec la XXX · France : trente ans après, au moment où Foucault faisait son cours, il y avait encore des prix administrés. Mais écouter le système des prix suppose que le marché fonctionne bien, quil repose sur un socle approprié : une fixité de la monnaie dune part, et surtout une action délibérée de lÉtat pour imposer le principe de concurrence. Le marché nest pas un ordre transcendant que toute initiative de lÉtat irait perturber ; il y a au contraire lidée quil est fragile, quil y a une pente naturelle vers la formation de monopoles et de col- lusions, quil ne fonctionne pas naturellement sans un cadre très rigide que précisément lÉtat apporte9. Pour préserver la concurrence, il faut dailleurs encourager les entreprises familiales (le Mittelstand dont on sait aujourdhui le succès économique), lartisanat et une agriculture formée de petites exploitations. On nest pas surpris alors du compromis trouvé entre la France et lAllemagne au moment du traité de Rome : un pivot venu de Paris, à savoir la politique agricole commune quacceptaient de plus ou moins bon gré nos ordolibéraux, et, venue de Bonn, une solide autorité de la concurrence, avec un soutien plus mitigé de Paris. Ainsi, lÉtat intervient en amont, sur la structure plutôt que sur ses effets. Il pose la « règle », un mot toujours très fort pour les Allemands. La structure, ce sera davantage lentreprise que le consommateur, ce sera davantage le droit que les dispositifs économiques et fiscaux. Par cohérence, lordolibéralisme avait une vue très restrictive de ce que devait être lÉtat social, bien loin du projet bismarckien. Cela en raison du primat donné aux prix et à lentreprise. Une santé, une éducation et une culture socialisées ou gratuites, cétait intervenir à rebours puisquon niait lapport du système de prix dans lallocation des ressources. La protection sociale était pour eux laffaire des individus. Si lon devait aider, cétait uni- quement par le jeu des revenus, en préservant les mécanismes de marché. Cela na bien sûr pas résisté aux contraintes politiques du moment, sous linfluence notamment du SPD, de la tradition bismarckienne et des milieux catholiques, si lon se rappelle linfluence quavait eue longuement le premier parti social-chrétien dEurope, le Zentrum, né en 1870. Mais, à elle seule, la concurrence ne saurait suffire. Röpke, cité par Foucault, le disait avec force : « Ne demandons pas à la concurrence plus quelle XXX ne peut donner. Elle est un principe dordre et de direction dans le domaine particulier de léconomie de marché et de la division du travail, mais non un principe sur lequel il serait possible dériger la société tout entière. Moralement et sociologiquement, elle est un principe dangereux, plutôt dissolvant quunifiant. Si la concurrence ne doit pas agir comme un explosif social ni dégénérer en même temps, elle présuppose un encadrement dautant plus fort, en dehors de léconomie, un cadre politique et moral dautant plus solide 10. » Foucault use de litote quand il interprète cette phrase comme une « ambiguïté » du libéralisme à lallemande. On voit ici la différence avec lautre versant, disons keynésien ou acti- viste, dans la bifurcation libérale mentionnée plus haut. Le camp activiste voyait le rôle de lÉtat en creux en quelque sorte, dans les défaillances du marché quil fallait compenser, nhésitant pas à se substituer à lui sil le fallait. Lordolibéralisme insistait au contraire pour une action de lÉtat en amont, consistant à créer les conditions par lequel le marché continuerait à jouer pleinement son rôle, pour éviter les interventions en aval. Ce débat persiste pleinement aujourdhui. Le libéralisme des écono- mistes américains, pour y venir, était plus extrême : cest parce quil y a inévitablement des défaillances de lÉtat quil faut y substituer le marché.XXXLÉcole de Chicago Foucault ne traite que cursivement des représentants de lécole améri- caine du libéralisme économique, dont Frank Knight, Henry Simons, George Stigler et, plus tard, Milton Friedman. Ces économistes allaient transposer dans lordre social, en le poussant à lextrême, une autre tradition économique libérale, née dans les années 1870, appelée néo- classique ou marginaliste. Dans un marché bien réglé, selon ce courant, les prix, le taux dintérêt ou les salaires simposent de façon transcendante aux entreprises et aux ménages. Ceux-ci sont immergés dans un monde dont les paramètres leur échappent. Ils reçoivent des informations externes et y répondent, méca- niquement, en ajustant leur comportement. La règle du profit maximum nest quune règle de survie de lentreprise. Lindividu est représenté via un modèle très sommaire, lhomo conomicus, égoïste et optimisateur, dont on ne trouve pas la moindre trace chez les pionniers du libéralisme 1XXX · économique, pas plus que chez les fondateurs de lordolibéralisme ou dailleurs chez Hayek. Le système des prix est comme le système nerveux, celui qui transmet les informations aux agents, qui les motive et les stimule. Chez ces libéraux américains, toute perturbation à son endroit est a priori néfaste. Par exemple, une organisation en syndicats ou un salaire minimum, parce quils sortent du marché « libre », se retournent finalement contre les travailleurs (et les plus modestes, pour faire bonne mesure) en créant du chômage. On voit la différence avec Adam Smith, qui recommande que les travailleurs sunissent dans la négociation sala- riale face à des patrons qui ont toute facilité, vu leur faible nombre, dorganiser la collusion entre eux11 ; de même quavec lordolibéralisme allemand qui, dès 1951, promulguait les premières lois daprès-guerre sur la codétermination et les comités dentreprise12. Les agents étant emmaillotés dans un tissu complexe dincitations exo- gènes, leur autonomie est extraordinairement réduite. Foucault disait de lâge classique que « le nouvel art gouvernemental consomme de la liberté ». Ici, il ny a plus une once de liberté. Lindividu est comme une molécule réa- gissant, selon des lois doptimisation, aux impulsions externes fournies par le marché. Léconomiste devient celui qui dit : « Incitations ! » La gouvernementalité par le marché est radicale et lanalyse de Foucault commence ici à prendre son sens : « L homo conomicus, cest celui qui est éminemment gouvernable. » Les règles de marché deviennent les étalons dune bonne action publique ; elles en donnent les codes. Foucault introduit ici une distinction entre ce quil appelle le sujet de droit et le sujet dintérêt : le sujet dintérêt répond à des incitations venues de lextérieur et renvoie rationnellement sa réponse ; le sujet de droit est mû par des motivations intrinsèques13. Par exemple, un interdit légal, assorti dune sanction ou dune peine, est un interdit pour le sujet de droit, mais un coût pour le sujet dintérêt. Le délinquant chez Gary Becker, un économiste de Chicago, calculera les avantages et les coûts de son acte (dont lamende ou la prison) et prendra sa décision en conséquence (et en cela nest pas délinquant, ou alors nous le sommes tous).XXX 1XXX On retrouvera toutefois au sein de cette école américaine la bifurcation décrite plus haut entre libertariens et activistes. On nabandonnera pas forcément lingénierie sociale chère à léconomiste, mais imbriquée dans lordre du marché : sil y a, par exemple, une discrimination par largent dans laccès à léducation, on distribuera des coupons pour permettre aux gens de tous les milieux de se présenter à lécole privée de leur choix, préservant ainsi la concurrence. Friedman recommandait le revenu uni- versel de base. Un courant intéressant a aussi émergé sous la désignation de market design, à savoir comment structurer un marché afin quil réponde à certains objectifs de politique économique ou sociale, un marché censé donc être asservi à la cause gouvernementale, à faire délibérément partie de sa panoplie dinstruments14. La relation État-marché est à deux voies. Si dailleurs les individus sont soumis aux incitations et quun Léviathan apprend à bien les manier, tout lui devient possible. Un pas sera franchi quelque dix ans après le cours de Foucault : celui de la montée en régime dun nouvel « espace de véridiction », le marché financier. Le voici qui, mieux que le marché des biens et du travail, pourra allouer les ressources au sein de léconomie et répartir le risque, avec des frictions minimales. Cest la valorisation en continu des actifs sur les marchés financiers qui est le juge de paix, y compris dans lallocation de capital aux entreprises, y compris, prétend-on, dans la gestion des entreprises. Sil faut caractériser en quelques mots ce quest le néolibéralisme dans le champ économique, on dira quil est laddition du primat donné aux messages des prix, de la mise en retrait de lÉtat, de la plus grande fluidité donnée aux marchés financiers et de linstrument-marché pour remédier aux défaillances que le marché peut entraîner et que laction de lÉtat entraîne à coup sûr. On est très loin alors du libéralisme classique, très loin aussi de sa variante allemande daprès-guerre, lesquels nous donnent, à nen pas douter, de meilleures pistes pour affronter les défis de léco- nomie globale de demain.XXX 1XXX · Powered by TCPDF (www.tcpdf.org)&quot; #On découpe en paragraphes t&lt;- str_split(t, &quot;XXX&quot;,simplify = TRUE) t2&lt;-as.data.frame(t(t)) Plus les textes sont standardisés et plus facile est le processus dimportation des pdf. Si lon souhaite aller plus loin on recommande par exemple https://ropensci.org/blog/2018/12/14/pdftools-20/ pour extraire un tableau. ( à développer en 4 ou 5 lignes avec des références) 3.4.3 la numérisation et lOCR : La solution tesseract Dimmenses archives sont numérisées, ce qui signifie quon en a prise une image. Linformation est contenu dans les pixels, et lenjeu est de reconnaitre parmis eux des formes caractéristiques : alphabet, ponctuation à travers de multiples variations. Les plus fortes sont celles manuscrites, mais lécriture typographique est aussi très variables dans ses formes. Cest un enjeu industriel anciens. La reconnaissance optique des caractère a cependant fait dimmense progrès et atteint des niveau de performance élevé.( ref) Le traitement des adresses a été le problème qui a stimulé le developpement des technologies de la reconnaissance optique de caractères (OCR) ne serait-ce que pour les problèmes de tri postaux ce petit film en donne une très bonne idée. La qualité du matériau est essentielle, et sassurer que les expéditeurs choisissent un modèle conventionnel et standardisé de rédaction de ladresse est une condition de leur succcès. La situation idéale ressemble à ceci. Modèle de rédaction correcte dune adresse postale Mais la réalité ressemble souvent à celà : à çà Dans un environnement en sciences sociales la situation est moins complexe, les documents analysés ne seront le plus souvent pas des documents manuscrits (sauf pour les médiévistes), mais le scan de documents plus structurés. Par exemple les jpg Une solution pour r est tesseract. Cest un package qui permet daccéder au programme du même nom, développé à lorigine chez Hewlett-Packard Laboratories entre 1985 et 1994, avec quelques modifications supplémentaires apportées en 1996 pour le portage sur Windows, et sur C en 1998.Tesseract a été mis en open Source par HP en 2005. Et de 2006 à novembre 2018, Google a continué a le développer. Il sappuie sur des réseaux neuronaux de type LSTM (voir chpitre deep for langage) . Cest une petite mais puissante intelligence artificielle qui supporte plus dune centaine de langues. Testons-le sans attendre avec le texte suivant : une lettre de motivation empruntée sur le net. Cest un fichier .jpeg de x ko. Lettre de motivation library(tesseract) #library(magick)#pour pré-traiter l&#39;image et améliorer la reconnaissance tesseract_download(&quot;fra&quot;) #pour télécharger le modèle de langage ## [1] &quot;C:\\\\Users\\\\33623\\\\AppData\\\\Local\\\\tesseract4\\\\tesseract4\\\\tessdata/fra.traineddata&quot; t1&lt;-Sys.time() text &lt;- tesseract::ocr(&quot;./images/LettreMotivation.jpg&quot;, engine = &quot;fra&quot;) t2&lt;-Sys.time() t&lt;- t2-t1 cat(text) ## [Nom / Prénom ## Adresse ## Tel ## Mail] ## [Coordonnées de lentreprise] ## À l&#39;attention de {nom de la personne chargée du recrutement] ## Madame/ Monsieur, ## Vous lavez certainement déjà entendu, [insérer une citation en italique et/ou entre guillemets]. Je ## partage cette pensée et cest pourquoi [donnez des indications concernant vos réalisations, vos ## anciennes fonctions ou votre façon de travailler]. ## C&#39;est pour cela que je vous contacte. Vous recherchez un {poste auquel vous candidatez] et mon ## expérience comme ma motivation me semble correspondre à vos besoins. ## Exerçant depuis X années, je suis en mesure de {indiquez vos savoir-faire] et [nom de lentreprise] a ## justement lobjectif de {donnez une corrélation entre vos capacités professionnelles et les besoins de ## lentreprise]. ## Très motivé, je suis disponible immédiatement et j&#39;aurais grand plaisir à vous rencontrer pour ## échanger de vive voix si vous souhaitiez me recevoir. ## Je vous remercie davoir pris connaissance de ma candidature et je vous prie dagréer, ## Madame/Monsieur, mes meilleures salutations. ## ## signature #tesseract_info() #voir les langues disponibles Pour améliorer la performance qui peut se mesurer au niveau des lettres mais doit surtout lêtre au niveau des mots, deux stratégies sont possibles, la première de préprocessing, la seconde de postprocessing avec un mécanisme de détection et de correction derreur. Le preprocessing consiste à traiter limage en renforçant les contrastes ou en éliminant le bruit, on en rend ainsi nles pixels mieux digestes pour tesseract. Cest ce à quoi sattache le pakage magick qui offre un bouquet de fonctions à cette fin. Nous laissons le lecteur le tester par lui-même. Le post-processing consiste à un introduire des mécanismes de correction derreurs au niveau des mots.Pour une idée de ce type de développement voir Gabriel, Yadir, Xiaojie, Mingyu Naturellement, un paramètre important est la vitesse de traitement des images. Dans un projet complet on peut être amener à traiter des centaines images en boucle. Dans notre exemple la durée est de ```` secondes, autrement dit 6 images à la minute ou 360 à lheure Si le chercheur était face à des document manuscrits, il faudra sans doute se pencher sur des services plus industriels et sans doute des trainings beacoups plus spécifiques. Nous avons tester tesseract sur ce manuscrit, et avouons le , il a été totalement désemparé par la graphie de voltaire. Voltaire 3.5 Les contenus vocaux (speech-to-text) La tradition méthodologique de la sociologie est celle de lentretien, avec toute sorte dacteurs. Elle aboutit à la production de transcriptions, plus ou moins détaillées et précises. Mais généralement opérées manuellement, ce qui est coûteux. Le rêve de tout enquêteur est daller du microphone au texte. Il rêve dun système qui reconnaise la voix, même plusieurs, et puisse retranscrire ce qui se dit de manière plus ou moins structurée : identifier des locuteurs, leur attribuer une parole, noter les silences, les interjection, les borborygmes. Dans cette direction les technologies évoluent rapidement dans une direction souhaitable.Google devient un des principaux fournisseur de ces solutions. Elles sont coûteuses. La bonne nouvelle cest quil existe déja des packages sur r qui permettent daccéder aux solutions de google langage : googleLangageR. Ils nécessitent une clé dAPI payante. Nous ne les avons pas testées, fidèle au principle de nexplorer dans ce manuel que des solutions ouvertes et gratuites.Il aurait été cependant regrettable de ne pas signaler cette perspective. Espérons que vite des solutions open soient proposées. En évoquant cette voie, profitons en pour signaler une dimension fondamentale du langage. Le texte qui se dit apporte au texte qui se lit une autre dimension, celle de la prosodie dont les éléments clés sont : Laccent Le ton Lintonation La jointure La pause Le rythme Le tempo et le débit Elle peut être utile pour saisir des éléments paratextuels et plus émotionnels ou pour annoter le texte de ces éléments. .https://hal.archives-ouvertes.fr/hal-02181469/document https://hal.archives-ouvertes.fr/hal-01821214/document https://rdrr.io/github/usagi5886/intonation/man/intonation.html https://ips-lmu.github.io/The-EMU-SDMS-Manual/ 3.6 Echantillonner les textes Un corpus reste un échantillon. Dans ce chapitre nous avons appris comment faire la cueillette dans les sources de textes et constituer matériellement un corpus. Il reste à traiter la question de la représentativité. La collecte doit rester raisonnée. Les unités de texte. Une unité de texte : un chaine de caractères intégrée dans un document. Celui ci peut être un livre un article, une note, une transcription, Un document Un ou des auteurs du document Une date Un endroit Un contexte : les unités précedentes, et subséquentes. Unités de production et de reception, Un texte est produit et puis il est lu, peut-être. Analyser le texte peut se faire dans deux perspectives, celle de la production et celle de la réception. Les corpus doivent être construits en fonction de ce critère. Examiner la question de lengagement dans ce cadre est essentiel, certains acteurs sur un sujet donnée sont amenés à parler plus que les autres et développent un surcroit de voix. la question du biais de selection Un corpus est un ensemble de documents. Ils peuvent être courts, les tweets par exemples, pas trop long - abstract articles court - long ( article de recherche, ou très longs (livres). Le moyen le plus simple danalyser ce problème est de sappuyer sur une typologie de méthode danalyse comparer le discours d acteurs distincts examiner les évolutions du discours au cours du temps Analyser un dialogue Etudier une narration 3.6.1 approche intra-discursive. Un seul auteur, ou groupe dauteurs et une production discursive relative à une période donnée. Deux grands problèmes danalyses : interne : il sagit identifier des thématiques et la manière dont elles sarticulent. externe : il sagit de relier les éléments précédents à son environnement. 3.6.2 approche comparative Lapproche comparative est la plus simple, 3.6.3 approche temporelle Une grande masse de textes se distribue dans le temps : les posts des réseaux sociaux, les dépêches dagence de presse en sont un autre exemple. Ce type de donnée présente lavantage que sa fréquence par période permet des mesures précises. 3.6.4 approche dialogique Le discours dialogique se produit dès lors que plusieurs locuteurs interagissent. Il se noue dans différente situations chat bot : un agent électronique répond à des questions naturelles. théatre : des acteurs échangent des assertions entretien table ronde  3.6.5 approche narrative 3.7 Conclusion Dans ce chapitre nous aurons égratigné des sujets techniques de constitution de corpus en envisageant différents moyens dacccès Scrapping API Pdf texte dans les images une ouverture à loral On soulignera la technicité On observera létendue des domaines à exploiter. "],["visualiser-et-réduire-le-corpus.html", "Chapitre 4 Visualiser et réduire le corpus 4.1 Explorer le corpus 4.2 Keyness index 4.3 Textrank", " Chapitre 4 Visualiser et réduire le corpus Maintenant que nous disposons de lensemble du corpus, il est intéressant, de sen donner un aperçu général. On examine ici quelques outils : corpora explorer les fonctions keyness et kwicks text réduction : avec textrank 4.1 Explorer le corpus (attention, cest un chapitre qui doit être par la suite détaché) Avant de procéder aux analyses du corpus, il est souvent utile de le représenter. On va utiliser le package Corpora explore à cette fin. Il permet de préparer un corpus et de le visualiser de manière interactive avec la génération dune app shiny. Malheureusement nous ne savons pas rendre compte de la dynamique de loutil. On peut naviguer aisément dans lensemble de texte. On va utiliser une collection de données préparée avec Manel Benzarafa de lUniversité Paris Nanterre, et qui comprend lintégralité des résumés, auteurs etc.. relatifs aux articles publiés par la revue [Politiques et Management] public () PMP. Une base bibliographique intégrale composée de 1025 articles. #install.packages(&quot;corporaexplorer&quot;) library(corporaexplorer) PMP &lt;- read_csv(&quot;data/PMPLast.csv&quot;) PMP&lt;-PMP %&gt;% select(Key, Author, Title, Issue, 3, 11) PMP&lt;-PMP%&gt;% rename(Text=6, Annee=5) %&gt;% filter(Text!=&quot;Null&quot; &amp; !is.na(Annee)) corpus &lt;- prepare_data(PMP, date_based_corpus =FALSE, grouping_variable = &quot;Annee&quot;, # change grouping variable within_group_identifier = &quot;Title&quot;, columns_doc_info = colnames(df)[1:4], tile_length_range = c(2, 10), use_matrix = FALSE ) #explore(corpus) #on n&#39;execute pas car diagramme interactif. Dans la photo décran suivante, on teste les termes \" politique\" et management. Chaque tuile ( tile) représente un des 1025 abstracts qui composent le corpus. Les couleurs correspondent à la fréquence des deux termes. Exploration des abstracts de PMP Un autre exemple avec David Bowie ( Nous aurons loccasion dexploiter ces corpus dans les chapitres suivants, alors donnons nous un second panorama de corpus). Cest un corpus constitué par Elisa Benavent pour un mémoire dhistoire contemporaine dans lidée simple détudier comment la novation se définit dans le temps. library(tidyverse) library(shiny) df &lt;- read_csv(&quot;data/bowie_txt_analysis.csv&quot;) df&lt;-df %&gt;% mutate(Annee=substr(date,7,8))%&gt;% rename(Text=texte, Data=date)%&gt;% filter(!is.na(Text)) library(corporaexplorer) corpus &lt;- prepare_data(df, date_based_corpus =FALSE, grouping_variable = &quot;Annee&quot;, # change grouping variable within_group_identifier = &quot;titre&quot;, columns_doc_info = colnames(df)[2:3], tile_length_range = c(1,4), use_matrix = TRUE ) explore(corpus) #la fonction lance une app interactive Shiny applications not supported in static R Markdown documents 4.2 Keyness index Cest un des outils les plus basiques et fondamentaux. Il est destiné à identifier rapidement les mots clés qui distinguent une partie du corpus des autres parties. Quanteda fournit une solution https://quanteda.io/reference/textstat_keyness.html avec quanteda statistics. https://www.researchgate.net/publication/319208347_Keyness_analysis_Nature_metrics_and_techniques 4.2.1 quick index 4.2.2 keyness index df$Annee&lt;-as.numeric(df$Annee) df&lt;-df %&gt;% mutate(group=ifelse(Annee &gt;90,&quot;cible&quot;,&quot;index&quot; )) corpus &lt;- corpus(df, text_field = &quot;Text&quot;) toks &lt;- tokens(corpus,remove_punct = TRUE, remove_numbers = TRUE)%&gt;% tokens_remove(pattern = stopwords(&quot;en&quot;)) dfm&lt;-dfm(toks) dfm1 &lt;- dfm_group(dfm, groups = group) tstat &lt;- textstat_keyness(dfm1) foo &lt;- tstat %&gt;% filter(n_target+n_reference&gt;50) %&gt;% as.data.frame() %&gt;% select(-p) ft&lt;-flextable(foo) %&gt;% theme_vanilla()%&gt;% add_footer_lines(&quot;La période cible se caractèrise &quot;) ft &lt;- color(ft, part = &quot;footer&quot;, color = &quot;#666666&quot;) ft &lt;- set_caption(ft, caption = &quot;Keyness index ( chi²&gt;10&quot;) ft .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-2ee12254{border-collapse:collapse;}.cl-2ed1ea8c{font-family:'Arial';font-size:11pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-2ed1ea8d{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-2ed1ea8e{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(102, 102, 102, 1.00);background-color:transparent;}.cl-2ed2117e{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-2ed2117f{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-2ed2ad64{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2ed2ad65{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2ed2ad66{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2ed2ad67{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2ed2ad68{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2ed2ad69{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2ed2ad6a{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2ed2ad6b{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2ed2ad6c{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 4.1: Keyness index ( chi²&gt;10 featurechi2n_targetn_referencemr57.820387957420266202s13.92132302259821649bowie's12.744513597661046223bowie11.960603998335896564band7.58112632089381243also5.62294310452171566dance4.75679255904271146little1.97150650706651053made1.82342089320361054music1.666845247028617104new1.457531156182922143sound0.8747657018380848musical0.4997117169371745record0.4997117169371745years0.49847400505331175work0.3718324723558962songs0.36805694854401393says0.0943892900732861last0.0491325249929863albums0.0416623182026647album0.028806757407224198like0.017763720832725208good0.0153935078860757heroes0.0153935078860757way0.0128324517486974back0.0014211081363976much0.0000003267105977world-0.0013051028845978never-0.00383041243201196first-0.0052427436191979get-0.0137660626542654things-0.0256548201674655pop-0.0298275303336873next-0.0600220167011548make-0.0836788160042549guitar-0.1329201647092660thing-0.2105232261430553two-0.2722477603567883man-0.315076731767810103just-0.340261419958214141aladdin-0.4267925408287448since-0.5390624224063450life-0.7235219399253453can-0.8581462737709786song-1.123176120497111131love-1.1401772678038459well-1.1401772678038459something-1.1439836771260348stardust-1.2247681750283349always-1.3680495166617462ziggy-1.3706380498399573going-1.3903817534440351title-1.5609089530350353even-1.9764494485149692time-2.36939472732799130still-2.8435871950116254know-2.9473153354248255rock-3.139352768942515205right-3.1561515618603257people-3.8994037554368264got-4.2228060475412267star-4.4397608877746269really-4.8499101234740158one-5.334530092895812201think-5.6511652073828165david-5.94578738681909173now-6.61183713249033103roll-6.6870859109610174La période cible se caractèrise ggplot(foo, aes(x=n_target, y=n_reference))+geom_point(aes(size=chi2))+geom_text(aes(label=feature))+ #scale_y_log10()+ #scale_x_log10()+ geom_abline(intercept = 0, slope=1) 4.3 Textrank Quand le corpus de texte est important et que la taille des textes lest aussi. Par exemple des compte-rendus dentretiens de trente pages, ou des rapports dactivités dentreprises, on peut souhaiter les réduire à des documents dune page dans laquelles les arguments principaux sont résumés. Dans le projet daller à la découverte du contenu du corpus, un tel outil est un rêve. Une solution est proposée depuis XXX avec lalgorithme Textrank "],["préparation-des-données.html", "Chapitre 5 Préparation des données 5.1 Manipuler des chaines de caractères 5.2 Nettoyer le texte 5.3 Corriger le texte 5.4 Identifier les sources", " Chapitre 5 Préparation des données Avant de se lancer dans lanalyse, il est nécessaire de préparer le texte, de le pré-traiter. Son format fondamental est celui dune chaine de caractères, sans signification particulière mais composé à partir dun alphbat, cun jeux de signes déterminés et démobralement. 1/0 pour lelangage bianre, AGCP pour ladan, 26 caractère de base pour lalphabet, sans compter les accents. Ces variations sont lobjet de convention en informatique. et de certaines opérations. traiter du texte cest avant tout disposer dopérateurs pour manipuler ces éléments élémentaire. la base est davoir des outils pour les manipuler. Le langage avant dêtre signifiant est signifié, littéralement produit comme une chaine de signes qui dans lusage suit certaine convention. Par exemple la satisfaction peut sexprimé par mmmm, une forte satisfaction par un mmmmmmmmmmmmmmmmmmmmm. Pour distinguer les significations, il faut dabord compter. les mmm sont sans doute courants car conventionnels (ce mot est à deux doigts dêtre incorporé au dictionnaire de lAcadémie Française, sil nétait quune onomatopée), les mmmmmmmmmmmmmmmmmmmmm sont sans doute beaucoups plus rares. De plus on trouvera des hum des hummm, des mmmmhummmm\". On comprend quà la nuance de lintensité que le locuteur veut exprimer, toute ces morphologies se rapportent à une même idée. Comment le rammener à une même formes est une question essentielle même si elle semble excessivement technique. 5.1 Manipuler des chaines de caractères Il faut donc traiter le texte, avant même de sengager dans des modèles compliqués. Il faut savoir traiter des chaines de caractères pour en réduire la diversité, et en produire des chaines grammaticalement exacte. Cest un travail dartisan, celui des des imprimeurs et de leurs coorecteur. Et en particulier dun métier celui du compositeur, ou ouvrier de la casse, qui distribue des caractères de plomb en séquences dans des casiers de bois. cmpositeur Lartisan navait pas de choix, la précision était essentielle pour éviter la coquille. Le texte moderne, numérique, est lobjet de plus daller et retours. Les mots quon pianotent sont corrigés avant même dêtre frappés. Les gestes techniques sont différents mais sarticulent sur une même idée : la langue écrite, du moins les langues alphabétiques sont des chaines de caractères dont la formation suit des règles fluctuantes à travers lhistoire mais contraignante à chaque moments. Les conventiosn peuvent changer, mais dans son temps elle simposent définitivement. Personne nécrirait deffert, pour dire dessert. Et pourtant la graphie du s était un f jusquau XVI ème siècle (trouver la source)! https://www.cairn.info/revue-la-linguistique-2003-1-page-3.htm De nombreuses ressources sont disponibles pour traiter ces chaines de caractères. On utilisera surtout Stringer qui est est un des composants essentiels de tidyverse. Dautres packages sont équivalents : stringi par exemple. 5.1.1 Les opérations sur les chaînes de caractères mettre en minuscule. Lalphabet se présente au moins en deux versions : des majuscules et des minuscules, il est souvent nécessaire de réduire le texte à une seule casse pour en réduire la variété, sauf si les majuscules signalent une information spécifique et socialement conventionnelle. Un mot qui débute par une majuscule signale un nom commun, désormais conceptualisé comme une entité nommée appartenant à différentes catégories : noms de lieux, noms de personnes, noms dorganisation ou lexpression dun sentiment, au sein des chats, la majuscule en série signale un niveau de langage loud, un cri , une engueulade, la véhémence. rechercher une chaine de caractères; remplacer une chaine de caractères extraire une chaine de caractère dun emplacement à lautre supprimer une chaine de caractères. Les nombres, concaténer des chaines de caractères. Le texte peut être divisés en unités. Un paragraphe par exemple, ou un titre. Si la manipulation deslaquelle ? vaccin ? 5.1.2 La technique des expressions régulières (regex) Il ne suffit pas de chercher une chaine de caractère particulière, il faut souvent saisir un ensemble de variations qui suivent un motif determiné et qui répond à une sorte de loi générale. Par exemple si je veux retrouvrer dans un corpus lensemble des mots relatif au monde de lhôpital, nous chercherions aussi le mot hopital. Nombreux seront les locuteur qui omeetent laccent circonflexe. Une formule pour trouver ces deux varietés serait dutiliser un opérateur, \"(), pour définir une option . soit lun soit lautre : h(ô,o)pital Une expression régulière est un masque qui permet didentifier des formes principales et leurs variétés. Il sappuit sur une codification dont quelques éléments clés permettent de se donner une bonne idée de la logique générale le ^, indique que la forme commence par le caractère qui suit ^A le . signifie nimporte quel caractère. le regex ^a. signifiera ainsi nimporte quelle chaine de caractère qui commence par a est est suivi de nimporte quel caractère. le * la répétition indéfinie du caractère . Dun point de vue linguistique les regex travaillent sur la morphologie et ses variations, indépendemment des règles de grammaires mais profitant de leur régularité. Les mots sont généralement composés dune racine, de suffixe et de préfixe qui contiennent les flexions grammaticales et sémantiques. des exemples : la négation : visible et in-visible. la conjugaison : aime et aim-ât la numération : fraise et fraise-s le genre : épicier et épicière-s. 5.1.3 Un fondement profond et ancien Le langage des regex a répondu dabord aux besoin des informaticiens, et sappuie sur une construction mathématique sophistiquée : les automatates finis https://swtch.com/~rsc/regexp/regexp1.html don t un des contributeurs essentiels à été doi.org/10.1145/363347.363387 Ken Thompson fondateur de Grepl a method for locating specific character strings embedded in character text is described and an implementation of this method in the form of a compiler is discussed. The compiler accepts a regular expression as source language and produces an IBM 7094 program as object language. The object program then accepts the text to be searched as input and produces a signal every time an embedded string in the text matches the given regular expression. Examples, problems, and solutions are also presented. https://swtch.com/~rsc/regexp/regexp1.html 5.1.4 Des applications très pratiques et à ceux qui face à des questions de métier, par exemple les professionnel de marketing direct ou des services postaux, ont été amené à traiter de jeux de données textuels limités tel quune adresse postale. dectecter une entité nommée : la majuscule détecter une adresse détecter une date détecter un compte détecter une url 5.2 Nettoyer le texte enlever les mentions enlever les url enlever ou recoder les emojis enlever la ponctuation enlever les nombres 5.3 Corriger le texte Si certains corpus sont par les conditions de leur production presque parfait du point de vue grammatical et lexical, cest le cas en principe des articles de presse et des documents officiels, dautres qui sappuient sur une langue vernaculaire on des graphies plus incertaines et des syntaxes approximatives. Dans un tiers des cas le mot \" opinion\" sorthographie opignons. Chaque mot du lexique sévanouit dans des morphologies nombreuses et approximatives. Cest un obstacle à lanalyse car la variété morphologique est aléatoire. plusieurs stratégies sont possibles. La première est de corriger le texte notamment en employant des outils de corrections efficaces. 5.3.1 La correction orthographique automatique voir hunspell https://cran.r-project.org/web/packages/hunspell/vignettes/intro.html#Custom_Dictionaries 5.3.2 Analyse ciblée par les regex Une application des regex est lanalyse ciblée dun certain nombre de termes. LA corection est partielle mais couvre les cibles essentielles exemple des gestes barières dans le flux twitter 5.4 Identifier les sources Dans lanalyse des contenus sociaux, les textes viennent de sources multiples et confuses. Elles peuvent être aisément multilingue. Analyser un corpus dentretien, une collection de discours, posent peu la questions des locuteurs car ils sont bien identifiés. Ce nest pas le cas dans les réseaux sociaux où les buts sont multiples et plus ou moins avoués. Les acteurs : des professionnels de la politiques et les institutions quils dirigent Journaliste et professionnels de la communication Experts et universitaires les marques et leur community manager les bots les trolls les activistes 5.4.1 Identifier la langue Dans les points précedents on suppose que la langue est homogène, mais les corpus peuvent être multi-langues. Par exemple, dans les corpus davis dhôtes sur Airbnb, les avis sont formulés dans une large variété de langues. Il va falloir en tenir compte et une tâche préliminaire sera de détecter les langues pour séparer les corpus. Le package textcat offre une solution basée sur la fréquence des ngrams (voir chapitre x). et compare la distribution du texte ciblée avec les distributions typiques des langues. Google propose un algo cpd3(https://github.com/google/cld3) plus sophistiqués dans la mesure où cest un réseau de neurones qui fait le travail. Comparons les. On utilise un jeu de donnée Airbnb sur Bruxelles qui accueillant les institutions européennnes est une des villes les plus cosmopolite qui soit avec des fonctionnaires venant de toutes leurope et sexprimant dans une large variété de langue, sans compter les représentations des autres pays du monde. En terme de durée de calcul, la différence en temps de calcul est faramineuse 7 secondes contre 7 minutes, ce qui sexplique car texcat sappuyant sur la distribution des ngrams doit les calculer pour les 36000 observations que nous avons retenues. Faisons un test sur un extrait du corpus Airbnb. BXL2021 &lt;- read_csv(&quot;./data/reviewsBXL2021.csv&quot;) BXL2021$Year&lt;- as.numeric(format(as.Date(BXL2021$date, format=&quot;%Y-%m-%d&quot;),&quot;%Y&quot;)) BXL2021&lt;- BXL2021 %&gt;% filter(Year&gt;2019) # on filtre sur la période de confinement library(cld3) t1&lt;-Sys.time() cld3&lt;-as.data.frame(detect_language(BXL2021$comments))%&gt;%rename(cld3=1) t2&lt;-Sys.time() t_cld3&lt;-t2-t1 #on calcule la durée de l&#39;opération en faisant la différence du temps de départ et d&#39;arrivée library(textcat) t1&lt;-Sys.time() textcat&lt;-textcat(BXL2021$comments) t2&lt;-Sys.time() t_texcat&lt;-t2-t1 foo&lt;-cbind(cld3, textcat) Examinons les résultats et la distribution des langues identifiées par les deux systèmes. Si lordre est respecté, des différences sobservent, cld3 identifie du chinois qui ne fait pas partie du répertoire de texcat. g1&lt;-foo%&gt;%mutate(n=1)%&gt;%group_by(textcat)%&gt;%summarise(n=sum(n))%&gt;% ggplot(aes(x=reorder(textcat,n), y=n))+geom_bar(stat=&quot;identity&quot;)+coord_flip()+scale_y_log10() g2&lt;-foo%&gt;%mutate(n=1)%&gt;% group_by(cld3)%&gt;% summarise(n=sum(n))%&gt;% ggplot(aes(x=reorder(cld3,n), y=n))+ geom_bar(stat=&quot;identity&quot;)+coord_flip()+scale_y_log10() plot_grid(g1, g2, labels = c(&#39;Texcat&#39;,&#39;Cld3&#39;), label_size = 12) Examions maintenant la convergence des méthodes en représentant la répartition du résultat dun système dans les langue de lautre. Si la convergence est parfaite 1000% des textes classé en Français par Textact devrait se retrouver dans 100% de ces textes classé par cld3 et réciproquement. foo1 &lt;-foo %&gt;% mutate(n=1)%&gt;%group_by(textcat) %&gt;%summarise(n=sum(n)) foo1&lt;-foo%&gt;% left_join(foo1) %&gt;% filter(n&gt;10) table&lt;-table(foo1$cld3,foo1$textcat) foo1&lt;-as.data.frame(prop.table(table,2)) ggplot(foo1, aes(reorder(Var2, Freq),Var1)) + geom_tile(aes(fill = Freq)) + scale_fill_gradient(low = &quot;White&quot;,high = &quot;Blue&quot;)+ theme_bw()+ theme(axis.text.x=element_text(angle = 45, hjust =1))+coord_flip() foo1 &lt;-foo %&gt;% mutate(n=1)%&gt;%group_by(cld3) %&gt;%summarise(n=sum(n)) foo1&lt;-foo%&gt;% left_join(foo1)%&gt;%filter(n&gt;10) table&lt;-table(foo1$cld3,foo1$textcat) foo1&lt;-as.data.frame(prop.table(table,1)) ggplot(foo1, aes(reorder(Var1, Freq),Var2)) + geom_tile(aes(fill = Freq, label=Freq)) + scale_fill_gradient(low = &quot;White&quot;,high = &quot;Red&quot;)+ theme_bw()+ theme(axis.text.x=element_text(angle = 45, hjust =1))+coord_flip() library(pheatmap) library(viridis) table2&lt;-as.data.frame(table) %&gt;% mutate(Freq=log10(Freq+1)) %&gt;% pivot_wider(names_from = Var1, values_from = Freq) %&gt;% column_to_rownames( var = &quot;Var2&quot;) pheatmap(table2 , color = inferno(10)) chi2&lt;- chisq.test(table) chi2&lt;- as.data.frame(chi2$residual) table2&lt;-chi2 %&gt;% mutate(Freq=Freq^2)%&gt;% pivot_wider(names_from = Var1, values_from = Freq) %&gt;% column_to_rownames( var = &quot;Var2&quot;) pheatmap(table2 , color = inferno(20, direction=1)) 5.4.2 Identifier les plagiats et réutilisations Dans la définition du corpus il peut être utile de se concentrer sur les contenus originaux Une autre question est de detection les contenus riginaux des contenu réutilisés ou carrément plagiés. https://github.com/ropensci/textreuse https://journal.r-project.org/archive/2020/RJ-2020-017/index.html 5.4.3 Identifier les fakes https://github.com/sherylWM/Fake-News-Detection-using-Twitter 5.4.4 Identifier les trolls http://golovchenko.github.io/tutorials/snatrolls 5.4.5 Identifier les bots botometer botchecks pour un benchmark https://rpubs.com/xil865/528096 detecter les fakes https://blogs.rstudio.com/ai/posts/2020-08-18-deepfake/ "],["une-première-analyse-quantitative.html", "Chapitre 6 Une première analyse quantitative 6.1 Comptons les mots 6.2 la production dans le temps 6.3 Lisibilité et complexité lexicale 6.4 Conclusion", " Chapitre 6 Une première analyse quantitative Avant tout un texte doit être analyser de manière volumétrique. Comment de texte? Quelle longueur ? combien de mots ? quelles variations? Dans ce chapitre nous allons analyser le flux des tweets produit par donald Trump, jusquau moment de son banissement en Janvier 2021, àprès sa défaite. Chargeons le fichier de données. On en profite pour compter le nombre de posts et de variables df &lt;- read_csv(&quot;./data/TrumpTwitterArchive01-08-2021.csv&quot;) nrow&lt;-nrow(df) #nombre de ligne ncol&lt;-ncol(df) #nombre de colonne 6.1 Comptons les mots Il y 56571 tweets et 9 variables. On peut vouloir compter le nombre de mots. A cette fin on emploie une fonction de stringr :str_count`. ( On reviendra sur la question de la manipulation des chaines de caractères dans un chapitre ad hoc) df$nb_mots&lt;-str_count(df$text, &quot; &quot;)+1 # l&#39;astuce : compter les espaces et ajouter 1, pour compter les mots sum_mots&lt;-sum(df$nb_mots) #ON COMPTE LE NOMBRE DE MOTS ggplot(df, aes(x=nb_mots))+ geom_histogram(fill=&quot;deepskyblue3&quot;)+ labs(title=paste0(&quot;Nombre total de mots du corpus : &quot;,sum_mots), x=&quot;Nombre de mots par post&quot;, y=&quot;Fréquence&quot;) Figure 6.1: Distribution du nombre de mots par post La bimodalité provient surement du changement de taille maximum effectué en septembre 2017, le passage de 180 caractères max à 280. On peut le vérifier en examinant cette même distribution - par les courbes de densité - pour chacune des années, avec cette technique rendue fameuse par la pochette de lalbum de Joy Division : un graphique en crêtes (ridges plot) avec ggridges df$Year&lt;-format(df$date, format = &quot;%Y&quot;) #on extrait l&#39;année de la date ggplot(df,aes(x = nb_mots, y = Year, group = Year)) + geom_density_ridges(scale = 3, fill=&quot;peachpuff&quot;)+ theme_ridges() + scale_x_continuous(limits = c(1, 70), expand = c(0, 0)) + coord_cartesian(clip = &quot;off&quot;)+labs(x=&quot;Nombre de mots par post&quot;, y=NULL) Figure 6.2: Evolution de la distribution du nombre de mots 6.2 la production dans le temps Le résultat remarquable est que si Trump dans un premier temps exploite cette nouvel fonctionnalité, il en revient avec un phrasé de 20 mots en moyenne, gardant cependant à loccasion dautre contenu en 50 mots environ. Concluons en examiner le nombre de tweets produit au cours du temps ## plot time series of tweets ts_plot(df, &quot;1 month&quot;, color=&quot;darkblue&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + labs( x = NULL, y = &quot;Nombre de tweets par mois&quot;,title = &quot;Fréquence des posts twitters Donald Trump&quot;)+ scale_x_datetime(date_breaks = &quot;1 year&quot;, labels = scales::label_date_short()) Figure 6.3: Evolution de la production mensuelle des tweets de Trump #raf : labeliser avec les dates clés 6.3 Lisibilité et complexité lexicale Pour aller un peu plus loin, dans les comptages, introduisons deux quantifications utiles du texte : la lisibilité et la complexité lexicale. On gardera le principe dune analyse longitudinale https://fr.wikipedia.org/wiki/Covfefe 6.3.1 Les indices de lisibilité La lisibilité est une vieille notion autant que sa mesure (par exemple Coleman and Liau (1975)). Il sagit dévaluer la complexité dun texte à partir de deux critères : la complexité des mots capturée par le nombre moyen de syllabes par mot, et la complexité des phrases mesurée par le nombre de mots. Le nombre dindicateurs est considérable et le package compagnon de quanteda , [quanteda.textstats](https://quanteda.io/reference/textstat_readability.html) , en fournit au moins des dizaines. Dans lexemple suivant, on se contente dun grand classique, le plus ancien, lindice de Flesch (Flesch 1948) et de ses constituants: le nombre moyen de syllabes par mot et le nombre moyen de mots par phrase. foo&lt;-df %&gt;% filter(isRetweet==FALSE) # on ne prend pas en compte les RT readability&lt;-textstat_readability(foo$text, measure = c(&quot;Flesch&quot;,&quot;meanSentenceLength&quot;, &quot;meanWordSyllables&quot;), min_sentence_length = 3,max_sentence_length = 1000) #la fonction de calcul de lisibilité foo&lt;-cbind(foo,readability[,2:4]) foo1&lt;-foo %&gt;% group_by(Year) %&gt;% summarise(Flesch=mean(Flesch, na.rm=TRUE), SentenceLength= mean(meanSentenceLength, na.rm=TRUE), WordSyllables= mean(meanWordSyllables, na.rm=TRUE)) %&gt;% gather(variable, value, -Year) ggplot(foo1,aes(x=Year, y=value, group=variable))+ geom_line(size=1.2, aes(color=variable), stat=&quot;identity&quot;)+ facet_wrap(vars(variable), scale=&quot;free&quot;, ncol=1)+ labs(title = &quot;Evolution de la lisibilité des tweets de Trump&quot;, x=NULL, y=NULL) Figure 6.4: Evolution de la lisibilité moyenne des tweets de Trump Pour aider le lecteur à donner un sens, voici labaque proposée par Fleschlui-même. Flesch. On peut aussi prendre pour références les éléments suivants: All Plain English examples in this book score at least 60. Here are the scores of some reading materials Ive tested. These are average scores of random samples. Comics 92 Consumer ads in magazines 82 Readers Digest 65 Time 52 Wall Street Journal 43 Harvard Business Review 43 Harvard Law Review 32 Auto insurance policy 10 Trump ne parait pas être sa caricature, non niveau de lisibilité correspond à la Licence. Il est moins simple que le Readers Digest mais plus compliqué à lire que la Harvard Business Review ! 6.3.2 Les indices de complexité lexicale La complexité lexicale rend compte de la diversité du vocabulaire, elle consiste à rapporter le nombre de mot uniques sur le nombre total de mots. La difficulté est que la taille des corpus joue fortement sur cette mesure et que lorsque cette taille est hétérogène, lindicateur marque plus cette variété que les variations de complexité lexicale.(Tweedie and Baayen 1998) https://quanteda.io/reference/textstat_lexdiv.html Une manière plus fine sera de considérer chaque période comme un texte, un pb est que lallongement des tweets peut expliquer laccroissement de diversité Dans notre univers trumpesque, ce nest pas trop sensible, dautant plus que nous allons moyenner les tweets par période.Notons au passage que si nous moyennons la diversité lexicale de chaque tweets , une autre approche pourrait être de concatener lensemble des tweets dune période ( un jour, une semaine) pour approcher cette variable à une autre échelle, qui couvrent lensemble des sujets dintérêt de trump, que les tweets fractionnenet nécessairement. Ce qui en en question dans la mise en pratique nest pas seulement la question du choix de lindice mais aussi la définition de lunité de calcul. La diversité lexical concerne sans doute plus le discours que la phrase. là, encore la nécessité davoir des points de repère, des échelles. lexdiv&lt;-tokens(foo$text) %&gt;% textstat_lexdiv(foo$text, measure = c(&quot;CTTR&quot;),remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_hyphens = FALSE) #la fonction de calcul de diversité foo&lt;-cbind(foo,lexdiv[,2]) foo1&lt;-foo %&gt;% group_by(Year) %&gt;% summarise(CTTR=mean(CTTR, na.rm=TRUE)) %&gt;% gather(variable, value, -Year) ggplot(foo1,aes(x=Year, y=value, group=variable))+ geom_line(size=1.2, aes(color=variable), stat=&quot;identity&quot;)+ facet_wrap(vars(variable), scale=&quot;free&quot;, ncol=1)+ labs(title = &quot;Evolution de la diversité lexicale des tweets de Trump&quot;, x=NULL, y=NULL) Figure 6.5: Evolution de la lisibilité moyenne des tweets de Trump 6.4 Conclusion Nous aurons appris à compter le nombre de document mesurer la complexité du langage mesurer la diversité de son vocabulaire. Ces mesures nont ne sens que si elles peuvent être comparées de manière interne de manière externe References "],["tokenisation.html", "Chapitre 7 Tokenisation 7.1 Objectifs du chapitre 7.2 Introduction 7.3 Tokeniser un corpus 7.4 Ngrams 7.5 Choisir des n grams pertinents 7.6 Propriétés statistiques des ngrams", " Chapitre 7 Tokenisation 7.1 Objectifs du chapitre Découper un texte en tokens Visualiser les ngrams du texte Identifier les ngrams pertinents et les transformer en tokens 7.1.1 Les outils Jeu de données : une citation de Max Weber et un extrait du corpus Airbnb Packages utilisés : tokenizer ; quanteda ; stopwords 7.2 Introduction Létape intiale de toute analyse textuelle est de découper le texte en unités danalyse, les tokens, ce qui transforme le texte écrit pour la compréhension humaine en données interprétables par lordinateur. Les tokens utilisés peuvent varier selon les objectifs de lanalyse et la nature du corpus, le texte peut ainsi être découpé en : lettres syllabes mots phrases paragraphes sections chapitres livres Les tokenizers sont les outils indispensables à cette tâche. Dans cet ouvrage, nous nous concentrons sur létude des mots. Dans cette étude, un certain nombre de mots apparaissent de nombreuses fois, pour permettre de donner du sens au langage humain, mais ils ne portent pas en eux dinformations particulièrement pertinentes pour lanalyse : ce sont les stopwords, quil conviendra souvent déliminer. Les ngrams, quant à eux, représentent des suites de n tokens. Un unigramme est donc équivalent à un token, un bigramme est une suite de deux tokens, etc. Lidentification des ngrams permet de détecter des suites de tokens qui reviennent plus souvent que leur probabilité doccurrences. Si lon se concentre sur les mots, nous sommes alors face à une unité sémantique, comme on le comprend facilement avec le bigramme Assemblée Nationale. 7.3 Tokeniser un corpus 7.3.1 Les lettres Commençons par un exemple simple, à laide dune courte citation de Max Weber. On choisit les lettres pour unité de découpe, et lon utilise le package tokenizer. Automatiquement, tokenizer met le texte en minuscule et élimine la ponctuation #Les données MaxWeber &lt;- paste0(&quot;Bureaucratie: le moyen le plus rationnel que lon connaisse pour exercer un contrôle impératif sur des êtres humains. la bureaucratie est une forme d&#39;organisation générale caractérisée par la prépondérance des règles et de procédures qui sont appliquées de façon impersonnelle par des agents spécialisés. Ces agents appliquent les règles sans discuter des objectifs ou des raisons qui les fondent. Ils doivent faire preuve de neutralité et oublier leurs propres intérêts personnels au profit de lintérêt général&quot;) #On tokenise, plus on transforme en dataframe le résultat. toc_maxweber&lt;-tokenize_characters(MaxWeber)%&gt;% as.data.frame()%&gt;% rename(tokens=1) #On compte pour chaque token sa fréquence d&#39;apparition foo&lt;-toc_maxweber %&gt;%mutate(n=1) %&gt;% group_by(tokens)%&gt;% summarise(n=sum(n))%&gt;%filter(n&gt;0) #On représente par un diagramme en barre cette distribution des occuences d&#39;apparition, en classant les tokens par fréquence ggplot(foo, aes(x=reorder(tokens,n), y=n))+ geom_bar(stat=&quot;identity&quot;, fill=&quot;royalblue&quot;)+ annotate(&quot;text&quot;, x=10,y=10, label=paste(&quot;nombre de tokens =&quot;, nrow(toc_maxweber)))+ coord_flip()+labs(title = &quot;Fréquence des tokens, unité = lettres&quot;, x=&quot;tokens&quot;, y=&quot;nombre d&#39;occurences&quot;, caption =&quot; &#39;Bureaucratie: le moyen le plus rationnel que lon connaisse pour exercer un contrôle impératif sur des êtres humains.&#39; &quot;) Figure 7.1: Distribution du nombre de lettres 7.3.2 Les mots On refait la même opération, mais avec un texte complété. Il y a bien moins de mots que de lettres ! #Les données MaxWeber &lt;- paste0(&quot;Bureaucratie: le moyen le plus rationnel que lon connaisse pour exercer un contrôle impératif sur des êtres humains. La bureaucratie est une forme d&#39;organisation générale caractérisée par la prépondérance des règles et de procédures qui sont appliquées de façon impersonnelle par des agents spécialisés. Ces agents appliquent les règles sans discuter des objectifs ou des raisons qui les fondent. Ils doivent faire preuve de neutralité et oublier leurs propres intérêts personnels au profit de lintérêt général.&quot;) #On tokenise, plus on transforme en dataframe le résultat. toc_maxweber&lt;-tokenize_words(MaxWeber)%&gt;% as.data.frame()%&gt;% rename(tokens=1) #On compte pour chaque token sa fréquence d&#39;apparition foo&lt;-toc_maxweber %&gt;%mutate(n=1) %&gt;% group_by(tokens)%&gt;% summarise(n=sum(n))%&gt;%filter(n&gt;4) #On représente par un diagramme en barre cette distribution des occurrences, en classant les tokens par fréquence ggplot(foo, aes(x=reorder(tokens,n), y=n))+ geom_bar(stat=&quot;identity&quot;, fill=&quot;royalblue&quot;)+ annotate(&quot;text&quot;, x=10,y=4, label=paste(&quot;nombre de tokens =&quot;, nrow(toc_maxweber)))+ coord_flip()+labs(title = &quot;Fréquence des tokens, unité = mots&quot;, x=&quot;tokens&quot;, y=&quot;nombre d&#39;occurences&quot;, caption =&quot; &#39;Bureaucratie: le moyen le plus rationnel que lon connaisse pour exercer un contrôle impératif sur des êtres humains.&#39; &quot;) Figure 7.2: Distribution du nombre de mots On constate que les deux mots les plus fréquents de cette citation sont un article indéfini et une préposition. Ces mots sont souvent superflus pour les analyses menées, il convient alors de les supprimer. Cest ce quon fait par la suite, en utilisant le package stopwords qui comprend des listes de stopwords dans différentes langues. 7.3.3 Les mots #On tokenise et on enlève les stopwords, puis on transforme en dataframe le résultat. toc_maxweber&lt;-tokenize_words(MaxWeber, stopwords = stopwords(&quot;fr&quot;))%&gt;% as.data.frame()%&gt;% rename(tokens=1) #On compte pour chaque token sa fréquence d&#39;apparition foo&lt;-toc_maxweber %&gt;%mutate(n=1) %&gt;% group_by(tokens)%&gt;% summarise(n=sum(n))%&gt;%filter(n&gt;0) #On représente par un diagramme en barre cette distribution des occurrences, en classant les tokens par fréquence ggplot(foo, aes(x=reorder(tokens,n), y=n))+ geom_bar(stat=&quot;identity&quot;, fill=&quot;royalblue&quot;)+ annotate(&quot;text&quot;, x=10,y=1.5, label=paste(&quot;nombre de tokens =&quot;, nrow(toc_maxweber)))+ coord_flip()+labs(title = &quot;Fréquence des tokens, unité = mots, stopwords éliminés&quot;, x=&quot;tokens&quot;, y=&quot;nombre d&#39;occurences&quot;, caption =&quot; &#39;Bureaucratie: le moyen le plus rationnel que lon connaisse pour exercer un contrôle impératif sur des êtres humains.&#39; &quot;) Figure 7.3: Distribution du nombre de mots, sans les stopwords On peut également constater que certains mots sont proches, par exemple les deux derniers sur le graphiques précédents qui sont des déclinaisons du verbe appliquer. Il peut alors être pertinent de regrouper ces différentes formes verbales (comme un mot au singulier et au pluriel, au féminin et au masculin, ou conjugué sous différentes formes), pour faciliter lanalyse. Cest ce quon fait avec les opérations de stemming ou de lemmatisation, présentées au chapitre XXX. 7.3.4 Les phrases On reproduit les mêmes opérations, mais cette fois sur les phrases de lexemple précédent. tokenize_sentences(MaxWeber)%&gt;%as.data.frame()%&gt;%rename(tokens=1)%&gt;%flextable(cwidth = 5) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-8d61b490{border-collapse:collapse;}.cl-8d575b6c{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-8d57825e{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-8d57f752{width:360pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8d57f753{width:360pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8d57f754{width:360pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} tokensBureaucratie: le moyen le plus rationnel que lon connaisse pour exercer un contrôle impératif sur des êtres humains.La bureaucratie est une forme d'organisation générale caractérisée par la prépondérance des règles et de procédures qui sont appliquées de façon impersonnelle par des agents spécialisés.Ces agents appliquent les règles sans discuter des objectifs ou des raisons qui les fondent.Ils doivent faire preuve de neutralité et oublier leurs propres intérêts personnels au profit de lintérêt général. 7.4 Ngrams Les ngrams sont des séquences de n tokens. Il peuvent être consécutifs, ou être toc_maxweber&lt;-tokenize_character_shingles(MaxWeber,n=3, n_min=1) %&gt;% as.data.frame()%&gt;%rename(tokens=1) ft&lt;-flextable(head(toc_maxweber)) ft .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-8d807218{border-collapse:collapse;}.cl-8d763fe6{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-8d7666e2{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-8d768dd4{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8d768dd5{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8d768dd6{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} tokensbbuburuurure foo&lt;-toc_maxweber %&gt;%mutate(n=1) %&gt;% group_by(tokens)%&gt;% summarise(n=sum(n))%&gt;%filter(n&gt;1) ggplot(foo, aes(x=reorder(tokens,n), y=n))+ geom_bar(stat=&quot;identity&quot;, fill=&quot;royalblue&quot;)+ coord_flip()+labs(title = &quot;unigram, bigram et trigram&quot;, x=&quot;ngram&quot;, y=&quot;nombre d&#39;occurences&quot;) Figure 7.4: Distribution du nombre de mots par post le principe de textcat est fondée sur ces ngram de lettre. Chaque langue se charactérise par une distribution particulière des ngrams. Pour décider de lappartenance dun text à une langue, si on dispose des profils de distribution, on comparera la distribution des ngrams du texte à ces références. On pourra ainsi calculer une distance et attribuer le texte à la langue dont il est le plus proche. 7.5 Choisir des n grams pertinents Dans ce livre lunité principales danalyse restera le mot. Mais nous savons, au moins intuitivement que certaines combinaisons de mots représentent des expressions qui ont la valeur dun mot, une valuer sémantique. Par exemple, lexpression \" Assemblée Nationale\". Ces deux mots réunis constituent un syntagme. Donc une unité de sens. Comment les identifier dans le flot des caractères? La technique est simple : si deux mots se retrouvent dans un ordre donné plus fréquemment que ce que le produit de leurs probabilités dapparition laisse espérer, cest quils constituent une expression. On peut imaginer faire un test du chi² pour décider si un couple de mots constitue une unité sémantique ou non. Le package quanteda propose une bonne solution à ce problème avec la fonction collocation. 7.6 Propriétés statistiques des ngrams Sur dun base dun corpus importants on peut calculer les probabilité dapparitions dun ngram. Cest une ressource de fournit google avec son Books Ngram Viewer. Processus de markov application à la correction "],["annotations-lexicales-et-syntaxiques.html", "Chapitre 8 Annotations lexicales et syntaxiques 8.1 Tokenization 8.2 Stemmatisation, lemmatisation et synonymisation 8.3 Part of Speech (POS) 8.4 Dépendances syntaxiques 8.5 reconnaissance dentités nommées 8.6 co-reférence", " Chapitre 8 Annotations lexicales et syntaxiques Pour aller au-delà de lanalyse du seul lexique et de lanalyse de la cooccurence des termes à travers les textes, comme le font les méthodes de typologie et danalyse factorielle des correspondance depuis longtemps, il est néçessaire danalyser le texte en tenant compte de ses propriétés syntaxiques. Depuis une dizaine dannées, des outils puissants, les annotateurs, sont proposés de manière accessible. Les plus connus sont Spacy, Stanford NLP et désormais UDpipe. Dans lenvironnement r différentes ressources sont disponibles : Quanteda, clean_nlp, Udpipe,  Ils sont disponibles désormais dans de nombreuses langues même si la richesse et la précision obtenues varient dune langue à lautres Ils sappuient sur des corpus plus ou moins étendus et spécialisés dannotations manuelle : les Treebanks. Ils réalisent souvent plusieurs tâches dont les principales sont les suivantes : Tokeniser Lemmatiser Identifier les parts of speech Identifier les dépendances syntaxiques Identifier les entités nommées. identifier les co-reférence 8.1 Tokenization 8.1.1 Les niveaux de tokenisation Un token est une unité danalyse dont la granularité est plus ou moins fine Le paragraphe est sans lunité lunité la plus générale, quand un texte est correctement rédigé, un paragraphe développe une idée. La phrase est lunité de langage, lui correpond un argument, une proposition. Lusage du point suivi dun espace est assez général pour les identifier. Cest lobjet de tokenizers qui feront mieux en analysant le contexte de la phrase pour décider plus précisément si le point sépare bien deux phrase distinctes. Cette unité de phrase elle essentielle. Le mot est la fois le niveau le plus évident et le plus courant. On peut aussi souhaiter extraire dun mot les suffixe et préfixe On peut pour certains problème descendre au niveau de la syllabe et donc du phonème. La lettre reste lunité insécable. 8.1.2 Un exemple en tidytext 8.2 Stemmatisation, lemmatisation et synonymisation Les mots prennent des formes variées, il peut être intéressant dans certains cas de réduire cette variété et ne considérer que lidée des mots. Deux techniques sont disponibles 8.2.1 la stemmatisation cest un 8.2.2 la lemmatisation Un lemme est un mot racine, sans inflexions de genre, de nombre ou de conjugaison. Cest généralement celui quon trouve dans les dictionnaire. 8.2.3 Synonymisation le cas de wordnet et linvention des synset synonymes, antonymes, hipponyne, hyperonymes.. https://cran.r-project.org/web/packages/wordnet/vignettes/wordnet.pdf 8.3 Part of Speech (POS) Dans une phrase les mots non pas la même valeur. Certains sont des nombres propres, ils se réfèrent à ce que nous venons de voir, cest à dire des entitées nommées, dautres désignent des catégories dobjet. Ce sont les noms communs qui se rapportent à des catégories de choses. Un marteau - si jen avais un - peut être nimporte quel marteau, la masse qui casse la pierre, ou ce petit marteau qui me permet denfoncer un clou dans le cadre du tableau. Des typologies universelles ont été construites, elles recouvrent des typologies plus spécifiques à certaines langues. Les désinences du latin ont par exemple disparu du français. Cette forme est spécifiques au latin, on la retrouvera en allemand. La notion de morphosntaxique désigne présisément que les variations de formes des mots dépendent dune règle syntaxique. Prenons le verbe, et sa forme, être, dont la forme au passé simple est était. La forme des mots change, mais lidée reste. Une catégorisation en 17 éléments est proposée. En voici les éléments et les définitions Un petit exemple avec le package UDpipe. library(udpipe) fr &lt;- udpipe_download_model(language = &quot;french&quot;) udmodel_french &lt;- udpipe_load_model(file = &quot;french-gsd-ud-2.5-191206.udpipe&quot;) Citations &lt;- read_csv(&quot;./data/Citations.csv&quot;) Flaubert&lt;-Citations %&gt;% filter(doc==1) UD &lt;- udpipe_annotate(udmodel_french, x=Flaubert$text) x &lt;- as.data.frame(UD) foo&lt;-x %&gt;% select(doc_id,paragraph_id, sentence_id, token_id,token,lemma,head_token_id, upos,feats)%&gt;%filter(sentence_id==1) flextable(foo) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-906d5450{border-collapse:collapse;}.cl-9061af9c{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9061d68e{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9061d68f{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-90621162{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-90621163{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-90621164{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-90621165{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-90621166{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-90621167{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} doc_idparagraph_idsentence_idtoken_idtokenlemmahead_token_iduposfeatsdoc1111Lele2DETDefinite=Def|Gender=Masc|Number=Sing|PronType=Artdoc1112lendemainlendemain9NOUNGender=Masc|Number=Singdoc1113futêtre9AUXMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Findoc1114,,6PUNCTdoc1115pourpour6ADPdoc1116EmmaEmma9PROPNdoc1117,,6PUNCTdoc1118uneun9DETDefinite=Ind|Gender=Fem|Number=Sing|PronType=Artdoc1119journéejournée0NOUNGender=Fem|Number=Singdoc11110funèbrefunèbre9ADJGender=Fem|Number=Singdoc11111..9PUNCT Les trois première colonnes identifient le document, les phrases et les mots. Des lemmes sont proposées. La colonne UPOS donne les part of Speech universel. 8.4 Dépendances syntaxiques Cest à Lucien Tesnière que lon doit lidée de la grammaire de la dépendance qui est au coeur du NLP moderne. Lidée est de déterminer au niveau de la phrase les relations entre ses termes de manière hierarchisée selon un principe de gouvernant à subordonné. Verdelhan-Bourgade (2020) résume son analyse de manière précise et concise : Tous les mots nont pas le même statut. Les mots pleins, qui « expriment directement la pensée » (p. 59), relèvent de quatre catégories structurales : les substantifs (notés par O), les adjectifs (A), les verbes (I), les adverbes (E). Les mots dits vides (souvent désigné de manière pratique par les stopwords aujourdhui) précisent le sens des autres, ou servent à marquer des relations.La connexion établit la relation entre mot régissant et mot subordonné. Lorsquun régissant commande un subordonné, cela constitue un nud, qui peut se faire à partir dune des quatre espèces de mots pleins. Il en donne lexemple suivant : « Très souvent mon vieil ami chante cette fort jolie chanson à ma fille » où lon peut repèrer: un nud verbal, central, qui commande des actants (ami, chanson, fille) et des circonstants (souvent). La valence est « le nombre de crochets par lesquels un verbe peut attraper des actants », à peu près équivalente à « voix ». les nud substantivaux (ami, chanson, fille), qui commandent des compléments (mon, vieil, cette, jolie, ma) le nud adjectival (jolie) qui commande ici le subordonné fort le nud adverbial, très étant subordonné à souvent. \" 8.4.1 Arbre syntaxique Larbre syntaxique est obtenue en analysant les relations entre les termes. Nous poursuivons avec UPpipe, lannovation précédente a déjà fait le travail. A chaque mot deux informations sont associée : la première est lindex du mot auxquel il se rapporte, la seconde est la nature de la relation. Onn utilise ici une fonction écrite par (bnosac](http://www.bnosac.be/index.php/blog/93-dependency-parsing-with-udpipe) pour donner une représentation graphique de larbre. plot_annotation &lt;- function(x, size = 3){ stopifnot(is.data.frame(x) &amp; all(c(&quot;doc_id&quot;,&quot;paragraph_id&quot;, &quot;sentence_id&quot;, &quot;token_id&quot;,&quot;token&quot;,&quot;lemma&quot;,&quot;head_token_id&quot;, &quot;upos&quot;,&quot;feats&quot;, &quot;dep_rel&quot;) %in% colnames(x))) x &lt;- x[!is.na(x$head_token_id), ] x &lt;- x[x$sentence_id %in% min(x$sentence_id), ] edges &lt;- x[x$head_token_id != 0, c(&quot;token_id&quot;, &quot;head_token_id&quot;, &quot;dep_rel&quot;)] edges$label &lt;- edges$dep_rel g &lt;- graph_from_data_frame(edges, vertices = x[, c(&quot;token_id&quot;, &quot;token&quot;, &quot;lemma&quot;, &quot;upos&quot;, &quot;xpos&quot;, &quot;feats&quot;)], directed = TRUE) ggraph(g, layout = &quot;linear&quot;) + geom_edge_arc(ggplot2::aes(label = dep_rel, vjust = -0.20), arrow = grid::arrow(length = unit(4, &#39;mm&#39;), ends = &quot;last&quot;, type = &quot;closed&quot;), end_cap = ggraph::label_rect(&quot;wordswordswords&quot;), label_colour = &quot;red&quot;, check_overlap = TRUE, label_size = size) + geom_node_label(ggplot2::aes(label = token), col = &quot;darkgreen&quot;, size = size, fontface = &quot;bold&quot;) + geom_node_text(ggplot2::aes(label = upos), nudge_y = -0.35, size = size) + labs(title = &quot;Tokenisation, PoS &amp; dependency relations&quot;) } plot_annotation(x, size = 3) Figure 8.1: arbre de dépendance 8.4.2 Vers des application plus générale Dans la phrase precédente on note que funèbre est ladjectif de journée. On peut être tenté de retrouver ces relations qui caractérisent des choses (les nouns ou noms choses) à des adjectifs. On souhaite faire une liste de ces paires. Lexemple va être court : un poème de Maupassant Maupassant&lt;-Citations %&gt;% dplyr::filter(doc==2) UD &lt;- udpipe_annotate(udmodel_french, x=Maupassant$text) foo &lt;- as.data.frame(UD) foo&lt;- foo %&gt;% select(paragraph_id,sentence_id, token_id,lemma,upos,head_token_id,dep_rel)%&gt;% mutate(key1=paste0(paragraph_id,sentence_id, token_id),key2=paste0(paragraph_id,sentence_id, head_token_id) ) On va donc construires un tableau lemme_cible x lemmes_associés, les premiers risqueront dêtres les noms communs, les seconds leurs adjectifs. # selection de la relation. res &lt;- foo %&gt;% filter(dep_rel == &quot;amod&quot;) #on y joint les dependences dep&lt;-res %&gt;% left_join(foo, by = c(&quot;key2&quot; = &quot;key1&quot;)) #on construit la tables des relations lemmes cibles -adjectifs table&lt;-as.data.frame.matrix(table(dep$lemma.x, dep$lemma.y)) #table$n&lt;-rowSums(table) #table$adj&lt;-rownames(table) #row.names(table) &lt;- table$adj le tableau obtenu est en fait la structure dun graphe bipartite. la représentation passe par un de igraph avec pour paramètres importants : * Taille des arcs (edge) : est proportionnelle à la force du lien ( nombre de relations) * Taille des noeud : proportiennel au rangs du noeud. * Couleur et forme des noeuds : lemme et lemme cible. * Un algorithme de force de Fruchterman and Reingold (1991) est employé pour représenter les positions relatives des mots et minimiser les superpositions. Dessiner le réseau bg &lt;-graph_from_incidence_matrix(table, weighted=TRUE) summary(bg) ## IGRAPH 9126fe5 UNWB 36 20 -- ## + attr: type (v/l), name (v/c), weight (e/n) #E(bg)$weight# See the vertex attributes #V(bg)$type #V(bg)$name # Plot the network shape = ifelse(V(bg)$type, &quot;circle&quot;, &quot;square&quot;) # assign shape by node type col = ifelse(V(bg)$type, &quot;peachpuff&quot;, &quot;darkolivegreen1&quot;) # assign color by node type plot(bg, vertex.shape = shape, vertex.label.cex=.9,vertex.label.color=&quot;black&quot;,vertex.color = col,edge.color=&quot;azure2&quot;,vertex.frame.color=col,vertex.label.family=&quot;TT Arial&quot;, vertex.size=0.5*igraph::degree(bg),layout=layout_with_fr,edge.width=1*E(bg)$weight,edge.curved=0.5) 8.5 reconnaissance dentités nommées En français courant les entités nommées correspondent largement à lidée de noms propres. Un nom propre à une entité. Une chose qui est est indépendemment des catégories qui peuvent létiqueter. John Dupont, né le 19 février 1898 à Glasgow et abattu à Verdun le 8 août 1917, est un personnage unique. John Dupont ne désigne par une catégorie, mais bien une personne singulière. La designation peut cependant être ambigüe, il y a un Paris, Texas., et un Paris sur Seine. La morphologie ne ressout pas lambiguité. les entités nommées appartiennent à différentes catégories dobjets : des noms de lieux, des noms de personnes, des noms de marques, des acronymes dorganisation, Elles ne représentent jamais une catégorie mais une unité singulière. https://cran.r-project.org/web/packages/nametagger/nametagger.pdf 8.6 co-reférence References "],["analyse-du-sentiment.html", "Chapitre 9 Analyse du sentiment 9.1 Un exemple avec syuzhet 9.2 La généralisation par le Liwc 9.3 Encore dautres généralisations 9.4 construire son propre dictionnaire", " Chapitre 9 Analyse du sentiment liu ping est le fondateur de lanalyse du sentiment et dès 2012 en donne une synthèse complète (Liu 2012) . Depuis des développement considérables ont été apportée par des méthodes de deep learning, et notamment les modèles transformer et renouvvelent consirablement le domaine. On restra ici à un niveau classique ou compositionnel. On travaillera sur un corpus davis trip advisor, sur la période avant co df&lt;-readRDS(&quot;./data/AvisTripadvisor.rds&quot;) 9.1 Un exemple avec syuzhet On utilise le package syuzhet et en particulier le dictionnaire nrc developpé et traduit par Mohammad and Turney (2013) ( Index Feel) Le même outil fournit un autre systéme dannotation qui compte les mentions déléments positifs ou négatifs, ainsi que démotions définies sur la base de linventaire de Plutchik (1982) on utilise simplement la fonction get_nrc_sentiment, en précisant le dictionnaire adéquat. Léchelle comprend en fait deux éléments : les 8 émotion de base *au sens de pluchik, et deux indicateurs de polarité. Lopérationnalisation réalisée par Mohammad and Turney (2013) sinscrit dans une tradition de la recherche en marketing, se souvenir de (???) et de (???). library(syuzhet) #analyse du sentimeent #paramétres method &lt;- &quot;nrc&quot; lang &lt;- &quot;french&quot; phrase&lt;-as.character(paste0(df$Titre,&quot; &quot;,df$Commetaire)) #extraction emotions &lt;- get_nrc_sentiment(phrase,language = &quot;french&quot;) polarity&lt;-subset(emotions,select=c(positive, negative)) df&lt;-cbind(df,polarity) On sintéresse surtout aux mentions positives et négatives (les émotions cest pour plus tard. (la mesure permet ainsi une dissymétrie des deux polarités, il y a le bien, le mal, le mal et le bien, mais aussi si qui nest ni mal ni bien). Les textes étant inégaux en taille on va ramener lindicateur de polarité au nombre de caractéres (sur une base de 500 c) de chaque contribution. En effet lalgo compte les valence et leur intensité est proportionnel à la longueur du texte. Ce qui est clairement démontré par la seconde figure. A partir de ces deux mesures, 4 indicateurs peuvent étre construits * Positivité : nombre de termes positifs pour 500 signes. * Négativitivé : nombre de termes négatifs pour 500 signes. * Valence : rapport du nombre de termes positifs sur les négatifs. * Expressivité : nombre de termes positifs et négatifs. le dernier graphe nous apprend que les jugements plutôt positifs sont aussi les plus expressifs. La froideur des avis les plus négatifs refléte-t-elle une crainte de la désaprobation sociale. Cest une piste de recherche à poursuivre, on pourrait sattendre à ce que les avis les plus négatifs surgissent plus facilement si la densité des négatives est plus importante et observer une sorte dautocorrélation. G1&lt;-ggplot(df, aes(x=positive))+geom_histogram(binwidth = 1, fill=&quot;darkred&quot;)+theme_minimal() G1 G2&lt;-ggplot(df, aes(x=negative))+geom_histogram(binwidth = 1,fill=&quot;Royalblue&quot;)+theme_minimal() G2 9.1.1 Valence et expression la linguistique donne aux mots une valence : elle peut être positive (bonheur), négative (malheur) ou neutre ( tranquité). Cest un régime ternaire. Chaque mot dune phrase est neutre, positif ou négatif. On peut doser les effets On a des dictionnaires df$text&lt;-as.character(paste0(df$Titre,&quot; &quot;,df$Commetaire)) df$WC&lt;-str_count(df$text, &quot;\\\\S+&quot;) df$positivity&lt;-(df$positive)/(df$WC) df$negativity&lt;-(df$negative)/(df$WC) df$valence&lt;-df$positivity-df$negativity df$expressivity&lt;-df$positivity+df$negativity G11&lt;-ggplot(df, aes(x=valence,y=expressivity ))+geom_point(color=&quot;grey&quot;)+geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x, bs = &quot;cs&quot;))+theme_minimal() G12&lt;-ggplot(df, aes(x=negativity,y=positivity ))+geom_point(color=&quot;grey&quot;)+geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x, bs = &quot;cs&quot;))+theme_minimal() G13&lt;-ggplot(df, aes(x=negativity,y= expressivity))+geom_point(color=&quot;grey&quot;)+geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x, bs = &quot;cs&quot;))+theme_minimal() G14&lt;-ggplot(df, aes(x=positivity,y= expressivity))+geom_point(color=&quot;grey&quot;)+geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x, bs = &quot;cs&quot;))+theme_minimal() plot_grid(G11, G12, G13,G14, labels = c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;,&#39;D&#39;)) 9.2 La généralisation par le Liwc Le liwc vient de lidée simple dun psychiatre qui a souhaité faire des diagnoistics de trauma craniens à partir des entretiens menés avec lmes patients atteints. (Tausczik and Pennebaker 2010) Le LIWC permet dobtenir dautres indicateurs du sentiment, une partie des 80 indicateurs proposés est relatif à des dimensions topicales dont trois groupes vont retenir notre attention dans la mesure où ils décrivent une partie de lexpérience relatée dans les commentaires. * La sensorialité ( voir, entendre, sentir) * Lorientation temporelle ( passé, présent, futur) * les émotions négatives (tristesse, colére, ) La procédure pour extraire ces notions est fort simple . On utilise linfrasturure de quanteda et une version francophone du dictionnaire (Piolat et al. 2011) # the devtools package needs to be installed for this to work #devtools::install_github(&quot;kbenoit/quanteda.dictionaries&quot;) library(cleanNLP) library(&quot;quanteda.dictionaries&quot;) dict_liwc_french &lt;- dictionary(file = &quot;FrenchLIWCDictionary.dic&quot;, format = &quot;LIWC&quot;) test&lt;-liwcalike(df$Commetaire,dictionary = dict_liwc_french) df&lt;-cbind(df,test) 9.3 Encore dautres généralisations Lapproche par dictionnaire sest déplacée vers lidentification dautre catégorie les valeurs morales 9.4 construire son propre dictionnaire faire des listes de lmots References "],["le-retour-des-méthodes-factorielles.html", "Chapitre 10 Le retour des méthodes factorielles 10.1 Retour sur lACP 10.2 SVD 10.3 LSA 10.4 NLM", " Chapitre 10 Le retour des méthodes factorielles Dun point de vue historique lanalyse factorielle est une idées de psychologue. Dailleur un des meilleurs packages jamais développé est sans doute psych Lécole française a choisit lACP, et ses dérivée appliquées à des données de comptage comme lAFCM. Ces méthodes sappuient sur des techniques de décomposition de matrice, et vise à réduire leur rang. Le problèmes quelle cherchent à résoudre est de réduire lespace dinformation. 10.1 Retour sur lACP LACP a été longtemps la méthode reine, elle est toujours fréquemmment utilisées.Elle vise à un but simple : représenter un ensemble de donnée comportant k variable à un petit nombre de combinaisons de ces variables qui représentent une grandes part de linformation. Historiquement elle a été développée pour analyser des matrices de corrélations multiples où X est une matrice de n individus et k variables ou mesure. Dans le domain textuel ce tableau correspond au dtm où les individus sont les documents et les colonnes les termes. \\(\\Sigma=X*X^\\top\\) Lidée principale est de trouver des combinaisons linéaires des variables qui capture succéssivement une part maximale de la variance. La solution à ce problème revient à décomposer cette matrice en fonction dun matrice de poids qui est determinée comme la matrice des valeur propre de \\(\\Sigma\\). \\(\\Sigma=F*V*F^\\top\\) https://fr.wikipedia.org/wiki/Jean-Paul_Benz%C3%A9cri https://www.math3ma.com/blog/understanding-entanglement-with-svd X=FSF 10.2 SVD quelle différence avec PCA https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca 10.3 LSA 10.4 NLM "],["le-retour-des-méthodes-factorielles-1.html", "Chapitre 11 Le retour des méthodes factorielles 11.1 Retour sur lACP 11.2 SVD 11.3 LSA 11.4 NLM", " Chapitre 11 Le retour des méthodes factorielles Dun point de vue historique lanalyse factorielle est une idées de psychologue. Dailleur un des meilleurs packages jamais développé est sans doute psych Lécole française a choisit lACP, et ses dérivée appliquées à des données de comptage comme lAFCM. Ces méthodes sappuient sur des techniques de décomposition de matrice, et vise à réduire leur rang. Le problèmes quelle cherchent à résoudre est de réduire lespace dinformation. 11.1 Retour sur lACP LACP a été longtemps la méthode reine, elle est toujours fréquemmment utilisées.Elle vise à un but simple : représenter un ensemble de donnée comportant k variable à un petit nombre de combinaisons de ces variables qui représentent une grandes part de linformation. Historiquement elle a été développée pour analyser des matrices de corrélations multiples où X est une matrice de n individus et k variables ou mesure. Dans le domain textuel ce tableau correspond au dtm où les individus sont les documents et les colonnes les termes. \\(\\Sigma=X*X^\\top\\) Lidée principale est de trouver des combinaisons linéaires des variables qui capture succéssivement une part maximale de la variance. La solution à ce problème revient à décomposer cette matrice en fonction dun matrice de poids qui est determinée comme la matrice des valeur propre de \\(\\Sigma\\). \\(\\Sigma=F*V*F^\\top\\) https://fr.wikipedia.org/wiki/Jean-Paul_Benz%C3%A9cri https://www.math3ma.com/blog/understanding-entanglement-with-svd X=FSF 11.2 SVD quelle différence avec PCA https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca 11.3 LSA 11.4 NLM "],["vectorisation-du-corpus.html", "Chapitre 12 Vectorisation du corpus 12.1 Word2vec 12.2 paragraph2vec 12.3 lavenir des modèles pré-entrainés", " Chapitre 12 Vectorisation du corpus 12.1 Word2vec 12.1.1 vectorisation pipe 12.1.2 closest2 12.1.3 map with tsne 12.2 paragraph2vec 12.3 lavenir des modèles pré-entrainés "],["topic-analysis.html", "Chapitre 13 Topic Analysis 13.1 LDA 13.2 STM", " Chapitre 13 Topic Analysis 13.1 LDA 13.1.1 le modèle de blei 13.1.2 implementation avec wor2vec 13.1.3 Représentation graphique 13.1.4 La 13.2 STM 13.2.1 13.2.2 "],["machine-learning-with-text.html", "Chapitre 14 Machine learning with text 14.1 simples models 14.2 art of featuring", " Chapitre 14 Machine learning with text 14.1 simples models 14.1.1 Naives bayes 14.1.2 elastic net 14.1.3 RF 14.2 art of featuring utiliser les plongements "],["deep-learning.html", "Chapitre 15 Deep Learning 15.1 Lenvironnement keras 15.2 Les architectures du texte : RNN, LTSM, Transformer et reformer 15.3 Les cas dapplications remarquables", " Chapitre 15 Deep Learning Pour lutilisateur en sciences sociales même si le deep learning est accessible dans un environnement r, et nous examinerons les possibilités offerte par kera, constuire son modèle de langage est sans doute hors de portée. Il sagira donc le plus souvent demployer des modèles pré-entrainés, et éventuellement de les réentrainer sur nos coprus de données. 15.1 Lenvironnement keras linterface de r pour Keras 15.1.1 Les fonctions principales cpu et gpu 15.1.2 Un premier exemple lequel? 15.1.3 Un deuxième exemple Lequel? 15.2 Les architectures du texte : RNN, LTSM, Transformer et reformer Les évolutions de ces 10 dernières années se caractèreisent par la recherche darchitectures qui prennent en comptent la structure du texte : il y a un ordre séquentiel : les mots font sens quand on les lis ou les entends après une succision dordre mots dont on connait les règles de composition. 15.2.1 rnn Il était logique que les rnn soient la première architecture qui a rendu des résultats intéressants. Leur acaractère autorégressif 15.2.2 ltsm Les ltsm on apportant une amélioration en prenant en compte des coorélations immédiates mais aussi plus lointaine dans le régl&amp;age de loubli et de la mémoire 15.2.3 transformer Linnovation des transformer, fondée sur des modèles a trou.. 15.2.4 reformer Les reformer étendent léchelle des corrélations possibles. Parfois ce qui donne le sens dun texte après 500 mots est peut _être le premier, même si nous ne lons pas perçu, il devient le contexte de la chute par exemple. exemple? 15.3 Les cas dapplications remarquables 15.3.1 Detection dintention quand la théorie des actes de langages rencontre linformatique 15.3.2 détection de toxicité des contenus 15.3.3 la detection des trolls 15.3.4 détection des sophismes et autres fallacies la lutte anti fake 15.3.5 La détection du sarcasme et de lironie 15.3.6 Lextraction dargumennts triplet "],["modèles-génératifs.html", "Chapitre 16 Modèles génératifs 16.1 simples models 16.2 art of featuring", " Chapitre 16 Modèles génératifs 16.1 simples models 16.1.1 Naives bayes 16.1.2 elastic net 16.1.3 RF 16.2 art of featuring utiliser les plongements "],["translation.html", "Chapitre 17 Translation 17.1 simples models 17.2 art of featuring", " Chapitre 17 Translation 17.1 simples models 17.1.1 Naives bayes 17.1.2 elastic net 17.1.3 RF 17.2 art of featuring utiliser les plongements "],["annexes-quelques-problèmes-très-techniques.html", "Chapitre 18 Annexes : quelques problèmes très techniques 18.1 La question de lencodage 18.2 Jouer avec les formats de données 18.3 Adopter des formats propres (tidy) 18.4 Les limites du calcul", " Chapitre 18 Annexes : quelques problèmes très techniques 18.1 La question de lencodage Le lecteur sera soumis très rapidement au problème compliqué et agaçant de lencodage des données. 18.2 Jouer avec les formats de données Le modèle excell appartient au passé ### json Le jason et la logique des liste ### xml lxml triomphant 18.2.1 des formats exotiques ris et bib pour la biblio 18.3 Adopter des formats propres (tidy) 18.4 Les limites du calcul Certaines opérations sont couteuse en calcul, notamment lannotation. calculer les temps de calculs parralélisation optimisation "],["references.html", "References", " References "]]
