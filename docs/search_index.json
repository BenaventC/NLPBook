[["le-retour-des-méthodes-factorielles.html", "Chapitre 9 Le retour des méthodes factorielles 9.1 Méthodes &amp; Données 9.2 Conclusions", " Chapitre 9 Le retour des méthodes factorielles Lobjectif de cette partie est de comprendre lévolution des différentes techniques de réduction de grands ensembles de données. Leur principe reste de réussir à exprimer sous forme de facteur(s), un ou plusieurs concepts non observés, latents, ou dintérêts, par le biais de calculs sur les attributs ou propriétés dune base de données. Replacées dans leur contexte de génèse et vocabulaire, ces méthodes de calculs feront ensuite lobjet dune description générale, centrée sur les prérequis statistiques et connaissances des modèles mathématiques utilisés. Nous proposerons une application exploratoire via le logiciel R, par un cas pratique avec un dataset de tweets scrappés pour lexemple sur la thématique du marketing TIC &amp; NTIC. Quatre algorithmes utilisés en analyse factorielle seront abordés, sous la lumière du cours de Philippe Malbos de Lyon 1 : (Malbos, n.d.) Analyse par Composante Principale (PCA) Analyse Factorielle sur Données Mixtes (AFDM) Analyse/Indexation Sémantique Latente (LSA/I) Factorisation de Matrices Non-Négatives (NMF) Développées dès 1904 par le psychologue Charles Spearman dans une ambition plus confirmatoire et hiérarchique, puis dans les années 1930 par les travaux de Hotelling (Thurstone 1949) et ensuite les travaux de Benzecri dès 1963 dans une dimension cette fois, plus exploratoire, les méthodes danalyses factorielles se sont proggressivement diluées dans la boîte à outils des différentes disciplines et courants académiques de la recherche scientifique. Ses principes de fonctionnement et autres calculs originels ont été confrontées à diverses sources et structures de données, que notre environnement de plus en plus numérique à amener à générer, à une vitesse saccélérant toujours plus. Benzecri (2006) A lissue de ce chapitre il sera possible de dresser un panorama synthtique de la diversité des techniques utilisées ainsi que de comprendre les avantages et inconvénients de chacune, en termes tant, de structures de données, que de modèles mathématiques, ou bien encore de vocabulaire idoine à privilégier afin de pouvoir interpréter convenablement les résultats fournis par ces algorithmes dans un contexte de traitements statistiques de données. (Forsé, n.d.) En effet, linteraction avec ces interfaces mathématiques sont aujourdhui largement informatisées, et le travail de danalyste se complète également par le devoir de savoir expliciter ses sorties logicielles. Afin de se soustraire pleinement de lillusion visuelle, il sera nécessaire pour chaque utilisateur de sintéresser aux coefficients plus quà leurs visualisations potentielles, et donc de raisonner en termes de relations de distances euclidiennes, dinterdépendance et décarts angulaires ou encore de corrélation, plutôt que de proximité. La diversité, parfois destabilisante aux premiers abords, des appellations que lon trouve dans la littérature (ACP/I, AFC/M, LSA/I, NFM) recoupent la même idée de synthèse des données appliquées selon k dimensions avec des déclinaisons robustes, complexes ou encore probabilistes. 9.1 Méthodes &amp; Données 9.1.1 Principes généraux Lensemble des données suivent des traitement obéissants aux règles du calcul matriciel et leur lecture est un prérequis aux explications qui vont suivre. Dun point de vue historique, lanalyse factorielle est une idée issue des courants de recherche en psychologie. Lapport de cette science dans lapplication computationnelle de lanalyse factorielle est importante, un des meilleurs packages jamais développé nest autre que psych. Les méthodes de calculs matriciels, comme nous avons pu le voir, ont également pu être généralisées par le développement de la puissance disponible, permettant de prendre en charge des traitements plus importants, liés à la taille, ou bien encore au caractère régulier ou singulier de ces objets. Lidée essentielle est quun objet (physique, symbolique) peut-être décrit par un ensemble de propriétés, dattributs observables et en ce sens, mesurables. Des relations peuvent exister entre les p variables précitées dont une synthèse factorielle de rang k est possible, exprimant des entitées latentes interprétables. Mathématiquement parlant, le développement accru des travaux de recherche autour de ces méthodes de calculs repose, entre autre, sur des questions comme le caractère unique ou multiple des facteurs / concepts sous jacents que lon souhaite utiliser pour résumer les données, ou bien encore sur les modalités de calculs, liéaires ou non, des combinaisons factorielles souhaitées.(Fodor 2002) Lidée principale est de trouver k combinaisons linéaires des p variables qui capturent succéssivement une part maximale de la variance, ce qui minimise réciproquement lécart total derreurs, dun effectif n dobservations, tel que k &lt;&lt; p. La matrice carrée symétrique de covariance K est obtenue pour un vecteur v normé (||v|| = vv= 1) tel que v Kv soit maximal. Cela se traduit par la production une à une de p matrices A de corrélations de Pearson de dimensions nixnj, sur lesquelles on applique la méthode des multiplicateurs de Lagrange  afin de maximiser la variance de chaque matrice Ap. La variance en est la somme pondérée, et sa valeur est reprise dans la cellule Knp de notre matrice de covariance. La diagonale contient alors lensemble des valeurs p, qui ne sont autres que les bases des eigeinvectors, qui serviront de base à lorthogonalité souhaitée de la projection de la ki dimensions par rapport à la ki-1. Cette approche est un cas particulier dun théorème dalgèbre liénaire quest le théorème spectral. Pour un ensemble de cas où les solutions sont impossibles, ou restent indéterminées, les matrices cibles ne peuvent être quapproximatives, ou estimées (nature convexe ou non convexe des interfaces mathématiques utilisées)(delporte2014?). Ces dernières sont alors plus désignées sous le symbole S. Dans le cas dénonication du théorème, cette matrice S est égale à toute matrice A, multipliée par sa diagonale D et son inverse A-1, tel que S= ADA-1. Pour autant, la plupart des matrices que lon peut trouver aujourdhui ne sont pas carrées, et leur structure ne retrace pas une sédimentation et répétition dun même phénomène de réponse à un questionnaire. En ce sens, pour un tableau doccurrences de mots, une décomposition selon une généralisation du théorème spectral que lon appelle Décomposition par Valeurs Singulières au matrice rectangulaires (SVD)  \\(\\Sigma\\). 9.1.2 Données Nous avons scrappé via le package rtweets un ensemble de 4 marqueurs : javascript, Cobol, Python, Java, selon les méthodologies robustes de construction de corpus déjà éprouvées(balech2019?) On peut supposer a priori que toute chose étant égale par elle même, les distinctions et structurations de la communication autour des langages de programmation soient similaires et uniformes, dans une première proposition relevant du marketing des technologies N/TIC.(benavent?) Dans un premier temps, nous allons chercher les fichiers cibles que lon charge dans une liste pour en automatiser laction de lecture et daggrégation des bases de données les unes avec les autres. Chaque extraction de la plateforme Twitter rapporte un ensemble de n tweets, contenant 91 variables, propres à la caractérisation de ces derniers. Lon retrouve des propriétés comme la localisation, la durée de vie, les nombres de posts, abonnées, dabonnements, nom de compte Cest à cette étape que la fréquence de publication sert à filtrer manuellement et qualitativement les internautes afin dauditer les comptes un à un. ###LECTURE DES FICHIERS data &lt;- list.files(&quot;C:/Users/jmonn/AppData/Local/Temp/Temp1_NLPBook-main.zip/NLPBook-main/NLPBook&quot;,pattern = &quot;Exemple&quot;) list_df &lt;- lapply(data, function(x) readRDS(x)) df_brut &lt;- bind_rows(list_df) ### ###ELIMINATION DOUBLONS doublons &lt;- which(duplicated(df_brut$status_id)) t_net_tex &lt;- df_brut[-doublons,] ### names(t_net_tex)[names(t_net_tex) == &#39;screen_name&#39;] &lt;- &#39;vect_exclude&#39; t_net_tex &lt;- t_net_tex %&gt;% anti_join(vect_exclu,by=&quot;vect_exclude&quot;) names(t_net_tex)[names(t_net_tex) == &#39;vect_exclude&#39;] &lt;- &#39;screen_name&#39; Après cette étape de constitution, il est nécessaire de supprimer les valeurs et lignes en double. Cette opération permet dobtenir les tweets dits primaires sur une période donnée allant du 30 juillet 2021 au 7 août 2021.De premiers calculs permettent dobserver que lhypothèse précédente dhomogénéïté de la structure de communication sur Twitter autour de nos quatre langages de programmation nest pas satisfaisante. ###CALCULS PRELIMINAIRES #Pourcentages t_1a &lt;- table(t_net_tex$Marque) t_net_tex &lt;- t_net_tex%&gt;%filter(is_retweet==&quot;FALSE&quot;) t_1b &lt;- table(t_net_tex$Marque) t_1perc &lt;- (t_1b/t_1a)*100 t_1perc &lt;- t(t_1perc) #Dates et maturité du compte t_net_tex$created_at &lt;- date(t_net_tex$created_at) t_net_tex$account_created_at &lt;- date(t_net_tex$account_created_at) t_net_tex$DureeV &lt;- ymd(t_net_tex$created_at)-ymd(t_net_tex$account_created_at) t_net_tex$DureeV &lt;- as.integer(t_net_tex$DureeV) #Sous ensemble pour visualisation t &lt;- t_net_tex%&gt;%select(screen_name,Marque)%&gt;%group_by(Marque)%&gt;%count(screen_name)%&gt;%arrange(desc(n)) ttab &lt;- t %&gt;%mutate(interv = cut(n, breaks=c(1,5,20,50,100,10000),labels = c(&quot;&lt;5&quot;,&quot;&lt;20&quot;,&quot;&lt;50&quot;,&quot;100&quot;,&quot;+&quot;))) t_tab &lt;- table(ttab$Marque,ttab$interv) library(vcd) ## Le chargement a nécessité le package : grid mosaic(~ interv + Marque, data = ttab,shade=T,legend=T, main = &quot;Production des langages&quot;) La répartition des tweets primaires pour chaque langage varie de 70% à 15%. Cette première observation peut justifier que lon souhaite étudier plus en détails le rôles de certaines variables dans les effets de diffusion. La mosaïque nous permet également de voir que les effectifs ne sont pas uninamiment distribués. Nous déduisons la maturité du compte (DureeV) par la soustraction de la date démission du tweet récolté à la création du compte.Une première visualiation filtrée, permet de voir la répartition du nombre dinternautes produisant plus de 10 tweets sur la semaine détude, pour chaque langage. Une première idée est que les communautés et les modes dapparitions de ces langages étudiés sur Twitter semblent, sur léchantillon donné, singuliers. Nous proposons de créer un sous ensemble de la base de données propres à chaque formats plébiscités. #Visualisation t &lt;- t_net_tex%&gt;%select(screen_name,Marque)%&gt;%group_by(Marque)%&gt;%count(screen_name)%&gt;%arrange(desc(n)) ty &lt;- t_net_tex%&gt;%select(screen_name,Marque)%&gt;%group_by(Marque)%&gt;%count(screen_name)%&gt;%arrange(desc(n))%&gt;%filter(n&gt;3) ty%&gt;%ggplot(aes(n,reorder(screen_name,n)),size=n)+geom_point() + facet_wrap(~Marque) +scale_x_log10() + theme(axis.text.y.left = element_blank()) ### ###EDITIONS DES FICHIERS kable(t_1perc) Cobol Java javascript Python 69.65782 42.65547 15.91388 28.40404 ### 9.1.3 Analyse par Composantes Principales (ACP/PCA) LACP et ses dérivées appliquées à des données de comptage comme lAFCM sous linfluence de J. Benzecri, ont longtemps étés les méthodes reines, et restent aujourdhui, toujours les plus fréquemmment utilisées. Elle vise à un but simple : représenter un ensemble de données comportant p variables, à un plus petit nombre entier k, tel que k combinaisons de ces variables représentent une grandes part de la variance de linformation exprimée dans la base de données. Lalgortihme cherche alors k combinaisons linéaires possibles dattributs, sans que ceux-ci soient trop génériques ou extrêmes, dun ensemble de données à résumer. Lexpression mathématique du résultat représente les k concepts supposés. En admettant que les combinaisons linéaires passent par le centre O dun nuage de points de coordonnées (Ox Oy), la variance en chaque point W (Wx Wy) présent dans léchantillon et différente de O est la distance au carré de la projection de chaque point W Wx Wy à Ox\" Oy\". On peut donc appliquer ce calcul pour k dimensions propres au nombre de combinaisons ou facteurs exigés de manière plus ou moins supervisée. La part totale de variance correspond à la moyenne de ces variances locales et est ajustée par lanalyse factorielle afin dêtre maximisée selon le procédé détaillé ci-dessus. Latteinte de ce premier objectif calculatoire de maximisation de la variance permet conséquemment de trouver lécart derreur minimal. Ce dernier est la moyenne du carré des distances entre les points observés Wn et théoriques Wn sur la kn combinaison linéaire. Le phénomène de prise de masse en fonction de lâge permet dadmettre, selon lespèce animale étudiée, un facteur (naturel) de croissance (naturelle).(Lemoine and Sartolou 1981) Dautres exemples se concentrent sur des données de natures mixtes (Newhouse et al., n.d.) sinon plus largement issues des sciences sociales, (Blanchard and Patou, n.d.) ou bien encore pour des cas concrets de besoins dévaluations délèves (Vallet 1985) Historiquement elle a été développée pour analyser des matrices de corrélations multiples où X est une matrice de n individus et p variables. Dans le domaine de lanalyse déléments textuels ce tableau correspond au document-term-matrix (dtm) et document-feature-matrix (dfm), où les individus sont alors considérés en tant que documents (tweets, reviews, litteratures) auxquels nous rattachons en colonne, les termes, afin den compter chaque occurence. Notre travail ici est donc de chercher à savoir si les paramètres structurels de chaque compte, traduisent une éventuelle présence différenciée. On sélectionne un sous ensemble de données quantitatives et lon calcule volontairement ici deux nouvelles variables dépendantes : le volume total de caractères produit sur la plateforme pour un internaute ainsi que le dénombrement de ses tweets. Chaque tweet étant borné à 280 caractères, ces deux variables sont donc liées par un certain effet naturel de lotissement, ou croissance par pallier. Contraint et limité par la puissance de calculs et ne pouvant donc faire une ACP sur 50 825 lignes, on filtre léchantillon sur les tweets ayant rencontrés plus dun retweet, et exigeons que le modèle soit factorisé en 3 dimensions (k=3). La population étudiée nest alors plus que composée de 6475 individus et 7 variables quantitatves. memory.limit(15000000) ## [1] 1.5e+07 df_pca &lt;- select(t_net_tex,Marque,screen_name,status_id,DureeV,retweet_count,followers_count,friends_count,statuses_count,display_text_width) df_pca$score &lt;- 1 df_pca &lt;- df_pca%&gt;%group_by(Marque,screen_name)%&gt;%summarise(DureeV,followers_count,friends_count,statuses_count,Totalcr=sum(display_text_width), Totaltwt=sum(score),retweet_count) ## `summarise()` has grouped output by &#39;Marque&#39;, &#39;screen_name&#39;. You can override using the `.groups` argument. ###ELIMINATION DOUBLONS doublons &lt;- which(duplicated(df_pca$screen_name)) df_pca_u &lt;- df_pca[-doublons,] ###MODEL PCA df_pca_u%&gt;%select(followers_count,friends_count,statuses_count,Totalcr,Totaltwt,DureeV,retweet_count)%&gt;%head(20) ## Adding missing grouping variables: `Marque`, `screen_name` ## # A tibble: 20 x 9 ## # Groups: Marque, screen_name [20] ## Marque screen_name followers_count friends_count statuses_count Totalcr ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Cobol ____ashy 14 447 288 5 ## 2 Cobol __Cobol__ 48 70 337 161 ## 3 Cobol __filipecosta 17 142 131 63 ## 4 Cobol __pimms__ 243 500 9927 5 ## 5 Cobol _64_64_128 875 946 63397 136 ## 6 Cobol _adriend_ 1687 220 14144 115 ## 7 Cobol _FunBot 859 2 243793 85 ## 8 Cobol _leandro_x 211 208 967 269 ## 9 Cobol _ManojC 329 606 7808 18 ## 10 Cobol _Masochysteria 144 68 56963 168 ## 11 Cobol _PedroPortela 176 208 25255 178 ## 12 Cobol _QiToY 235 225 49170 10 ## 13 Cobol _RuanKlein_ 81 182 930 14 ## 14 Cobol _solractg 3257 4993 75254 210 ## 15 Cobol _StaticVoid 4689 496 23445 153 ## 16 Cobol _syotarow 22393 23177 55850 11 ## 17 Cobol _vermeer_ 118 120 3254 87 ## 18 Cobol _victorsk 385 171 18152 159 ## 19 Cobol 02Talon 1452 745 15480 75 ## 20 Cobol 0to999 5093 5011 20525 14 ## # ... with 3 more variables: Totaltwt &lt;dbl&gt;, DureeV &lt;int&gt;, retweet_count &lt;int&gt; df_pca_uf &lt;- df_pca_u%&gt;%filter(retweet_count&gt;1) res.pca &lt;- PCA(df_pca_uf,quali.sup = 1:2,scale.unit = TRUE, graph = FALSE,ncp =3) var &lt;- get_pca_var(res.pca) set.seed (123) my.cont.var &lt;- rnorm (7) # Colorer les variables en fonction de la variable continue fviz_pca_var(res.pca, col.var = my.cont.var, gradient.cols = c(&quot;red&quot;, &quot;Pink&quot;, &quot;Purple&quot;), legend.title = &quot;Cont.Var&quot;) fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 40)) Les eigenvectors et eigenvalues sont remarquables dans les cos2 et contributions de nos 7 variables au 3 dimensions souhaitées respectivement. Les deux premières dimensions expriment près de 50% de la variance et les variables naturelles issues de la plateforme sont distinctes de nos deux totaux, mais évoluent avec la maturitée (DureeV) du compte. Leffet de lotissement, ou de croissance par pallier exprimée précedemment sobserve ici dans le sens où ces propriétés peuvent être exprimée en une seule. par(mfrow=c(1,2)) corrplot(var$cos2, is.corr=FALSE) corrplot(var$contrib, is.corr=FALSE) Ces deux matrices expriment différemment le graphique des dimensions précédentes. Si lon peut raisonner en terme déquivalence, la direction des flèches est propre à la couleur des cercles, la taille dun vecteur est proportionnelle à la taille des points de corrélation ci-dessus, pour chacune des dimensions 1 et 2. Il est intéressant maintenant de pouvoir proposer un graphique des individus, tout en utilisant les étiquettes qualitatives afin de décrire la composition du nuage de points. fviz_pca_ind(res.pca, geom.ind = &quot;point&quot;, # Montre les points seulement (mais pas le &quot;text&quot;) col.ind = df_pca_uf$Marque, # colorer by groups palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;,&quot;#AA4E07&quot;), addEllipses = TRUE, # Ellipses de concentration legend.title = &quot;Groups&quot; ) De prime abord, nous remarquons que les ellipses de confiance se chevauchent et sont toutes centrées sur lorigine du repère. Pour autant, les facteurs exposés ici semblent influer sur la ventilation de la distribution des comptes des internautes. Pour une analyse détaillée et les détails du code, se réferrer à Kassambora. Le dernier graphique, mélange des données qui ne sont pas traitables directement de manière simultanée par lACP, qui fonctionne sur des données exclusivement quantitatives. (La variable Marque nétait pas présente sur le graphique des eigeinvectors) Pour ce faire, on propose lutilisation dun algorithme factoriel admettant en entrées un fichier de données mixtes, afin de pouvoir pleinement voir les influences dune variable qualitative propre au langage de programmation (Marque) du tableau de données ci-dessus. (pages?) 9.1.4 Analyse Factorielle sur Données Mixtes (AFDM/FAMD) Le tableau dentrées est le même que celui de lACP, auquel nous rajoutons la variable des Marques. df_pca_uf &lt;- df_pca_uf%&gt;%ungroup() df_famd&lt;- df_pca_uf%&gt;%select(Marque,DureeV,retweet_count,followers_count,friends_count,statuses_count,Totalcr,Totaltwt) res.famd &lt;- FAMD(df_famd,graph = F, ncp = 3) fviz_screeplot(res.famd) Les trois premières dimensions groupes près de 40 % de la variance totale de léchantillon. # Contribution à la première dimension g1 &lt;- fviz_contrib (res.famd, &quot;var&quot;, axes = 1) # Contribution à la deuxième dimension g2 &lt;- fviz_contrib (res.famd, &quot;var&quot;, axes = 2) #Contribution à la troisième dimension g3 &lt;- fviz_contrib (res.famd, &quot;var&quot;, axes = 3) plot_grid(g1,g2,g3,ncol=3,nrow = 1) Leur description est renseignée pour chacune des dimensions, propre à lactivité mesurée sur le set de données, la maturité du compte, et sa marque. Nous présentons par la suite les résultats principaux dune AFDM. fviz_famd_var(res.famd, &quot;quanti.var&quot;, col.var = &quot;contrib&quot;, gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = TRUE) fviz_famd_var(res.famd, &quot;quali.var&quot;, col.var = &quot;contrib&quot;, gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;) ) fviz_famd_ind(res.famd, col.ind = &quot;cos2&quot;, gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = T) ## Warning: ggrepel: 6461 unlabeled data points (too many overlaps). Consider ## increasing max.overlaps On peut donc obtenir sur le même modèle que précedemment, les eigen composants des variables quantitatives et qualitatives. La visualisation est similaire à celle obtenue par lACP, mais lon peut faire de même avec la visualisation des modalités de notre variable qualitative, où lon peut observer un positionnement légèrement différents en terme de contribution aux dimensions. 9.1.5 Analyse Sémantique Latente (ASL/LSA) Lanalyse sémantique latente sest développée sous le travail des linguistiques, et la généralisation doutils tels que les moteurs de recherche. (evangelopoulos2012?) Le retour sur histoire que nous fait lauteur, ainsi que sa description des différents principes de calculs et vigilances pour lanalyste à garder en mémoire, renseigne sur les heuristiques de cette méthode : Son principe fondateur est de réussir à retrouver dans une collection de documents, un ensemble de patterns présents ou absents, dans un système déquations sexprimant chacune en fonction des autres. Cette expression réciproque sopère à laide de la décomposittion spectrale de chacun de ses mots. En ce sens, elle peut se comprendre comme un résultat de la recherche allant dans la quantisation des vecteurs, la régression multinomiale, ou bien encore la topologie, comme identifiée par Fodor en 2002, ou dautres applications croisées. (song2009?) Le document dentrée pour la LSA est un VSM, équivalent à la dfm énoncée ci-dessus, appelée A. Au sens de la théorie spectrale (SVD) on obtient : A = UEVt avec U = , V =, et E la matrice diagonale, Vt la transposée. Nous nous intéressons donc à la décomposition du tableau précédent. Nous intégrons donc a notre analyse sur les variables, les éléments textuels. Nous supprimons les lemmes propres au entitées nommées étudiées : Cobol &amp; Javascript. Pour les détails et limplémentations du code, se réferrer à C. Benavent. library(udpipe) library(quanteda) library(quanteda.textmodels) library(stringr) vect_upos &lt;- c(&quot;ADJ&quot;,&quot;NOUN&quot;,&quot;VERB&quot;) annot_udd &lt;- df_trt_annot%&gt;%filter(upos%in%vect_upos) annot_udd &lt;- annot_udd%&gt;%select(Marque,lemma)%&gt;%filter(Marque==&quot;Cobol&quot;) annot_udd &lt;- annot_udd%&gt;%mutate(Off_Acc=ifelse(str_detect(lemma,&quot;@.&quot;)==TRUE,&quot;TRUE&quot;, ifelse(str_detect(lemma,&quot;https.&quot;)==TRUE,&quot;TRUE&quot;, &quot;FALSE&quot;))) annot_udd &lt;- annot_udd%&gt;%filter(Off_Acc==&quot;FALSE&quot;) dfm_lsa &lt;- dfm(annot_udd$lemma, tolower = TRUE, what = &quot;word&quot;, docvars = &quot;Marque&quot;) ## Warning: &#39;dfm.character()&#39; is deprecated. Use &#39;tokens()&#39; first. ## Warning: docvars argument is not used. ## Warning: &#39;...&#39; should not be used for tokens() arguments; use &#39;tokens()&#39; first. ## Warning: docvars argument is not used. ## Warning: docvars argument is not used. mylsa &lt;- textmodel_lsa(dfm_lsa,2) proxD&lt;-mylsa$docs[, 1:2] library(Rtsne) #rtsne tsne_out &lt;- Rtsne(proxD, dims = 2, initial_dims = 100, perplexity = 20, theta = 0.5, check_duplicates = FALSE, pca = TRUE, max_iter = 300) x&lt;-tsne_out$Y x&lt;-as.data.frame(x) x$F1&lt;-x[,1] x$F2&lt;-x[,2] x&lt;-cbind(annot_udd,x) df&lt;-subset(x) #pour un quartier library(ggrepel) lsacob &lt;- ggplot(df,aes(x = F1,y=F2))+geom_point() prox&lt;-mylsa$features[, 1:2] terms&lt;-as.data.frame(prox) #rtsne tsne_out &lt;- Rtsne(terms, dims = 2, initial_dims = 50, perplexity = 20, theta = 0.5, check_duplicates = FALSE, pca = TRUE, max_iter = 300) plot(tsne_out$Y) x&lt;-tsne_out$Y terms$term&lt;-row.names(prox) plot&lt;-cbind(x,terms) plot$F1&lt;-plot[,1] plot$F2&lt;-plot[,2] gglsacob &lt;- ggplot(plot,aes(x = F1,y=F2))+geom_point()+geom_text(aes(label=term),hjust=0.5, vjust=0.5, size=2.5) ### ### annot_udd &lt;- df_trt_annot%&gt;%filter(upos%in%vect_upos) annot_udd &lt;- annot_udd%&gt;%select(Marque,lemma)%&gt;%filter(Marque==&quot;javascript&quot;) annot_udd &lt;- annot_udd%&gt;%mutate(Off_Acc=ifelse(str_detect(lemma,&quot;@.&quot;)==TRUE,&quot;TRUE&quot;, ifelse(str_detect(lemma,&quot;https.&quot;)==TRUE,&quot;TRUE&quot;, &quot;FALSE&quot;))) annot_udd &lt;- annot_udd%&gt;%filter(Off_Acc==&quot;FALSE&quot;) dfm_lsa &lt;- dfm(annot_udd$lemma, tolower = TRUE, what = &quot;word&quot;, docvars = &quot;Marque&quot;) ## Warning: &#39;dfm.character()&#39; is deprecated. Use &#39;tokens()&#39; first. ## Warning: docvars argument is not used. ## Warning: &#39;...&#39; should not be used for tokens() arguments; use &#39;tokens()&#39; first. ## Warning: docvars argument is not used. ## Warning: docvars argument is not used. dfm_lsa &lt;- dfm_trim( dfm_lsa, min_termfreq = 10) mylsa &lt;- textmodel_lsa(dfm_lsa,2) proxD&lt;-mylsa$docs[, 1:2] library(Rtsne) #rtsne tsne_out &lt;- Rtsne(proxD, dims = 2, initial_dims = 100, perplexity = 20, theta = 0.5, check_duplicates = FALSE, pca = TRUE, max_iter = 300) x&lt;-tsne_out$Y x&lt;-as.data.frame(x) x$F1&lt;-x[,1] x$F2&lt;-x[,2] x&lt;-cbind(annot_udd,x) df&lt;-subset(x) #pour un quartier library(ggrepel) lsajav &lt;- ggplot(df,aes(x = F1,y=F2))+geom_point() prox&lt;-mylsa$features[, 1:2] terms&lt;-as.data.frame(prox) #rtsne tsne_out &lt;- Rtsne(terms, dims = 2, initial_dims = 50, perplexity = 20, theta = 0.5, check_duplicates = FALSE, pca = TRUE, max_iter = 300) plot(tsne_out$Y) x&lt;-tsne_out$Y terms$term&lt;-row.names(prox) plot&lt;-cbind(x,terms) plot$F1&lt;-plot[,1] plot$F2&lt;-plot[,2] gglsajav &lt;- ggplot(plot,aes(x = F1,y=F2))+geom_point()+geom_text(aes(label=term),hjust=0, vjust=0, size=3) lsacob gglsacob lsajav gglsajav Les différentes visualisations produites montrent que les langages baignent, outre certains phénomènes de pollution, dans un vocabulaire emprunt à leurs domaines technico-fonctionnels propres. On observe la présence des domaines du Web en ce qui concerne JS et de la gestion financière des grands groupes pour ce qui concerne le vieil ami Cobol. En outre, les mots associés à ce dernier sont dans une thématique métier plus professionnelle, tant dun point de vue de lingénierie informatique que financière. Lutilisation de javascript semble plus orientée vers son environnement informatique digital, web et outils propres (frameworks et apparentés) plus quau domaine fonctionnel sur lesquels il est potentiellement appliqué. Cela peut également expliquer en partie les différentes populations pressenties : évoquer Cobol nécessite de sintéresser à linformatique, ce langage est inconnu pour bon nombre dinformaticiens débutants et invisible du grand public, en revanche, plein de tutoriels et de parcours de formations proposent des travaux pratiques, qui permettent aux apprennants de développer un robot à larchitecture plus ou moins complexe, dont la durée de vie se limite pour les plus élémentaires quà un script javascript. Cette première analyse sémantique qualifie différemment les comptes étudiés et complète lanalyse des premiers indicateurs soumis à lACP/AFDM par une restitution de leurs univers sémantiques. Se concentrant sur les éléments destructurés textuels, cet algortihme est capable de recomposer une structure vectorielle à partir destimations algébriques itératives empruntant aux règles de calculs matriciels précedemment énoncées. Réussir à croiser les différents éléments structurels et sémantiques, permet de qualifier un peu mieux la population dinternautes, et de rattacher des éléments dinformations librements exprimés, à des éléments plus latents. Ici, un modèle simple, exploratoire et pédagoqiue de LSA a été mené. Cependant, difféfentes variantes, plus ou moins probabilistes ou supervisées existent, et leurs spécificités permet de comprendre les équivalences et traits communs entre les divers algortihmes retenus ici pour létude. (gaussier2005?)(ding2005?) 9.1.6 Non-negative Matrix Factorization La branche mathématique appliquée au calcul matriciel développe depuis longtemps un important travail de recherche sur les propriétés mathématiques de cet objet, (chen1984?) dont les problèmes se classifient selon une échelle de difficulté NP. (vavasis2007?) Plusieurs questions ont été abordées, notamment sur linterprétabilité des facteurs (lee?), lajustement de certains paramètres comme la co-entropie, (li2014?) la détection du nombre de composantes, (shitov2017?) ou encore la divergence B. (votte?) Certaines propriétés des matrices offrent en effet des commodités opérationnelles dans la construction darchitectures techniques computationnelles diverses, notamment dans les domaines du Machine et Deep Learning, mais génère dans un usage statistique scientifique nécessairement de lincertitude sur lunicité du résultat obtenu. (campbell1981?) Un important travail de recherche faisant lobjet dune thèse en 2014 documente historiquement et techniquement les diverses approches de factorisation matricielle et plus précisément, celles appliquées aux matrices de données non-négatives dont lhypothèse méthodologique de calculs admettent la positivité des composantes ou se basent sur leur non-négativité. (limem?) Aujourdhui, la généralisation de son utilisation est possible pour les utilisateurs de différents langages de programmation,(gaujoux2010?) et plusieurs travaux témoignent de ses applications courantes à divers secteurs : lanalyse dimage, le text mining, (zurada2013?) lanalyse spectrophotométrique, (gillis2014?) ou bien encore lanalyse financière,(cazalet?) et la détection des fake news. (shu2019?)  Ici, le premier pas de lanalyse est de créer une matrice A des termes et des documents. Le système considéré est donc de m langages exprimés via y tweets par t mots. On peut donc définir un ensemble A  R m  t de mots relatifs à ces dernières quil va falloir factoriser via cet algorithme NMF, en k dimensions, pour générer deux matrices H  R t  ket W  R m  k représentant lassociation aux dimensions du vocabulaire et des marques respectivement. Le regroupement de ce tableau depuis notre base détude, appelé Document-feature-matrix est un ensemble de features (Mots) associés à chaque groupe (Marques) qui permet de fournir un fichier dentrée à lalgorithme dimplémentation de linterface NMF, soit une matrice m  t. Notre modèle est dit simple approché, non informé ou pondéré. Le fonctionnement de cet algorithme de factorisation est de décomposer le fichier dentrée A en deux matrices, W, tel que W= u x k et H, tel que H = k x v ou k est ici déterminé à laide dune fonction intégrée au package R NMF, et déterminé selon le corpus fourni à k = 15 dimensions. Lorsque cet algorithme opère une coefficientisation par approximation de la décomposition du fichier A, en deux fichiers W &amp; H de rang k, il nous permet de réaliser le produit matriciel de ces deux fichiers de sorties, et donc de proposer une réorganisation du fichier A selon k  facteurs sous-jacents, et dainsi obtenir le rang de factorisation non-négative. De manière naturelle, il serait intéressant de savoir si lon peut observer un modèle théorique de vocabulaires spécifiques dans la réorganisation finale obtenue.  Ici donc, la matrice dentrée A est composée des 4 langages précités, et de 3641 mots retenus pour létude, composer des verbes, noms et adjectifs. Afin de visualiser les résultats produits, nous utilisons des fonctions de type heatmap, adaptées à la représentations de matrices. aheatmap(my.nmf@fit@W) aheatmap(my.nmf@fit@H) Les représentations graphiques des matrices W et H permettent dobserver la distributivité des mots selon les facteurs (H) ou les variables (W), le gradient de coloration étant proportionnel à la covariance des éléments. (gaujoux?) En réalisant le produit matriciel de ces deux tableaux il est possible, dobtenir la liste des mots les plus typiques de la modalité de variable étudiée, ici le langage de programmation. Les matrices de coefficients représentent des résultats intermédiaires, permettant de cenraliser dans un tableau de données lensemble des expressions linéaires possibles dun système en fonction de ses paramètres et positions. Le consensus, sobtient suites aux différentes itérations demandées, ici fixées à 30. basismap(my.nmf) coefmap(my.nmf,Colv=&quot;basis&quot;) consensusmap(my.nmf,Rowv=TRUE, Colv=TRUE, scale=&quot;none&quot;) tcob ## Cobol javascript ## 1 développeur citation ## 2 utiliser parendre ## 3 aimer vuej ## 4 technologie 4tchat ## 5 vouloir css ## 6 langage nuitdebour ## 7 avoir woocommerce ## 8 cobol opencart ## 9 sgbd php ## 10 rigoler ecommerce ## 11 aller webdevembre ## 12 jouer pari ## 13 machine gir ## 14 vieux mrproverber ## 15 falloir symfony ## 16 mettre prestashop ## 17 bancaire seo ## 18 it faire ## 19 miagiste france ## 20 parler saintdeni ## 21 indien livraison ## 22 banque magento ## 23 pouvoir castre ## 24 développer drogue ## 25 oui nuitdebout ## 26 incroyable rastabot ## 27 fête mrproverbe ## 28 chance passer ## 29 ancien avoir ## 30 ordinateur plan ## 31 cours disponible ## 32 dernier weed ## 33 étonner dispo ## 34 apprendre pass ## 35 mauvais mode ## 36 trouver javascriptjob ## 37 taf commande ## 38 jour test ## 39 augmenter sanitaire ## 40 découvrir fafa ## 41 module carrer ## 42 gratuit sécurité ## 43 passer réduire ## 44 finir livraire ## 45 coder tout ## 46 ben code ## 47 source compilateur ## 48 raison python ## 49 truc &amp; ## 50 tourner pouvoir 9.2 Conclusions Ces techniques se sont développées en se confrontant aux différentes structures de données et développements des courants de recherche en mathématiques. Leur existence nempêche pas leur utilisation conjointe à des fins de classifications, ou réduction de dimensions. (hassani2020?) Part ailleurs, dautres modèles existent et permettre de comprendre les différentes équivalences entre les méthodes décrites ci-dessus. (buntine2002?) En ce sens, le travail de Pochon en 2020 dresse un panorama plus situé, des différentes techniques et de leurs enjeux, tant en termes daccessibilité, que dapplicabilité. (pochon?) A vos claviers ? References "]]
