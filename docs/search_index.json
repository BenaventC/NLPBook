[["topic-analysis.html", "Chapitre 9 Topic Analysis 9.1 Outils 9.2 LDA 9.3 STM", " Chapitre 9 Topic Analysis Lanalyse thématique sattache à résumer de grands ensembles plus ou moins structurés de données textuelles en principaux thèmes probables. Les données dentrées sont des éléments textuels constituant une collection de documents, eux-mêmes composés de mots, le tout est alors à considérer comme une mixture, ou un mélange de thèmes à générer. Ces algorithmes entraînés reposent sur des méthodes de calculs empruntent aux domaines du machine learning et de lintelligence artificielle. Les différentes variantes de sa mise en oeuvre tiennent compte : Du plongement lexical Du rôle de potentielles métadonnées à intégrer 9.1 Outils Les packages, [Word2Vec] et [stm] seront utilisés ci-dessous afin de mener les analyses. 9.2 LDA 9.2.1 Le modèle original de Blei Le rôle de cet algorithme est donc, à un corpus donné, établir un modèle possible de mixture thématique, en lisant et reliant successivement les mots entre eux. Cela est rendu possible quand les variables sont dîtes interchangeables.(Blei, n.d.) Nous avons donc, au sens de Blei : Un vocabulaire \\(V\\) indexant tous les mots, De documents composé dune séquence de \\(N\\) mots Un Corpus \\(D\\), collection \\(M\\) de documents Un ensemble \\(Z\\) de topics potentiels Définir un réel \\(k\\) égal au nombre de topic souhaité Chaque mot \\(w\\) se voit donc associer des coefficients \\(\\beta_{i,j}\\) et \\(\\alpha_{i}\\) dans un espace \\(\\theta_{i}\\) de distribution au sein des documents, obtenu par une allocation de Dirichlet. Ainsi, on a successivement : \\[\\begin{align} p(\\theta|\\alpha) = \\frac{\\Gamma(\\sum_{i=1}^k \\alpha_{i}}{\\prod_{i = 1}^{k}\\Gamma(\\alpha_{i})}\\theta_{1}^{\\alpha_{1}-1}...\\theta_{k}^{\\alpha_{k}-1} \\end{align}\\] et lensemble des éléments prédéfinis reliés de la manière suivante : \\[\\begin{align} p(\\theta,z,w\\|\\alpha,\\beta)=p(\\theta\\|\\alpha)\\prod_{n=1}^{N}p(z_{n}\\|\\theta)p(w\\_{n}\\|z\\_{n},{\\beta}) \\end{align}\\] Un schéma explictif est proposé par H. Naushan, en 2020. Daprès le théorème de Finetti, lorsque les variables sont échangeables, il est possible de les visualiser selon une infinité de mixtures. La probabilité dune séquence de mots/topics peut donc sexprimer de la sorte : \\[\\begin{align} p(w|z)=\\int{} p(\\theta) (\\prod_{n=1}^{N}p(z_{n}|\\theta)p(w_{n}|z_{n})d\\theta \\end{align}\\] Cela se considère comme une mixture continue dunigrammes ou la probabilité de rencontrer un mot se résume à sa distribution \\(p(w|\\theta,\\beta)\\) : \\[\\begin{align} p(w|\\theta,\\beta)=\\sum_{z}p(w|z,\\beta)p(z|\\theta) \\end{align}\\] La distribution marginale \\(p(w|\\alpha,\\beta)\\) de chaque document, est donc intrinsèque à cette idée de mixture de thématique, et sobtient ainsi : \\[\\begin{align} p(w|\\alpha,\\beta)= \\int p(\\theta,\\alpha)(\\prod_{n=1}^{N}p(w_{n}|\\theta,\\beta))d\\theta \\end{align}\\] 9.2.2 Implementation avec wor2vec Ces techniques se sont développées sous langle de lhypothèse, ou contrainte de Harris, dont le postulat propose que les mots apparaissants dans des contextes similaires soient de sens identiques. Le développement des techniques danalyses traitant le mot comme un vecteur avec un ensemble de coordonnées dans un reprère propre à un corpus entièrement vectorisé permet de tester cette hypothèse originelle en sémantique distributionnelle. En ce sens, lapproche par lintégration des mots permet de réinduire une certaine dépendance, contrainte, linéarité et ordonnancement naturel du corpus au sein dune mixture, donc le principe même temps à avoir une infinité de représentation. La structure de cette approche sappuie sur différentes couches de réseaux de neurones venant travailler réciproquement sur des obervations et des prédictions : Les mots observés, dont on peut prédire le contexte (Skip-gram) Les éléments du contexte observés, dont on peut prédire le mot (CBOW) Lidée de plongement lexical tient alors dans cette dynamique double didentification et de rattachament des éléments textuels ensembles, selon différentes méthodes de vraisemblance/mesure. 9.2.2.1 Skip-gram 9.2.2.2 Continus Bag-Of-Words 9.2.3 Représentation graphique 9.3 STM La Modélisation Thématique Structurelle est un prolongement du modèle LDA développé ci-dessus. Permettant de parvenir aux mêmes types de résultats de regroupements thématiques par plongement lexical, cette dernière se distingue dans le sens où elle permet dassocier dautres variables, ou méta-données, au corpus traité afin de prendre en compte les relations de leurs modalités au contenu. Ainsi, elle crée la notion de prévalence dun topic, qui permet de prendre en compte sa fluctuation en fonction de la propre évolution de la covariance des éléments dune même mixture. (Roberts, Stewart, and Airoldi 2016) References "]]
