[["index.html", "NLP avec r et en français Chapter 1 Préface 1.1 La structure du livre 1.2 Les jeux de données 1.3 Prérequis 1.4 A propos de bookdown", " NLP avec r et en français Sophie Balech et Christophe Benavent 2021-06-26 Chapter 1 Préface Ce livre synthètise ce que nous avons appris depuis quelques années. Il a pour but dêtre reproductible 1.1 La structure du livre L analyse NLP peut être analysée comme un processus qui va de la collecte ( ou production ) jusqu à la diffusion. 1.2 Les jeux de données Plusieurs cas, et les données afférantes sont utilisées : Trump Twitter Archive : Lintégralité des tweets de Trump jusquà son banissement le 8 Janvier 2021. Confinement Jour J 1.3 Prérequis Ceci est un livre écrit en Markdown. avec le package Bookdown Le code sappuie très largement sur tidyverse et emploie largement les ressources de ggplot . Les packages seront introduits au fur et à mesure. En voici la liste complète. knitr::opts_chunk$set(echo = TRUE, message=FALSE,warning=FALSE) #boite à outils library(tidyverse) # inclut ggplot pour la viz, readr et ## Warning: le package &#39;tidyverse&#39; a été compilé avec la version R 4.0.5 ## Registered S3 methods overwritten by &#39;tibble&#39;: ## method from ## format.tbl pillar ## print.tbl pillar ## -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- ## v ggplot2 3.3.3 v purrr 0.3.4 ## v tibble 3.0.1 v dplyr 1.0.5 ## v tidyr 1.1.3 v stringr 1.4.0 ## v readr 1.4.0 v forcats 0.5.1 ## Warning: le package &#39;ggplot2&#39; a été compilé avec la version R 4.0.5 ## Warning: le package &#39;tidyr&#39; a été compilé avec la version R 4.0.5 ## Warning: le package &#39;readr&#39; a été compilé avec la version R 4.0.5 ## Warning: le package &#39;dplyr&#39; a été compilé avec la version R 4.0.5 ## Warning: le package &#39;forcats&#39; a été compilé avec la version R 4.0.5 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(cowplot) #pour créer des graphiques composés ## Warning: le package &#39;cowplot&#39; a été compilé avec la version R 4.0.3 library(colorspace) #pour les couleurs library(ggridges) ## Warning: le package &#39;ggridges&#39; a été compilé avec la version R 4.0.5 # NLP library(rtweet) ## ## Attachement du package : &#39;rtweet&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## flatten library(quanteda) ## Warning: le package &#39;quanteda&#39; a été compilé avec la version R 4.0.5 ## Package version: 3.0.0 ## Unicode version: 13.0 ## ICU version: 69.1 ## Parallel computing: 4 of 4 threads used. ## See https://quanteda.io for tutorials and examples. library(quanteda.textstats) ## Warning: le package &#39;quanteda.textstats&#39; a été compilé avec la version R 4.0.5 library(udpipe) ## Warning: le package &#39;udpipe&#39; a été compilé avec la version R 4.0.5 library(tidytext) ## Warning: le package &#39;tidytext&#39; a été compilé avec la version R 4.0.5 library(cleanNLP) ## Warning: le package &#39;cleanNLP&#39; a été compilé avec la version R 4.0.5 #mise en page des tableau library(flextable) ## Warning: le package &#39;flextable&#39; a été compilé avec la version R 4.0.5 ## ## Attachement du package : &#39;flextable&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## compose #HREPR2SENTATION DE R2SEAUX library(igraph) ## Warning: le package &#39;igraph&#39; a été compilé avec la version R 4.0.5 ## ## Attachement du package : &#39;igraph&#39; ## The following object is masked from &#39;package:flextable&#39;: ## ## compose ## The following objects are masked from &#39;package:dplyr&#39;: ## ## as_data_frame, groups, union ## The following objects are masked from &#39;package:purrr&#39;: ## ## compose, simplify ## The following object is masked from &#39;package:tidyr&#39;: ## ## crossing ## The following object is masked from &#39;package:tibble&#39;: ## ## as_data_frame ## The following objects are masked from &#39;package:stats&#39;: ## ## decompose, spectrum ## The following object is masked from &#39;package:base&#39;: ## ## union library(ggraph) ## Warning: le package &#39;ggraph&#39; a été compilé avec la version R 4.0.5 #statistiques et modèles #ML theme_set(theme_bw()) Lensemble du code est disponible ici Quelques conventions décriture du code r On appele les dataframes de manière générale df, les tableaux intermédiaires sont appelé systématiquement foo et gestion des palettes de couleurs 1.4 A propos de bookdown The bookdown package can be installed from CRAN or Github: #install.packages(&quot;bookdown&quot;) # or the development version #devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],["intro.html", "Chapter 2 Introduction 2.1 Un nouveau champs méthodologique 2.2 Les facteurs de développement 2.3 Le projet de louvrage 2.4 Labelliser les figures et les tableaux", " Chapter 2 Introduction Le texte connaît une double révolution. la première est celle de son système de production, la seconde est celle de sa lecture. Il se produit désormais tant de textes que personne ne peut plus tous lire, même en réduisant son effort à sa sphère dintérêt et de compétence. La production primaire de texte se soumet ensuite à ceux qui en contrôlent les flux et en exploitent les contenus, qui les mettent en avant ou les écartent, définissant la composition de ce que chacun va lire. La révolution de la lecture est venue avec les moteurs de recherche, et les pratiques de curations (ref), cest une lecture sélectionnée et digérée par les moteurs de recommandation. (ref). Sil ne fallait quun exemple on pourrait évoquer la transformation radicale de la littérature dite scientifique sur le plan technique. La recherche par mots clés est complétée de plus en plus par des outils de veille, lindexation a donné naissance à limmatriculation de la moindre note, les fichiers ont adopté des standards, linteropérabilité est de mise, le réseau des co-citation est maintenu en temps réel. Les scores qualifient autant les articles que leurs auteurs et les revues qui les accueillent. 2.1 Un nouveau champs méthodologique Pour le chercheur qui étudie les organisations, cette révolution textuelle offre de nouvelles opportunités dobtenir et danalyser des données pour vérifier ses hypothèses. La production abondante davis de consommateurs, de discours de dirigeants, de compte-rendus de conseils, darticles techniques, rencontre une multiplication des techniques, provenant de la linguistique computationnelle, de la fouille de données, des moteurs de recommandation, de la traduction automatique, et des ressources nouvelles et précieuses pour traiter labondance des données. Un nouveau paradigme méthodologique se construit à la croisée de données abondantes et de techniques de traitement intelligentes. Il permet daller plus loin que lanalyse lexicale traditionnelle en incorporant des éléments syntaxiques, sémantiques, et pragmatiques, proposés par lensemble des outils des techniques de traitement du langage naturel. Il se dessine surtout une nouvelle approche méthodologique qui prend place entre lanalyse qualitative, et les traditionnelles enquêtes par questionnaires capables de traiter des corpus dune taille inédite. Le travail de (Humphreys et Wang 2018) en donne une première synthèse dans le cadre dun processus qui sarticule autour des différentes phases dune recherche : la formulation de la question de recherche, la définition des construits, la récolte des données, lopérationnalisation des construits, et enfin linterprétation, lanalyse et la validation des résultats obtenus. Dans le champ du management, on trouvera des synthèses pour la recherche en éthique (Lock et Seele 2015), en comportement du consommateur Humphreys and Wang (2018) en management public (Anastasopoulos, Moldogaziev, et Scott 2017) ou en organisation (Kobayashi et al. 2018). ( et ajouter en sociologie le texte recent de xxx) 2.2 Les facteurs de développement Ces développements sont favorisés par un environnement fertile dont trois facteurs se renforcent mutuellement. * Le premier est lexpansion de deux langages, proprement statistiques pour r et plus généraliste pour python. Le propre de ces langages est, prenons le cas de r, de permettre délaborer des fonctions, dont un ensemble cohérent pour réaliser certaines tâches peuvent être rassemblées dans une bibliothèque. Coder une analyse revient ainsi à jouer avec un immense jeu de lego, dont de nombreuses pièces sont déjà pré-assemblées. Dun point de vue pratique, les lignes décriture sont fortement simplifiées permettant à un chercheur sans compétence de codage deffectuer simplement des opérations complexes. Lautre conséquence est la croissance exponentielle du nombre de packages disponibles : près de 20 000 pour r. Dans le domaine de lanalyse du texte, on citera lancêtre tm, le sophistiqué quanteda,  cleanNLP, tidytext. La dimension langagière se traduit aussi dans lédition : le rôle du markdown et des carnets Jupyters1. De plus de nouvelles familles de techniques se généralisent et sont accessibles en open-source au travers de langages tels que python ou r, qui est privilégié dans ce chapitre. * Le second, intimement lié au premier, est la constitution dune large communauté de développeurs et dutilisateurs qui se retrouvent aujourdhui dans des plateformes de dépots (Github, Gitlab), darxiv, de plateformes de type quora (StalkOverFlow), de tutoriaux, de blogs (BloggeR) et de journaux (Journal of Statistical Software). Des ressources abondantes sont ainsi disponibles et facilitent la formation des chercheurs et des data scientists. Toutes les conditions sont réunies pour engendrer une effervescence créative. * Le troisième est la multiplication des sources de données et leur facilité daccès. Les données privées, et en particulier celles des réseaux sociaux, même si un péage doit être payé pour accéder aux APIs, popularisent le traitement de données massives. Le mouvement des données ouvertes (open data) proposent et facilitent laccès à des milliers de corps de données : retards de la SNCF, grand débat, le formidable travail de lInsee, european survey etc. 2.3 Le projet de louvrage Dans ce chapitre, on choisit de présenter les différentes facettes de ce qui sappelle TAl, NPL, Text Mining, dans une approche procédurale qui suit les principales étapes du traitement des données. On rendra compte à chaque étape des techniques disponibles, et on illustre dexemples. Nous suivrons ici une approche plus fidèle au processus de traitement des données, lequel peut connaître une stratégie inférentielle et exploratoire - quelles informations sont utiles au sein dun corpus de texte -, tout aussi bien quune stratégie hypothético-déductive. Nous resterons agnostiques sur cette question, restant délibérément à un niveau technique et procédural. 2.4 Labelliser les figures et les tableaux You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2021) in this sample book, which was built on top of R Markdown and knitr (???). References "],["constitution-du-corpus.html", "Chapter 3 Constitution du corpus 3.1 Méthodes de collecte 3.2 Un corpus reste un échantillon 3.3 Filtrage 3.4 références", " Chapter 3 Constitution du corpus La constitution dun corpus est la première étape dun projet NLP. Il se définit dabord par des méthodes de collecte, ensuite par des techniques déchantillonnage, et enfin par des techniques de filtrage. 3.1 Méthodes de collecte Un corpus est un ensemble de documents. Ils peuvent être courts, les tweets, paas trop long - abstract articles court - long ( article de recherche, ou très long ( livres). 3.1.1 Les techniques de recueil direct Ces document sont dabord obtenus directements les entretiens le processus de la transcription 3.1.1.1 La gestion dun ensemble de documents pdftool 3.1.2 Lexploitation de base de données textuelles Prenons lexemple de factiva https://github.com/koheiw/newspapers 3.1.3 Scrapping De ces deux approches on pourra considérer que la première correspond à un internet sauvage où la collecte dinformation se traduit par une technique de chasseurs-cueilleurs, le glanage. Le recours aux APIs est civilisé, ne serait-ce parce quon introduit une sorte détiquette, des règles de courtoisie, un système de reconnaissance réciproque et dattribution de droits. Le scraping est lactivité qui consiste à moissonner les informations disponibles sur le net en simulant et en automatisant la lecture par un butineur. Elle consiste à construire un robot capable de lire et denregistrer les informations disponibles sous forme html puis à les distribuer (parsing) dans des tableaux structurés, selon une stratégie dexploration du web préalablement définie. De nombreuses ressources sont disponibles, mais pour en rester à r , le package rvest permet de réaliser des extractions simples mais suffisantes pour de nombreux usages. Les caractéristiques clés du scraping : * La nécessité de programmer de manière ad hoc, en fonction des spécificités de chaque site. * Des stratégies mécaniques, en boule de neige. 3.1.3.1 Prendre avantage de la structure du html Le langage html est un langage à balise Les balises sont la cible du scrapping 3.1.3.2 des problèmes pratiques, juridiques et éthiques La pratique du scrapping se heurte à différents problèmes éthiques et juridique. Si elle nest pas interdite en tant que telle, elle se confronte à différents droits et principes éthiques En termes pratiques Le risque de deny of service, cest à dire de saturer ou de parasiter un système et de sexposer à ses contre-mesures. Le risque dinformation parcellaires, tronquées, inexactes qui résultent de ces contre-mesures. Les producteurs développent des stratégies moins naives. Lexemple des pages numérotée par ordre de production auxquels on substitue un nombre au hasard pour annihilier linformation temporelle. En termes de droits Les conditions légales ne sont pas homogènes et relèvent de différents droits : de la propriété intellectuelle, du respect de la vie privée, du droit de la concurrence. Cependant des facilités et tolérances sont souvent accordées quand cest dans un objectif de recherche et que des précautions minimales danonymisation ou de pseudonymisation sont prises, et que les règles de conservation et de destruction des données sont précisées. En termes éthiques Un principe éthique essentiel dans la recherche, et ailleurs, et de ne pas nuire à la socité dans son ensemble, hors cette technique participe à la robotisation du web (plus de 50% du trafic résulterait de la circulation des spiders , scrapers, sniffers et autres bots). Elle contribue à la complexification du web, et implique une consommation excessive de ressources energétiques. 3.1.3.3 rvest avec r le package rvest 3.1.4 API Les API doivent être considérées comme la voie normale daccès à linformation: les requêtes sont reproductibles au moins par les requêtes, les bases visée peuvent varier. Parmi les contre-mesures, le développement des APIs qui permet un accès contrôlé par le détenteur des informations en échange dune facilitation de laccès, et dune plus grande fiabilité des données. Lutilisation dAPI lève lambiguïté légale qui accompagne le scraping est peut ainsi paraître comme plus civilisée. Elle nécessite naturellement que le gestionnaire de la base de données fournissent les moyens de sidentifier et de requêter, elle peut avoir linconvénient dêtre coûteuse quand laccès est payant. Elle affirme aussi le pouvoir de ceux qui contrôlent les données. Ces dernières ne sont pas, insuffisamment pour le chercheur, un bien commun. 3.1.4.1 Un tour dhorizon des API La plus part des grande plateformes offrent des API plus ou moins ouvertes twitter est celle quon utilise dans cet ouvrage 3.1.4.2 Un exemple avec Rtwitter library (twittr) # on appelle la librairie twittr qui permet les requêtes consumerKey&lt;-Xq #paramètres requis par l API de twitter (Ouvrir un compte au prélable) consumerSecret&lt;-30l access_token&lt;-27A access_secret&lt;-zA7 fonction dinitialisation des requêtes setup_twitter_oauth(consumerKey, consumerSecret, access_token,access_secret) recherche des tweets tweets1 &lt;- searchTwitter(#IA, n = 2000, lang = fr, resultType = recent, since = 2019-08-01) transformer en data frame tweets_df1 &lt;- twListToDF(tweets1) 3.2 Un corpus reste un échantillon La collecte doit rester raisonnée. Les unités de texte Une unité de texte : un chaine de caractère intégrée dans un document. Celui ci peut être un livre un article, une note, une transcription, Un document Un ou des auteurs du document Une date Un endroit Un contexte : les unités précedente, et subséquentes. 3.3 Filtrage Dans un corpus massif il est souvent néçéssaire de filtrer le contenu déléments qui sortent du champs détudes. POur étudier lopinion inutile danalyser ce qui est produit par les bots ou les trolls. Il faut aussi réduire les contenus en éliminant les doublons 3.3.1 Eliminer les bots et les fakes botometer 3.3.2 Eliminer les doublons unique 3.4 références "],["préparation-des-données.html", "Chapter 4 Préparation des données 4.1 Identifier la langue 4.2 Nettoyer le texte 4.3 Corriger le texte", " Chapter 4 Préparation des données Avant de se lancer dans lanalyse, il est nécessaire de préparer le texte, de le pré-traiter. Il sagira dabord de savoir dans quelle langue il est exprimé, ensuite de simplifier le teste en enlevant linformation redondante 4.1 Identifier la langue Pour travailler la langue, il faut avant tout lidentifier. Nombreux sont les corpus multi-lingues. Les commentaire de airbnb par exemple. De manière générale les outils de traitement du langage nécessitent que lon détermine la langue. Le package textcat offre une solution basée sur la frénquence des ngrams. en voici une appplication sur le corpus airbnb. 4.2 Nettoyer le texte enlever les mentions enlever les url enlever ou recoder les emojis enlever la ponctuation enlever les nombres 4.3 Corriger le texte Dans un tiers des cas le mot \" opinion\" sorthographie opignons. Chaque mot du lexique sévanouit dans la langue vernaculaire dans des morphologies nombreuses. plusieurs stratégies sont possibles 4.3.1 La correction orthographique automatique voir hunspell https://cran.r-project.org/web/packages/hunspell/vignettes/intro.html#Custom_Dictionaries 4.3.2 Lusage des regex Une expression régulière est un masque qui permet didentifier des formes principales et leurs variétés. "],["une-première-analyse-quantitative.html", "Chapter 5 Une première analyse quantitative 5.1 Comptons les mots 5.2 la production dans le temps 5.3 Lisibilité et complexité lexicale", " Chapter 5 Une première analyse quantitative Chargeons le fichier de données. On en profite pour compter le nombre de post et de variables df &lt;- read_csv(&quot;TrumpTwitterArchive01-08-2021.csv&quot;) nrow&lt;-nrow(df) ncol&lt;-ncol(df) 5.1 Comptons les mots Il y 56571 tweets et 9 variables. On peut vouloir compter le nombre de mots. A cette fin on emploie une fonction de stringr. df$nb_mots&lt;-str_count(df$text, &quot; &quot;)+1 # l&#39;astuce : compter les espaces et ajouter 1, pour compter les mots sum_mots&lt;-sum(df$nb_mots) #ON COMPTE LE NOMBRE DE MOTS ggplot(df, aes(x=nb_mots))+ geom_histogram(fill=&quot;deepskyblue3&quot;)+ labs(title=paste0(&quot;Nombre total de mots du corpus : &quot;,sum_mots), x=&quot;Nombre de mots par post&quot;, y=&quot;Fréquence&quot;) Figure 5.1: Distribution du nombre de mots par post La bimodalité provient surement du changement de taille maximum effectué en setembre 2017. On peut le vérifier en examinant cette même distribution - par les courbes de densité - pour chacune des années, avec cette technique rendue fameuse par la pochette de lalbum de Joy Division : un graphique en crêtes (ridges plot) avec ggridges df$Year&lt;-format(df$date, format = &quot;%Y&quot;) #on extrait l&#39;année de la date ggplot(df,aes(x = nb_mots, y = Year, group = Year)) + geom_density_ridges(scale = 3, fill=&quot;peachpuff&quot;)+ theme_ridges() + scale_x_continuous(limits = c(1, 70), expand = c(0, 0)) + coord_cartesian(clip = &quot;off&quot;)+labs(x=&quot;Nombre de mots par post&quot;, y=NULL) Figure 5.2: Evolution de la distribution du nombre de mots 5.2 la production dans le temps Le résultat remarquable est que si Trump dans un premier temps exploite cette nouvel fonctionnalité, il en revient avec un phrasé de 20 mots en moyenne, gardant cependant à loccasion dautre contenu en 50 mots environ. Concluons en examiner le nombre de tweets produit au cours du temps ## plot time series of tweets ts_plot(df, &quot;1 month&quot;, color=&quot;darkblue&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + labs( x = NULL, y = &quot;Nombre de tweets par mois&quot;,title = &quot;Fréquence des posts twitters Donald Trump&quot;)+ scale_x_datetime(date_breaks = &quot;1 year&quot;, labels = scales::label_date_short()) Figure 5.3: Evolution de la production mensuelle des tweets de Trump #raf : labeliser avec les dates clés 5.3 Lisibilité et complexité lexicale Pour aller un peu plus loin, dans les comptages, introduisons deux quantifications utiles du texte : la lisibilité et la complexité lexicale. On gardera le principe dune analyse longitudinale https://fr.wikipedia.org/wiki/Covfefe 5.3.1 Les indices de lisibilité La lisibilité est une vieille notion autant que sa mesure (par exemple Coleman and Liau (1975)). Il sagit dévaluer la complexité dun texte à partir de deux critères : la complexité des mots capturée par le nombre moyen de syllabes par mot, et la complexité des phrases mesurée par le nombre de mots. Le nombre dindicateurs est considérable et le package compagnon de quanteda , quanteda.textstats , en fournit au moins une trentaine. On en utilise les ressources. readability&lt;-textstat_readability(df$text, measure = &quot;Flesch&quot;) df&lt;-cbind(df,readability[,2]) foo&lt;-df %&gt;% group_by(Year)%&gt;% summarize(Flesch=mean(Flesch, na.rm=TRUE)) ggplot(foo,aes(x=Year, y=Flesch, group=1))+geom_line(size=1.2, color=&quot;darkblue&quot;)+ labs(title = &quot;Evolution de la lisibilité des tweets de Trump&quot;, x=NULL, y=&quot;indice de Flesch&quot;) Figure 5.4: Evolution de la lisibilité des tweets de Trump 5.3.2 Les indices de complexité lexicale La complexité lexicale rend compte de la diversité du vocabulaire, elle consiste à rapporter le nombre de mot uniques sur le nombre total de mots. La difficulté est que la taille des corpus joue fortement sur cette mesure et que lorsque cette taille est hétérogène, lindicateur marque plus cette variété que les variations de complexité lexicales. Cest un pb auquel se sont attaché x et y Dans notre univers trumpesque, ce nest pas trop sensible, dautant plus que nous allons moyenner les tweets par période. Une manière plus fine sera de considérer chaque période comme un texte, References "],["tokenisation.html", "Chapter 6 Tokenisation 6.1 Le principe 6.2 les outils 6.3 Collocation", " Chapter 6 Tokenisation Létape intiale de toute analyse textuelle est de découper le texte en unités danalyse. Les tokenizers sont les outils indispensables à cette tâche. 6.1 Le principe découper un texte en unités des lettres des syllabes des mots des phrases des paragraphes des sections des chapitres des livres 6.2 les outils quanteda 6.3 Collocation Dans ce livres lunité principales danalyse restera le mot. Mais nous savons, au moins intuitivement que certaines combinaisons de mots repréentent des expressions qui ont la valeur dun mot, une valuer sémantique. Par exemple, lexpression \" Assemblée Nationale\". Ces deux mots réunis constitue un syntagme. Et une unité de sens. Comment les identifier dans le flot des caractères? La technique est simple : si deux morts se retrouvent dans un ordre donné plus pfrquemment que ce que le produit de leur probabilité dapparition laisse espérer, cest quil constitue une expression régulière. On peut imaginer faire un test du chi² pour décider si un couple de mots constitue une unité sémantique ou non. Le package quanteda p^ropose une bonne solution à ce problème avec la fonction collocation. "],["analyse-du-sentiment.html", "Chapter 7 Analyse du sentiment 7.1 Valence et expression 7.2 La généralisation par le liwc 7.3 Encore dautres généralisations 7.4 construire son propre dictionnaire", " Chapter 7 Analyse du sentiment liu ping est le fondateur 7.1 Valence et expression la linguistique donne aux mots une valence : elle peut être positive (bonheur), négative (malheur) ou neutre ( tranquité). Cest un régime ternaire. Chaque mot dune phrase est neutre, positif ou négatif. On peut doser les effets On a des dictionnaires 7.2 La généralisation par le liwc Le liwc vient de lidée simple dun psychiatre qui a souhaité faire des diagnoistics de trauma craniens à partir des entretiens menés avec lmes patients atteints. 80 dictionnaires permettent de saisirs des 7.3 Encore dautres généralisations Lapproche par dictionnaire sest déplacée vers lidentification dautre catégorie les valeurs morales 7.4 construire son propre dictionnaire faire des listes de lmots "],["annotations-lexicales-et-syntaxiques.html", "Chapter 8 Annotations lexicales et syntaxiques 8.1 Tokenization 8.2 Stemmatisation, lemmatisation et synonymisation 8.3 Part of Speech (POS) 8.4 Dépendances syntaxiques 8.5 reconnaissance dentités nommées 8.6 co-reférence", " Chapter 8 Annotations lexicales et syntaxiques Pour aller au-delà de lanalyse du seul lexique et de lanalyse de la cooccurence des termes à travers les textes, comme le font les méthodes de typologie et danalyse factorielle des correspondance depuis longtemps, il est néçessaire danalyser le texte en tenant compte de ses propriétés syntaxiques. Depuis une dizaine dannées, des outils puissants, les annotateurs, sont proposés de manière accessible. Les plus connus sont Spacy, Stanford NLP et désormais UDpipe. Dans lenvironnement r différentes ressources sont disponibles : Quanteda, clean_nlp, Udpipe,  Ils sont disponibles désormais dans de nombreuses langues même si la richesse et la précision obtenues varient dune langue à lautres Ils sappuient sur des corpus plus ou moins étendus et spécialisés dannotations manuelle : les Treebanks. Ils réalisent souvent plusieurs tâches dont les principales sont les suivantes : Tokeniser Lemmatiser Identifier les parts of speech Identifier les dépendances syntaxiques Identifier les entités nommées. identifier les co-reférence 8.1 Tokenization 8.1.1 Les niveaux de tokenisation Un token est une unité danalyse dont la granularité est plus ou moins fine Le paragraphe est sans lunité lunité la plus générale, quand un texte est correctement rédigé, un paragraphe développe une idée. La phrase est lunité de langage, lui correpond un argument, une proposition. Lusage du point suivi dun espace est assez général pour les identifier. Cest lobjet de tokenizers qui feront mieux en analysant le contexte de la phrase pour décider plus précisément si le point sépare bien deux phrase distinctes. Cette unité de phrase elle essentielle. Le mot est la fois le niveau le plus évident et le plus courant. On peut aussi souhaiter extraire dun mot les suffixe et préfixe On peut pour certains problème descendre au niveau de la syllabe et donc du phonème. La lettre reste lunité insécable. 8.1.2 Un exemple en tidytext 8.2 Stemmatisation, lemmatisation et synonymisation Les mots prennent des formes variées, il peut être intéressant dans certains cas de réduire cette variété et ne considérer que lidée des mots. Deux techniques sont disponibles 8.2.1 la stemmatisation cest un 8.2.2 la lemmatisation Un lemme est un mot racine, sans inflexions de genre, de nombre ou de conjugaison. Cest généralement celui quon trouve dans les dictionnaire. 8.2.3 Synonymisation le cas de wordnet et linvention des synset synonymes, antonymes, hipponyne, hyperonymes.. https://cran.r-project.org/web/packages/wordnet/vignettes/wordnet.pdf 8.3 Part of Speech (POS) Dans une phrase les mots non pas la même valeur. Certains sont des nombres propres, ils se réfèrent à ce que nous venons de voir, cest à dire des entitées nommées, dautres désignent des catégories dobjet. Ce sont les noms communs qui se rapportent à des catégories de choses. Un marteau - si jen avais un - peut être nimporte quel marteau, la masse qui casse la pierre, ou ce petit marteau qui me permet denfoncer un clou dans le cadre du tableau. Des typologies universelles ont été construites, elles recouvrent des typologies plus spécifiques à certaines langues. Les désinences du latin ont par exemple disparu du français. Cette forme est spécifiques au latin, on la retrouvera en allemand. La notion de morphosntaxique désigne présisément que les variations de formes des mots dépendent dune règle syntaxique. Prenons le verbe, et sa forme, être, dont la forme au passé simple est était. La forme des mots change, mais lidée reste. Une catégorisation en 17 éléments est proposée. En voici les éléments et les définitions Un petit exemple avec le package UDpipe. library(udpipe) fr &lt;- udpipe_download_model(language = &quot;french&quot;) udmodel_french &lt;- udpipe_load_model(file = &quot;french-gsd-ud-2.5-191206.udpipe&quot;) Citations &lt;- read_csv(&quot;Citations.csv&quot;) Flaubert&lt;-Citations %&gt;% filter(doc==1) UD &lt;- udpipe_annotate(udmodel_french, x=Flaubert$text) x &lt;- as.data.frame(UD) foo&lt;-x %&gt;% select(doc_id,paragraph_id, sentence_id, token_id,token,lemma,head_token_id, upos,feats)%&gt;%filter(sentence_id==1) flextable(foo) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-33fd2610{border-collapse:collapse;}.cl-33f23106{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-33f23107{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-33f23108{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-33f2a5e6{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-33f2a5e7{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-33f2a5e8{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-33f2a5e9{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-33f2a5ea{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-33f2a5eb{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} doc_idparagraph_idsentence_idtoken_idtokenlemmahead_token_iduposfeatsdoc1111Lele2DETDefinite=Def|Gender=Masc|Number=Sing|PronType=Artdoc1112lendemainlendemain9NOUNGender=Masc|Number=Singdoc1113futêtre9AUXMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Findoc1114,,6PUNCTdoc1115pourpour6ADPdoc1116EmmaEmma9PROPNdoc1117,,6PUNCTdoc1118uneun9DETDefinite=Ind|Gender=Fem|Number=Sing|PronType=Artdoc1119journéejournée0NOUNGender=Fem|Number=Singdoc11110funèbrefunèbre9ADJGender=Fem|Number=Singdoc11111..9PUNCT Les trois première colonnes identifient le document, les phrases et les mots. Des lemmes sont proposées. La colonne UPOS donne les part of Speech universel. 8.4 Dépendances syntaxiques Cest à Lucien Tesnière que lon doit lidée de la grammaire de la dépendance qui est au coeur du NLP moderne. Lidée est de déterminer au niveau de la phrase les relations entre ses termes de manière hierarchisée selon un principe de gouvernant à subordonné. Verdelhan-Bourgade (2020) résume son analyse de manière précise et concise : Tous les mots nont pas le même statut. Les mots pleins, qui « expriment directement la pensée » (p. 59), relèvent de quatre catégories structurales : les substantifs (notés par O), les adjectifs (A), les verbes (I), les adverbes (E). Les mots dits vides (souvent désigné de manière pratique par les stopwords aujourdhui) précisent le sens des autres, ou servent à marquer des relations.La connexion établit la relation entre mot régissant et mot subordonné. Lorsquun régissant commande un subordonné, cela constitue un nud, qui peut se faire à partir dune des quatre espèces de mots pleins. Il en donne lexemple suivant : « Très souvent mon vieil ami chante cette fort jolie chanson à ma fille » où lon peut repèrer: un nud verbal, central, qui commande des actants (ami, chanson, fille) et des circonstants (souvent). La valence est « le nombre de crochets par lesquels un verbe peut attraper des actants », à peu près équivalente à « voix ». les nud substantivaux (ami, chanson, fille), qui commandent des compléments (mon, vieil, cette, jolie, ma) le nud adjectival (jolie) qui commande ici le subordonné fort le nud adverbial, très étant subordonné à souvent. \" 8.4.1 Arbre syntaxique Larbre syntaxique est obtenue en analysant les relations entre les termes. Nous poursuivons avec UPpipe, lannovation précédente a déjà fait le travail. A chaque mot deux informations sont associée : la première est lindex du mot auxquel il se rapporte, la seconde est la nature de la relation. Onn utilise ici une fonction écrite par (bnosac](http://www.bnosac.be/index.php/blog/93-dependency-parsing-with-udpipe) pour donner une représentation graphique de larbre. plot_annotation &lt;- function(x, size = 3){ stopifnot(is.data.frame(x) &amp; all(c(&quot;doc_id&quot;,&quot;paragraph_id&quot;, &quot;sentence_id&quot;, &quot;token_id&quot;,&quot;token&quot;,&quot;lemma&quot;,&quot;head_token_id&quot;, &quot;upos&quot;,&quot;feats&quot;, &quot;dep_rel&quot;) %in% colnames(x))) x &lt;- x[!is.na(x$head_token_id), ] x &lt;- x[x$sentence_id %in% min(x$sentence_id), ] edges &lt;- x[x$head_token_id != 0, c(&quot;token_id&quot;, &quot;head_token_id&quot;, &quot;dep_rel&quot;)] edges$label &lt;- edges$dep_rel g &lt;- graph_from_data_frame(edges, vertices = x[, c(&quot;token_id&quot;, &quot;token&quot;, &quot;lemma&quot;, &quot;upos&quot;, &quot;xpos&quot;, &quot;feats&quot;)], directed = TRUE) ggraph(g, layout = &quot;linear&quot;) + geom_edge_arc(ggplot2::aes(label = dep_rel, vjust = -0.20), arrow = grid::arrow(length = unit(4, &#39;mm&#39;), ends = &quot;last&quot;, type = &quot;closed&quot;), end_cap = ggraph::label_rect(&quot;wordswordswords&quot;), label_colour = &quot;red&quot;, check_overlap = TRUE, label_size = size) + geom_node_label(ggplot2::aes(label = token), col = &quot;darkgreen&quot;, size = size, fontface = &quot;bold&quot;) + geom_node_text(ggplot2::aes(label = upos), nudge_y = -0.35, size = size) + labs(title = &quot;Tokenisation, PoS &amp; dependency relations&quot;) } plot_annotation(x, size = 3) Figure 8.1: arbre de dépendance 8.4.2 Vers des application plus générale Dans la phrase precédente on note que funèbre est ladjectif de journée. On peut être tenté de retrouver ces relations qui caractérisent des choses (les nouns ou noms choses) à des adjectifs. On souhaite faire une liste de ces paires. Lexemple va être court : un poème de Maupassant Maupassant&lt;-Citations %&gt;% dplyr::filter(doc==2) UD &lt;- udpipe_annotate(udmodel_french, x=Maupassant$text) foo &lt;- as.data.frame(UD) foo&lt;- foo %&gt;% select(paragraph_id,sentence_id, token_id,lemma,upos,head_token_id,dep_rel)%&gt;% mutate(key1=paste0(paragraph_id,sentence_id, token_id),key2=paste0(paragraph_id,sentence_id, head_token_id) ) On va donc construires un tableau lemme_cible x lemmes_associés, les premiers risqueront dêtres les noms communs, les seconds leurs adjectifs. # selection de la relation. res &lt;- foo %&gt;% filter(dep_rel == &quot;amod&quot;) #on y joint les dependences dep&lt;-res %&gt;% left_join(foo, by = c(&quot;key2&quot; = &quot;key1&quot;)) #on construit la tables des relations lemmes cibles -adjectifs table&lt;-as.data.frame.matrix(table(dep$lemma.x, dep$lemma.y)) #table$n&lt;-rowSums(table) #table$adj&lt;-rownames(table) #row.names(table) &lt;- table$adj le tableau obtenu est en fait la structure dun graphe bipartite. la représentation passe par un de igraph avec pour paramètres importants : * Taille des arcs (edge) : est proportionnelle à la force du lien ( nombre de relations) * Taille des noeud : proportiennel au rangs du noeud. * Couleur et forme des noeuds : lemme et lemme cible. * Un algorithme de force de Fruchterman and Reingold (1991) est employé pour représenter les positions relatives des mots et minimiser les superpositions. Dessiner le réseau bg &lt;-graph_from_incidence_matrix(table, weighted=TRUE) summary(bg) ## IGRAPH 349d806 UNWB 36 20 -- ## + attr: type (v/l), name (v/c), weight (e/n) #E(bg)$weight# See the vertex attributes #V(bg)$type #V(bg)$name # Plot the network shape = ifelse(V(bg)$type, &quot;circle&quot;, &quot;square&quot;) # assign shape by node type col = ifelse(V(bg)$type, &quot;peachpuff&quot;, &quot;darkolivegreen1&quot;) # assign color by node type plot(bg, vertex.shape = shape, vertex.label.cex=.9,vertex.label.color=&quot;black&quot;,vertex.color = col,edge.color=&quot;azure2&quot;,vertex.frame.color=col,vertex.label.family=&quot;TT Arial&quot;, vertex.size=0.5*igraph::degree(bg),layout=layout_with_fr,edge.width=1*E(bg)$weight,edge.curved=0.5) 8.5 reconnaissance dentités nommées En français courant les entités nommées correspondent largement à lidée de noms propres. Un nom propre à une entité. Une chose qui est est indépendemment des catégories qui peuvent létiqueter. John Dupont, né le 19 février 1898 à Glasgow et abattu à Verdun le 8 août 1917, est un personnage unique. John Dupont ne désigne par une catégorie, mais bien une personne singulière. La designation peut cependant être ambigüe, il y a un Paris, Texas., et un Paris sur Seine. La morphologie ne ressout pas lambiguité. les entités nommées appartiennent à différentes catégories dobjets : des noms de lieux, des noms de personnes, des noms de marques, des acronymes dorganisation, Elles ne représentent jamais une catégorie mais une unité singulière. https://cran.r-project.org/web/packages/nametagger/nametagger.pdf 8.6 co-reférence References "],["constitution-du-corpus-1.html", "Chapter 9 Constitution du corpus 9.1 Scrapping 9.2 API", " Chapter 9 Constitution du corpus Some significant applications are demonstrated in this chapter. 9.1 Scrapping De ces deux approches on pourra considérer que la première correspond à un internet sauvage où la collecte dinformation se traduit par une technique de chasseurs-cueilleurs, le glanage. Le recours aux APIs est civilisé, ne serait-ce parce quon introduit une sorte détiquette, des règles de courtoisie, un système de reconnaissance réciproque et dattribution de droits. Le scraping est lactivité qui consiste à moissonner les informations disponibles sur le net en simulant et en automatisant la lecture par un butineur. Elle consiste à construire un robot capable de lire et denregistrer les informations disponibles sous forme html puis à les distribuer (parsing) dans des tableaux structurés, selon une stratégie dexploration du web préalablement définie. De nombreuses ressources sont disponibles, mais pour en rester à r , le package rvest permet de réaliser des extractions simples mais suffisantes pour de nombreux usages. Les caractéristiques clés du scraping :  La nécessité de programmer de manière ad hoc, en fonction des spécificités de chaque site.  Des stratégies mécaniques, en boules de neige.  Le risque de deny of service, cest à dire de saturer ou de parasiter un système et de sexposer à ses contre-mesures.  Les conditions légales ne sont pas homogènes et relèvent de différents droits : de la propriété intellectuelle, du respect de la vie privée, du droit de la concurrence. Cependant des facilités et tolérances sont souvent accordées quand cest dans un objectif de recherche et que des précautions minimales danonymisation ou de pseudonymisation sont prises, et que les règles de conservation et de destruction des données sont précisées.  La question éthique va au-delà du droit, elle concerne les conséquences de cette action sur lévolution densemble. On notera quelle participe à la robotisation du web ( plus de 50% du trafic résulterait de la circulation des spiders , scrapers2, sniffers et autres bots. Et quelle fait lobjet de contre-mesures. Parmi les contre-mesures, le développement des APIs qui permet un accès contrôlé par le détenteur des informations en échange dune facilitation de laccès, et dune plus grande fiabilité des données. Lutilisation dAPI lève lambiguïté légale qui accompagne le scraping est peut ainsi paraître comme plus civilisée. Elle nécessite naturellement que le gestionnaire de la base de données fournissent les moyens de sidentifier et de requêter, elle peut avoir linconvénient dêtre coûteuse quand laccès est payant. Elle affirme aussi le pouvoir de ceux qui contrôlent les données. Ces dernières ne sont pas, insuffisamment pour le chercheur, un bien commun. 9.1.1 Prendre avantage de la structure du html 9.1.2 des problèmes de droits et déthique 9.1.3 rvest avec r 9.2 API 9.2.1 Un tour dhorizon des API 9.2.2 Un exemple avec Rtwitter library (twittr) # on appelle la librairie twittr qui permet les requêtes consumerKey&lt;-Xq #paramètres requis par l API de twitter (Ouvrir un compte au prélable) consumerSecret&lt;-30l access_token&lt;-27A access_secret&lt;-zA7 fonction dinitialisation des requêtes setup_twitter_oauth(consumerKey, consumerSecret, access_token,access_secret) recherche des tweets tweets1 &lt;- searchTwitter(#IA, n = 2000, lang = fr, resultType = recent, since = 2019-08-01) transformer en data frame tweets_df1 &lt;- twListToDF(tweets1) "],["vectorisation-du-corpus.html", "Chapter 10 Vectorisation du corpus 10.1 Word2vec 10.2 paragraph2vec 10.3 lavenir des modèles pré-entrainés", " Chapter 10 Vectorisation du corpus 10.1 Word2vec 10.1.1 vectorisation pipe 10.1.2 closest2 10.1.3 map with tsne 10.2 paragraph2vec 10.3 lavenir des modèles pré-entrainés "],["topic-analysis.html", "Chapter 11 Topic Analysis 11.1 LDA 11.2 STM", " Chapter 11 Topic Analysis 11.1 LDA 11.1.1 le modèle de blei 11.1.2 implementation avec wor2vec 11.1.3 Représentation graphique 11.1.4 La 11.2 STM 11.2.1 11.2.2 "],["machine-learning-with-text.html", "Chapter 12 Machine learning with text 12.1 simples models 12.2 art of featuring", " Chapter 12 Machine learning with text 12.1 simples models 12.1.1 Naives bayes 12.1.2 elastic net 12.1.3 RF 12.2 art of featuring utiliser les plongements "],["machine-learning-with-text-1.html", "Chapter 13 Machine learning with text 13.1 simples models 13.2 art of featuring", " Chapter 13 Machine learning with text 13.1 simples models 13.1.1 Naives bayes 13.1.2 elastic net 13.1.3 RF 13.2 art of featuring utiliser les plongements "],["machine-learning-with-text-2.html", "Chapter 14 Machine learning with text 14.1 simples models 14.2 art of featuring", " Chapter 14 Machine learning with text 14.1 simples models 14.1.1 Naives bayes 14.1.2 elastic net 14.1.3 RF 14.2 art of featuring utiliser les plongements "],["machine-learning-with-text-3.html", "Chapter 15 Machine learning with text 15.1 simples models 15.2 art of featuring", " Chapter 15 Machine learning with text 15.1 simples models 15.1.1 Naives bayes 15.1.2 elastic net 15.1.3 RF 15.2 art of featuring utiliser les plongements "],["references.html", "References", " References "]]
