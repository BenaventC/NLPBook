[["index.html", "NLP avec r et en français - un Manuel synthétique Chapitre 1 Préface", " NLP avec r et en français - un Manuel synthétique Sophie Balech et Christophe Benavent et al 2021-07-10 Chapitre 1 Préface 1 Leco système r sest enrichi ces dernière année à grande vitesse dans le domaine du traitement du langage naturels, lobjet de ce manuel a pour but de donnner une synthèses des ressources de r en matière de NLP. Sa vocation est pratique. La bibliographie synthétique en fait un essai détat de lart essentiel. On ouvrira cependant chaque fois que cest possibles aux questions théoriques et éthiques de ces méthodes. Leur réalisation computationnelle est le fruit souvent dune longue histoire, au cours de laquelle les linguistes ont semé des idées essentielles quont systématisés les informaticiens. Il a pour but dêtre reproductible, cest pourquoi le choix de ce support et des jeux de données associés. Il sera dynamique, modifié à mesure de nos cours, séminaires et ateliers. Incantation for 6 voices Scott helmes, 2001. Museum of Minessota "],["plan-et-outils-du-manuel.html", "Chapitre 2 Plan et outils du manuel 2.1 La structure du livre 2.2 Les jeux de données 2.3 Le cadre technique 2.4 A faire", " Chapitre 2 Plan et outils du manuel 2.1 La structure du livre L analyse NLP peut être analysée comme un processus qui va de la production jusqu à la diffusion des analyses. Elle est aussi traversée par des évolutions profondes de méthodes qui ont complexifié au sens formels les modèles initiaux. Lapprentissage submerge le comptage,et les catégorisations. Rappelons nous que les modèles de langages désormais distribués par les grands acteurs, comprennent des dizaines, voir des centaines de milliards de paramètres. Le plan suit une logique du simple au compliqué, de lacquition au traitement. acquisition des données : directe, api et scrapping corpus dtm et cooccurence AFC et typologie lannotation syntaxique et lexicale analyse du sentiment et sa généralisation word embedness factorial models Topic analysis ML deep learning translation génératives 2.2 Les jeux de données Au cours du développement, plusieurs cas pratiques - souvent réduit en volume pour rester exemplaires, seront employés. Les donées seront partagées. En voici la présentation systématique. Trump Twitter Archive : Lintégralité des tweets de Trump jusquà son banissement le 8 Janvier 2021. Confinement Jour J Citations : un recueil de citations littéraires pour de petits exemples et ponctuer le texte aride dun peu de littérature et de poésie. Trip advisor Polynésie, un extrait dun corpus établi par Pierre Ghewy et Sebastien de lUPF Airbnb Covid disponibles dans le repositery avec le code du book. Les amendements et améliorations sont souhaitées et attendues. 2.3 Le cadre technique Ce livre est écrit en Markdown (Allaire et al. 2021) et avec le package Bookdown (Xie 2021) Le code sappuie très largement sur tidyverse et emploie largement les ressources de ggplot . Les packages seront introduits au fur et à mesure. En voici la liste complète. knitr::opts_chunk$set(echo = TRUE, message=FALSE,warning=FALSE) #boite à outils et viz library(tidyverse) # inclut ggplot pour la viz, readr et library(cowplot) #pour créer des graphiques composés library(ggridges) # le joy division touch library(citr) #networks library(igraph) library(ggraph) # Accéder aux données library(rtweet) # une interface efficace pour interroger l&#39;api de Twitter # NLP library(tokenizers) library(quanteda) library(quanteda.textstats) library(udpipe) #annotation syntaxique library(tidytext) library(cleanNLP) #annotation syntaxique #sentiment library(syuzhet) #analyse du sentimeent #mise en page des tableaux library(flextable) #statistiques et modèles library(lme4) library(jtools) library(interactions) #ML library(caret) #graphismes theme_set(theme_bw()) #palettes library(colorspace) #pour les couleurs Lensemble du code est disponible sur github. A ce stade cest encore très embryonnaire. Les proches pourrons cependant y voir lévolution du projet et de la progression Quelques conventions décriture du code r On appele les dataframes de manière générale df, les tableaux intermédiaires sont appelé systématiquement foo Gestion des palettes de couleurs ** une couleur :\" royalblue\" ** deux couleurs ** 3 à 7 couleurs On emploie autant que possible le dialecte tidy. Les chunks sont notés X, le chapitre, 01 à n, les jeux. 502 est le second chunk du chapitre 4. On commente au maximum les lignes de code pour épargner le corps du texte et le rendre lisible 2.4 A faire todo list : insérer un compteur google analytics ( voir https://stackoverflow.com/questions/41376989/how-to-include-google-analytics-in-an-rmarkdown-generated-github-page) modifier le titre en haut à gauche vérifier le système de références voir ( https://doc.isara.fr/tuto-zothero-5-bibtex-rmarkdown-zotero/) Vérifier la publication en pdf References "],["intro.html", "Chapitre 3 Introduction 3.1 Un nouveau champ méthodologique 3.2 Les facteurs de développement 3.3 Des racines anciennes", " Chapitre 3 Introduction Le texte connaît une double révolution. la première est celle de son système de production, il se produit désormais tant de textes que personne ne peut plus tous les lire, même en réduisant son effort à sa propre sphère dintérêt et de compétence, la seconde est celle de sa lecture, cest une lecture conditionnée et recommandée.. La production primaire de texte voit son volume croître exponentiellement. Prenons quelques exemples : le contenu écrit des réseaux sociaux les rapports dactivités des entreprises, les compte-rendu archivé de réunion Les avis des consommateurs sur les catalogues de produit Les articles et les revues scientifiques Même les livres La production se soumet ensuite à ceux qui en contrôlent les flux et en exploitent les contenus, qui les mettent en avant ou les écartent, définissant la composition de ce que chacun va lire. La diffusion de cette production suit des loi puissances, cest ainsi que la révolution de la lecture est venue avec les moteurs de recherche, et les pratiques de curations (ref), cest une lecture sélectionnée et digérée par les moteurs de recommandation. (ref). Sil ne fallait quun exemple on pourrait évoquer la transformation radicale de la littérature dite scientifique sur le plan technique. La recherche par mots clés est complétée de plus en plus par des outils de veille, lindexation a donné naissance à limmatriculation de la moindre note, les fichiers ont adopté des standards, linteropérabilité est de mise, le réseau des co-citation est maintenu en temps réel. Les scores qualifient autant les articles que leurs auteurs et les revues qui les accueillent. Elle risque de ce poursuivre par la production de résumé par exemple, la transcription automatique (speech2tex) etc. Le NLP est au coeur de ces technologies, il se nourrit de plus en plus dintelligence artificielle. Mais le NLP est aussi une nouvelle ressource pour les chercheurs en sciences sociales à la fois par les matériaux empiriquement, lemballement de la production de texte génère une nouvelle matière détude pour le sociologues, le gestionnaire, léconomiste, le psychologue pour névoquer que quelques disciplines, mais aussi de nouvellle techniques danalyses. 3.1 Un nouveau champ méthodologique Pour le chercheur qui étudie les organisations, cette révolution textuelle offre de nouvelles opportunités dobtenir et danalyser des données pour vérifier ses hypothèses. La production abondante davis de consommateurs, de discours de dirigeants, de compte-rendus de conseils, darticles techniques, rencontre une multiplication des techniques, provenant de la linguistique computationnelle, de la fouille de données, des moteurs de recommandation, de la traduction automatique, et des ressources nouvelles et précieuses pour traiter labondance des données. Un nouveau paradigme méthodologique se construit à la croisée de données abondantes et de techniques de traitement intelligentes. Il permet daller plus loin que lanalyse lexicale traditionnelle en incorporant des éléments syntaxiques, sémantiques, et pragmatiques, proposés par lensemble des outils des techniques de traitement du langage naturel. Il se dessine surtout une nouvelle approche méthodologique qui prend place entre lanalyse qualitative, et les traditionnelles enquêtes par questionnaires capables de traiter des corpus dune taille inédite. Le travail de (Humphreys and Wang 2018) en donne une première synthèse dans le cadre dun processus qui sarticule autour des différentes phases dune recherche : la formulation de la question de recherche, la définition des construits, la récolte des données, lopérationnalisation des construits, et enfin linterprétation, lanalyse et la validation des résultats obtenus. Dans le champ du management, on trouvera des synthèses pour la recherche en éthique (Lock and Seele 2015), en comportement du consommateur Humphreys and Wang (2018) en management public (Anastasopoulos, Moldogaziev, and Scott 2017) ou en organisation (Kobayashi et al. 2018) , en sociologie (Kozlowski, Taddy, and Evans 2019). 3.2 Les facteurs de développement Ces développements sont favorisés par un environnement fertile dont trois facteurs se renforcent mutuellement. 3.2.1 Une lingua franca Le premier est lexpansion de deux langages, proprement statistique pour r et plus généraliste pour Python. Le propre de ces langages est, prenons le cas de r, de permettre délaborer des fonctions, dont un ensemble cohérent pour réaliser certaines tâches peut être rassemblé dans une bibliothèque appelée package (et chargé par library(nomdupackage)). On dispose désormais de milliers de packages (17 788 sur le CRAN) destinés à résoudre un nombre incalculable de tâches. hornik Coder une analyse revient ainsi à jouer avec un immense jeu de lego, dont de nombreuses pièces sont déjà pré-assemblées. Dun point de vue pratique, les lignes décriture sont fortement simplifiées permettant à un chercheur sans grande compétence de codage deffectuer simplement des opérations complexes. En retour, cette facilitation de lanalyse abonde le stock de solutions. 3.2.2 Une communauté Le second facteur , intimement lié au premier, est la constitution dune large communauté de développeurs et dutilisateurs qui se retrouvent aujourdhui dans des plateformes de dépots (Github, Gitlab), de plateformes de type quora (StalkOverFlow), de tutoriaux, de blogs (BloggeR), de journaux (Journal of Statistical Software) et de bookdown. Des ressources abondantes sont ainsi disponibles et facilitent la formation des chercheurs et des data scientists. Toutes les conditions sont réunies pour engendrer une effervescence créative. 3.2.3 La multiplication des sources de données. Le troisième est la multiplication des sources de données et leur facilité daccès. Les données privées, et en particulier celles des réseaux sociaux, même si un péage doit être payé pour accéder aux APIs, popularisent le traitement de données massives. Le mouvement des données ouvertes (open data) proposent et facilitent laccès à des milliers de corps de données : retards de la SNCF, grand débat, le formidable travail de lInsee, european survey etc. 3.3 Des racines anciennes Si dans ce manuel, on choisit de présenter les différentes facettes de ce qui sappelle TAl, NPL, Text Mining, dans une approche procédurale qui suit les principales étapes du traitement des données. On rendra compte à chaque étape des techniques disponibles, et on illustre dexemples. Nous suivrons ici une approche plus fidèle au processus de traitement des données, lequel peut connaître une stratégie inférentielle et exploratoire - quelles informations sont utiles au sein dun corpus de texte -, tout aussi bien quune stratégie hypothético-déductive. Nous resterons agnostique sur cette question, restant délibérément à un niveau technique et procédural. Cependant on se doit pas se faire aveugler par léclat de la nouveautés, les techniques daujourdhui dépendent didées semée depuis longtemps dans au moins deux champs disciplinaire la convergence de deux grands mouvements. Sans en faire lhistoire minutieuse que ce domaine réclame, nous pouvons au moins rappeler un certains nombre détapes clés. 3.3.1 Linguistique Tesnière et les dépendances syntaxiques Zipf et fisth pour lapproche distributionnnelle. Chomsky et la grammaire générative 3.3.2 Informatique ngram et théorie du signal information retrieval 3.3.3 la psychologie Très tôt la psychogie sest intéressée au langage, pas seulement comme produit des processus psychologique, mais comme expression de ceux-ci. Une partie du domaine du nlp se dénomme linguistique psycho computationnelle. Dans le champs de la psychologie de léducation et avec une forte motivation scientiste, dès les années 60 sest posée la question de la mesure de la difficulté dun texte pour un niveau déducation donné. La mesure de la lisibilité des texte sest développée profitant à dautre secteurs tels que ceux de la propagande. Dans cette même êrspective la richesse lexicographique comme représenant les compétences a a son tour développé de nouvelles instrumentation. James W. Pennebaker a développé son approche à partir de létude des traumas; donnant une grande importance à la production discursive des patients. Sa contribution majeure est létablissement dun ensemble de dictionnaires destinés à mesurer des caractéristiques du discours. Un instrument quon présentatera dans les chapitre 7 ( à vérifier). Auteur en 2011 dun livre sintéresant à lusage des petits mots. Son approche se poursuit en psychiatrie avec lanalyse des troubles du langage, et a connu des coup déclat avec la demonstration que lanalyse des messages sur les réseaux sociaux comme facebook permet de detecter des risques de depression. 3.3.4 Dautres champs aussi la biologie et en particulier la génétique la finance et lanalyse du sentiment La sociologie. Peut emploie lanalyse du langage. On citera cependant jean-baptiste Coulmont et son obstinatio à étudier les entités nommées, prénoms et autres marqueurs culturels de lidentité et des classes. References "],["constitution-du-corpus.html", "Chapitre 4 Constitution du corpus 4.1 Méthodes de collecte 4.2 La gestion des documents", " Chapitre 4 Constitution du corpus La constitution dun corpus est la première étape dun projet NLP. Il se définit dabord par des méthodes de collecte, ensuite par des techniques déchantillonnage, et enfin par des techniques de filtrage. 4.1 Méthodes de collecte Un corpus est un ensemble de documents. Ils peuvent être courts, les tweets, pas trop long - abstract articles court - long ( article de recherche, ou très long (livres). La collecte peut se faire dabord sur des matiériaux primaire, numérisé sous forme dimages, et dans lesquels en analusant les pixels on peut reconnaitre un texte. 4.1.1 la numérisation Dimmenses archives sont numérisées, quand du texte est dans les images il faut le détecter lOcr a fait dimmense progrès et prédit avec un erreur du demi pourt cent de qui est vrai. Le traitement des adresses est ainsi lobsession de nombreux métiers depuis quarante ans : les banques , la vad par exemple. Le problème matériel est que si lidéal est que les les scripteurs suivent des conventions indicatives comme la suivante : Modèle de rédaction correcte dune adresse postale La réalité ressemble souvent 4.1.2 Les techniques de recueil direct La tradition méthodologique de la sociologie est celle de lentretien, avec toute sorte dacteur. Elle aboutit à la production de transcriptions, plus ou moins détaillées et précise. Mais des textes On peut désormais enregistré des réaction par des interfaces vocales. le speech to text est de plus en plus effiace 4.1.3 Lexploitation de base de données textuelles Prenons lexemple de factiva https://github.com/koheiw/newspapers 4.1.4 Scrapping De ces deux approches on pourra considérer que la première correspond à un internet sauvage où la collecte dinformation se traduit par une technique de chasseurs-cueilleurs, le glanage. Le recours aux APIs est civilisé, ne serait-ce parce quon introduit une sorte détiquette, des règles de courtoisie, un système de reconnaissance réciproque et dattribution de droits. Le scraping est lactivité qui consiste à moissonner les informations disponibles sur le net en simulant et en automatisant la lecture par un butineur. Elle consiste à construire un robot capable de lire et denregistrer les informations disponibles sous forme html puis à les distribuer (parsing) dans des tableaux structurés, selon une stratégie dexploration du web préalablement définie. De nombreuses ressources sont disponibles, mais pour en rester à r , le package rvest permet de réaliser des extractions simples mais suffisantes pour de nombreux usages. Les caractéristiques clés du scraping : * La nécessité de programmer de manière ad hoc, en fonction des spécificités de chaque site. * Des stratégies mécaniques, en boule de neige. 4.1.4.1 Prendre avantage de la structure du html Le langage html est un langage à balise Les balises sont la cible du scrapping une application rvest https://www.r-bloggers.com/2018/10/first-release-and-update-dates-of-r-packages-statistics/ 4.1.4.2 des problèmes pratiques, juridiques et éthiques La pratique du scrapping se heurte à différents problèmes éthiques et juridique. Si elle nest pas interdite en tant que telle, elle se confronte à différents droits et principes éthiques En termes pratiques Le risque de deny of service, cest à dire de saturer ou de parasiter un système et de sexposer à ses contre-mesures. Le risque dinformation parcellaires, tronquées, inexactes qui résultent de ces contre-mesures. Les producteurs développent des stratégies moins naives. Lexemple des pages numérotée par ordre de production auxquels on substitue un nombre au hasard pour annihilier linformation temporelle. En termes de droits Les conditions légales ne sont pas homogènes et relèvent de différents droits : de la propriété intellectuelle, du respect de la vie privée, du droit de la concurrence. Cependant des facilités et tolérances sont souvent accordées quand cest dans un objectif de recherche et que des précautions minimales danonymisation ou de pseudonymisation sont prises, et que les règles de conservation et de destruction des données sont précisées. En termes éthiques Un principe éthique essentiel dans la recherche, et ailleurs, et de ne pas nuire à la socité dans son ensemble, hors cette technique participe à la robotisation du web (plus de 50% du trafic résulterait de la circulation des spiders , scrapers, sniffers et autres bots). Elle contribue à la complexification du web, et implique une consommation excessive de ressources energétiques. 4.1.4.3 rvest avec r le package rvest est générique 4.1.5 les API Les API doivent être considérées comme la voie normale daccès à linformation: les requêtes sont reproductibles au moins par les requêtes, les bases visée peuvent varier. Parmi les contre-mesures, le développement des APIs qui permet un accès contrôlé par le détenteur des informations en échange dune facilitation de laccès, et dune plus grande fiabilité des données. Lutilisation dAPI lève lambiguïté légale qui accompagne le scraping est peut ainsi paraître comme plus civilisée. Elle nécessite naturellement que le gestionnaire de la base de données fournissent les moyens de sidentifier et de requêter, elle peut avoir linconvénient dêtre coûteuse quand laccès est payant. Elle affirme aussi le pouvoir de ceux qui contrôlent les données. Ces dernières ne sont pas, insuffisamment pour le chercheur, un bien commun. 4.1.5.1 Un tour dhorizon des API La plus part des grande plateformes offrent des API plus ou moins ouvertes twitter est celle quon utilise dans cet ouvrage facebook crunchbase 4.1.5.2 Un exemple avec Rtwitter Plusieur package sur r permettent dinterroger le fire nose, cest àdire le sytème dacces que twitter a organisé. library (twittr) # on appelle la librairie twittr qui permet les requêtes consumerKey&lt;-Xq #paramètres requis par l API de twitter (Ouvrir un compte au prélable) consumerSecret&lt;-30l access_token&lt;-27A access_secret&lt;-zA7 fonction dinitialisation des requêtes setup_twitter_oauth(consumerKey, consumerSecret, access_token,access_secret) recherche des tweets tweets1 &lt;- searchTwitter(#IA, n = 2000, lang = fr, resultType = recent, since = 2019-08-01) transformer en data frame tweets_df1 &lt;- twListToDF(tweets1) 4.2 La gestion des documents 4.2.1 Collections et tableaux pdftool 4.2.2 Un corpus reste un échantillon La collecte doit rester raisonnée. Les unités de texte Une unité de texte : un chaine de caractère intégrée dans un document. Celui ci peut être un livre un article, une note, une transcription, Un document Un ou des auteurs du document Une date Un endroit Un contexte : les unités précedente, et subséquentes. 4.2.3 Filtrage Dans un corpus massif il est souvent néçéssaire de filtrer le contenu déléments qui sortent du champs détude. POur étudier lopinion utile écarter danalyser ce qui est produit par les bots ou les trolls. Il faut aussi réduire les contenus en éliminant les doublons 4.2.4 Eliminer les bots et les fakes botometer 4.2.5 Eliminer les doublons unique "],["préparation-des-données.html", "Chapitre 5 Préparation des données 5.1 Manipuler des chaines de caractères 5.2 Nettoyer le texte 5.3 Corriger le texte 5.4 Identifier les sources", " Chapitre 5 Préparation des données Avant de se lancer dans lanalyse, il est nécessaire de préparer le texte, de le pré-traiter. Son format fondamental est celui dune chaine de caractères, sans signification particulière mais composé à partir dun alphbat, cun jeux de signes déterminés et démobralement. 1/0 pour lelangage bianre, AGCP pour ladan, 26 caractère de base pour lalphabet, sans compter les accents. Ces variations sont lobjet de convention en informatique. et de certaines opérations. traiter du texte cest avant tout disposer dopérateurs pour manipuler ces éléments élémentaire. la base est davoir des outils pour les manipuler. Le langage avant dêtre signifiant est signifié, littéralement produit comme une chaine de signes qui dans lusage suit certaine convention. Par exemple la satisfaction peut sexprimé par mmmm, une forte satisfaction par un mmmmmmmmmmmmmmmmmmmmm. Pour distinguer les significations, il faut dabord compter. les mmm sont sans doute courants car conventionnels (ce mot est à deux doigts dêtre incorporé au dictionnaire de lAcadémie Française, sil nétait quune onomatopée), les mmmmmmmmmmmmmmmmmmmmm sont sans doute beaucoups plus rares. De plus on trouvera des hum des hummm, des mmmmhummmm\". On comprend quà la nuance de lintensité que le locuteur veut exprimer, toute ces morphologies se rapportent à une même idée. Comment le rammener à une même formes est une question essentielle même si elle semble excessivement technique. 5.1 Manipuler des chaines de caractères Il faut donc traiter le texte, avant même de sengager dans des modèles compliqués. Il faut savoir traiter des chaines de caractères pour en réduire la diversité, et en produire des chaines grammaticalement exacte. Cest un travail dartisan, celui des des imprimeurs et de leurs coorecteur. Et en particulier dun métier celui du compositeur, ou ouvrier de la casse, qui distribue des caractères de plomb en séquences dans des casiers de bois. cmpositeur Lartisan navait pas de choix, la précision était essentielle pour éviter la coquille. Le texte moderne, numérique, est lobjet de plus daller et retours. Les mots quon pianotent sont corrigés avant même dêtre frappés. Les gestes techniques sont différents mais sarticulent sur une même idée : la langue écrite, du moins les langues alphabétiques sont des chaines de caractères dont la formation suit des règles fluctuantes à travers lhistoire mais contraignante à chaque moments. Les conventiosn peuvent changer, mais dans son temps elle simposent définitivement. Personne nécrirait deffert, pour dire dessert. Et pourtant la graphie du s était un f jusquau XVI ème siècle (trouver la source)! https://www.cairn.info/revue-la-linguistique-2003-1-page-3.htm De nombreuses ressources sont disponibles pour traiter ces chaines de caractères. On utilisera surtout Stringer qui est est un des composants essentiels de tidyverse. Dautres packages sont équivalents : stringi par exemple. 5.1.1 Les opérations sur les chaînes de caractères mettre en minuscule. Lalphabet se présente au moins en deux versions : des majuscules et des minuscules, il est souvent nécessaire de réduire le texte à une seule casse pour en réduire la variété, sauf si les majuscules signalent une information spécifique et socialement conventionnelle. Un mot qui débute par une majuscule signale un nom commun, désormais conceptualisé comme une entité nommée appartenant à différentes catégories : noms de lieux, noms de personnes, noms dorganisation ou lexpression dun sentiment, au sein des chats, la majuscule en série signale un niveau de langage loud, un cri , une engueulade, la véhémence. rechercher une chaine de caractères; remplacer une chaine de caractères extraire une chaine de caractère dun emplacement à lautre supprimer une chaine de caractères. Les nombres, concaténer des chaines de caractères. Le texte peut être divisés en unités. Un paragraphe par exemple, ou un titre. Si la manipulation deslaquelle ? vaccin ? 5.1.2 La technique des expressions régulières (regex) Il ne suffit pas de chercher une chaine de caractère particulière, il faut souvent saisir un ensemble de variations qui suivent un motif determiné et qui répond à une sorte de loi générale. Par exemple si je veux retrouvrer dans un corpus lensemble des mots relatif au monde de lhôpital, nous chercherions aussi le mot hopital. Nombreux seront les locuteur qui omeetent laccent circonflexe. Une formule pour trouver ces deux varietés serait dutiliser un opérateur, \"(), pour définir une option . soit lun soit lautre : h(ô,o)pital Une expression régulière est un masque qui permet didentifier des formes principales et leurs variétés. Il sappuit sur une codification dont quelques éléments clés permettent de se donner une bonne idée de la logique générale le ^, indique que la forme commence par le caractère qui suit ^A le . signifie nimporte quel caractère. le regex ^a. signifiera ainsi nimporte quelle chaine de caractère qui commence par a est est suivi de nimporte quel caractère. le * la répétition indéfinie du caractère . Dun point de vue linguistique les regex travaillent sur la morphologie et ses variations, indépendemment des règles de grammaires mais profitant de leur régularité. Les mots sont généralement composés dune racine, de suffixe et de préfixe qui contiennent les flexions grammaticales et sémantiques. des exemples : la négation : visible et in-visible. la conjugaison : aime et aim-ât la numération : fraise et fraise-s le genre : épicier et épicière-s. grepl 5.1.3 Un fondement profond et ancien Le langage des regex a répondu dabord aux besoin des informaticiens, et sappuie sur une construction mathématique sophistiquée : les automatates finis https://swtch.com/~rsc/regexp/regexp1.html don t un des contributeurs essentiels à été doi.org/10.1145/363347.363387 Ken Thompson fondateur de Grepl a method for locating specific character strings embedded in character text is described and an implementation of this method in the form of a compiler is discussed. The compiler accepts a regular expression as source language and produces an IBM 7094 program as object language. The object program then accepts the text to be searched as input and produces a signal every time an embedded string in the text matches the given regular expression. Examples, problems, and solutions are also presented. https://swtch.com/~rsc/regexp/regexp1.html 5.1.4 Des applications très pratiques et à ceux qui face à des questions de métier, par exemple les professionnel de marketing direct ou des services postaux, ont été amené à traiter de jeux de données textuels limités tel quune adresse postale. dectecter une entité nommée : la majuscule détecter une adresse détecter une date détecter un compte détecter une url 5.2 Nettoyer le texte enlever les mentions enlever les url enlever ou recoder les emojis enlever la ponctuation enlever les nombres 5.3 Corriger le texte Si certains corpus sont par les conditions de leur production presque parfait du point de vue grammatical et lexical, cest le cas en principe des articles de presse et des documents officiels, dautres qui sappuient sur une langue vernaculaire on des graphies plus incertaines et des syntaxes approximatives. Dans un tiers des cas le mot \" opinion\" sorthographie opignons. Chaque mot du lexique sévanouit dans des morphologies nombreuses et approximatives. Cest un obstacle à lanalyse car la variété morphologique est aléatoire. plusieurs stratégies sont possibles. La première est de corriger le texte notamment en employant des outils de corrections efficaces. 5.3.1 La correction orthographique automatique voir hunspell https://cran.r-project.org/web/packages/hunspell/vignettes/intro.html#Custom_Dictionaries 5.3.2 Analyse ciblée par les regex Une application des regex est lanalyse ciblée dun certain nombre de termes. LA corection est partielle mais couvre les cibles essentielles exemple des gestes barières dans le flux twitter 5.4 Identifier les sources Dans lanalyse des contenus sociaux, les textes viennent de sources multiples et confuses. Elles peuvent être aisément multilingue. Analyser un corpus dentretien, une collection de discours, posent peu la questions des locuteurs car ils sont bien identifiés. Ce nest pas le cas dans les réseaux sociaux où les buts sont multiples et plus ou moins avoués. Les acteurs : des professionnels de la politiques et les institutions quils dirigent Journaliste et professionnels de la communication Experts et universitaires les marques et leur community manager les bots les trolls les activistes 5.4.1 Identifier la langue Dans les points précedents on suppose que la langue est homogène, mais les corpus peuvent être multi-langues. Par exemple, dans les corpus davis dhôtes sur Airbnb, les avis sont formulés dans une large variété de langues. Il va falloir en tenir compte et une tâche préliminaire sera de détecter les langues pour séparer les corpus. Le package textcat offre une solution basée sur la fréquence des ngrams (voir chapitre x). et compare la distribution du texte ciblée avec les distributions typiques des langues. Google propose un algo cpd3(https://github.com/google/cld3) plus sophistiqués dans la mesure où cest un réseau de neurones qui fait le travail. Comparons les. On utilise un jeu de donnée Airbnb sur Bruxelles qui accueillant les institutions européennnes est une des villes les plus cosmopolite qui soit avec des fonctionnaires venant de toutes leurope et sexprimant dans une large varité de langue, sans compter les représentations des autres pays du monde. En terme de durée de calcul, la différence en temps de calcul est faramineuse 7 secondes contre 7 minutes, ce qui sexplique car texcat sappuyant sur la distribution des ngrams doit les calculer pour les 36000 observations que nous avons retenues. Faisons un test sur un extrait du corpus Airbnb. BXL2021 &lt;- read_csv(&quot;reviewsBXL2021.csv&quot;) BXL2021$Year&lt;- as.numeric(format(as.Date(BXL2021$date, format=&quot;%Y-%m-%d&quot;),&quot;%Y&quot;)) BXL2021&lt;- BXL2021 %&gt;% filter(Year&gt;2019) # on filtre sur la période de confinement library(cld3) t1&lt;-Sys.time() cld3&lt;-as.data.frame(detect_language(BXL2021$comments))%&gt;%rename(cld3=1) t2&lt;-Sys.time() t_cld3&lt;-t2-t1 #on calcule la durée de l&#39;opération en faisant la différence du temps de départ et d&#39;arrivée library(textcat) t1&lt;-Sys.time() textcat&lt;-textcat(BXL2021$comments) t2&lt;-Sys.time() t_texcat&lt;-t2-t1 foo&lt;-cbind(cld3, textcat) Examinons les résultats et la distribution des langues identifées par les deux systèmes. Si lordre est respecté. Des différences sobservent, cld3 identifie du chinois qui ne fait pas partie du répertoire de texcat. g1&lt;-foo%&gt;%mutate(n=1)%&gt;%group_by(textcat)%&gt;%summarise(n=sum(n))%&gt;% ggplot(aes(x=reorder(textcat,n), y=n))+geom_bar(stat=&quot;identity&quot;)+coord_flip()+scale_y_log10() g2&lt;-foo%&gt;%mutate(n=1)%&gt;% group_by(cld3)%&gt;% summarise(n=sum(n))%&gt;% ggplot(aes(x=reorder(cld3,n), y=n))+ geom_bar(stat=&quot;identity&quot;)+coord_flip()+scale_y_log10() plot_grid(g1, g2, labels = c(&#39;Texcat&#39;,&#39;Cld3&#39;), label_size = 12) Examions maintenant la convergence des méthodes en représentant la répartition du résultat dun système dans les langue de lautre. Si la convergence est parfaite 1000% des textes classé en Français par Textact devrait se retrouver dans 100% de ces textes classé par cld3 et réciproquement. foo1 &lt;-foo %&gt;% mutate(n=1)%&gt;%group_by(textcat) %&gt;%summarise(n=sum(n)) foo1&lt;-foo%&gt;% left_join(foo1) %&gt;% filter(n&gt;10) table&lt;-table(foo1$cld3,foo1$textcat) foo1&lt;-as.data.frame(prop.table(table,2)) ggplot(foo1, aes(reorder(Var2, Freq),Var1)) + geom_tile(aes(fill = Freq)) + scale_fill_gradient(low = &quot;White&quot;,high = &quot;Blue&quot;)+ theme_bw()+ theme(axis.text.x=element_text(angle = 45, hjust =1))+coord_flip() foo1 &lt;-foo %&gt;% mutate(n=1)%&gt;%group_by(cld3) %&gt;%summarise(n=sum(n)) foo1&lt;-foo%&gt;% left_join(foo1)%&gt;%filter(n&gt;10) table&lt;-table(foo1$cld3,foo1$textcat) foo1&lt;-as.data.frame(prop.table(table,1)) ggplot(foo1, aes(reorder(Var1, Freq),Var2)) + geom_tile(aes(fill = Freq, label=Freq)) + scale_fill_gradient(low = &quot;White&quot;,high = &quot;Red&quot;)+ theme_bw()+ theme(axis.text.x=element_text(angle = 45, hjust =1))+coord_flip() library(pheatmap) library(viridis) table2&lt;-as.data.frame(table) %&gt;% mutate(Freq=log10(Freq+1)) %&gt;% pivot_wider(names_from = Var1, values_from = Freq) %&gt;% column_to_rownames( var = &quot;Var2&quot;) pheatmap(table2 , color = inferno(10)) chi2&lt;- chisq.test(table) chi2&lt;- as.data.frame(chi2$residual) table2&lt;-chi2 %&gt;% mutate(Freq=Freq^2)%&gt;% pivot_wider(names_from = Var1, values_from = Freq) %&gt;% column_to_rownames( var = &quot;Var2&quot;) pheatmap(table2 , color = inferno(20, direction=1)) 5.4.2 Identifier la source botometer botchecks "],["une-première-analyse-quantitative.html", "Chapitre 6 Une première analyse quantitative 6.1 Comptons les mots 6.2 la production dans le temps 6.3 Lisibilité et complexité lexicale 6.4 Conclusion", " Chapitre 6 Une première analyse quantitative Avant tout un texte doit être analyser de manière volumétrique. Comment de texte? Quelle longueur ? combien de mots ? quelles variations? Dans ce chapitre nous allons analyser le flux des tweets produit par donald Trump, jusquau moment de son banissement en Janvier 2021, àprès sa défaite. Chargeons le fichier de données. On en profite pour compter le nombre de posts et de variables df &lt;- read_csv(&quot;TrumpTwitterArchive01-08-2021.csv&quot;) nrow&lt;-nrow(df) #nombre de ligne ncol&lt;-ncol(df) #nombre de colonne 6.1 Comptons les mots Il y 56571 tweets et 9 variables. On peut vouloir compter le nombre de mots. A cette fin on emploie une fonction de stringr :str_count`. ( On reviendra sur la question de la manipulation des chaines de caractères dans un chapitre ad hoc) df$nb_mots&lt;-str_count(df$text, &quot; &quot;)+1 # l&#39;astuce : compter les espaces et ajouter 1, pour compter les mots sum_mots&lt;-sum(df$nb_mots) #ON COMPTE LE NOMBRE DE MOTS ggplot(df, aes(x=nb_mots))+ geom_histogram(fill=&quot;deepskyblue3&quot;)+ labs(title=paste0(&quot;Nombre total de mots du corpus : &quot;,sum_mots), x=&quot;Nombre de mots par post&quot;, y=&quot;Fréquence&quot;) Figure 6.1: Distribution du nombre de mots par post La bimodalité provient surement du changement de taille maximum effectué en septembre 2017, le passage de 180 caractères max à 280. On peut le vérifier en examinant cette même distribution - par les courbes de densité - pour chacune des années, avec cette technique rendue fameuse par la pochette de lalbum de Joy Division : un graphique en crêtes (ridges plot) avec ggridges df$Year&lt;-format(df$date, format = &quot;%Y&quot;) #on extrait l&#39;année de la date ggplot(df,aes(x = nb_mots, y = Year, group = Year)) + geom_density_ridges(scale = 3, fill=&quot;peachpuff&quot;)+ theme_ridges() + scale_x_continuous(limits = c(1, 70), expand = c(0, 0)) + coord_cartesian(clip = &quot;off&quot;)+labs(x=&quot;Nombre de mots par post&quot;, y=NULL) Figure 6.2: Evolution de la distribution du nombre de mots 6.2 la production dans le temps Le résultat remarquable est que si Trump dans un premier temps exploite cette nouvel fonctionnalité, il en revient avec un phrasé de 20 mots en moyenne, gardant cependant à loccasion dautre contenu en 50 mots environ. Concluons en examiner le nombre de tweets produit au cours du temps ## plot time series of tweets ts_plot(df, &quot;1 month&quot;, color=&quot;darkblue&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + labs( x = NULL, y = &quot;Nombre de tweets par mois&quot;,title = &quot;Fréquence des posts twitters Donald Trump&quot;)+ scale_x_datetime(date_breaks = &quot;1 year&quot;, labels = scales::label_date_short()) Figure 6.3: Evolution de la production mensuelle des tweets de Trump #raf : labeliser avec les dates clés 6.3 Lisibilité et complexité lexicale Pour aller un peu plus loin, dans les comptages, introduisons deux quantifications utiles du texte : la lisibilité et la complexité lexicale. On gardera le principe dune analyse longitudinale https://fr.wikipedia.org/wiki/Covfefe 6.3.1 Les indices de lisibilité La lisibilité est une vieille notion autant que sa mesure (par exemple Coleman and Liau (1975)). Il sagit dévaluer la complexité dun texte à partir de deux critères : la complexité des mots capturée par le nombre moyen de syllabes par mot, et la complexité des phrases mesurée par le nombre de mots. Le nombre dindicateurs est considérable et le package compagnon de quanteda , [quanteda.textstats](https://quanteda.io/reference/textstat_readability.html) , en fournit au moins des dizaines. Dans lexemple suivant, on se contente dun grand classique, le plus ancien, lindice de Flesch (Flesch 1948) et de ses constituants: le nombre moyen de syllabes par mot et le nombre moyen de mots par phrase. foo&lt;-df %&gt;% filter(isRetweet==FALSE) # on ne prend pas en compte les RT readability&lt;-textstat_readability(foo$text, measure = c(&quot;Flesch&quot;,&quot;meanSentenceLength&quot;, &quot;meanWordSyllables&quot;), min_sentence_length = 3,max_sentence_length = 1000) #la fonction de calcul de lisibilité foo&lt;-cbind(foo,readability[,2:4]) foo1&lt;-foo %&gt;% group_by(Year) %&gt;% summarise(Flesch=mean(Flesch, na.rm=TRUE), SentenceLength= mean(meanSentenceLength, na.rm=TRUE), WordSyllables= mean(meanWordSyllables, na.rm=TRUE)) %&gt;% gather(variable, value, -Year) ggplot(foo1,aes(x=Year, y=value, group=variable))+ geom_line(size=1.2, aes(color=variable), stat=&quot;identity&quot;)+ facet_wrap(vars(variable), scale=&quot;free&quot;, ncol=1)+ labs(title = &quot;Evolution de la lisibilité des tweets de Trump&quot;, x=NULL, y=NULL) Figure 6.4: Evolution de la lisibilité moyenne des tweets de Trump Pour aider le lecteur à donner un sens, voici labaque proposée par Fleschlui-même. Flesch. On peut aussi prendre pour références les éléments suivants: All Plain English examples in this book score at least 60. Here are the scores of some reading materials Ive tested. These are average scores of random samples. Comics 92 Consumer ads in magazines 82 Readers Digest 65 Time 52 Wall Street Journal 43 Harvard Business Review 43 Harvard Law Review 32 Auto insurance policy 10 Trump ne parait pas être sa caricature, non niveau de lisibilité correspond à la Licence. Il est moins simple que le Readers Digest mais plus compliqué à lire que la Harvard Business Review ! 6.3.2 Les indices de complexité lexicale La complexité lexicale rend compte de la diversité du vocabulaire, elle consiste à rapporter le nombre de mot uniques sur le nombre total de mots. La difficulté est que la taille des corpus joue fortement sur cette mesure et que lorsque cette taille est hétérogène, lindicateur marque plus cette variété que les variations de complexité lexicale.(Tweedie and Baayen 1998) https://quanteda.io/reference/textstat_lexdiv.html Une manière plus fine sera de considérer chaque période comme un texte, un pb est que lallongement des tweets peut expliquer laccroissement de diversité Dans notre univers trumpesque, ce nest pas trop sensible, dautant plus que nous allons moyenner les tweets par période.Notons au passage que si nous moyennons la diversité lexicale de chaque tweets , une autre approche pourrait être de concatener lensemble des tweets dune période ( un jour, une semaine) pour approcher cette variable à une autre échelle, qui couvrent lensemble des sujets dintérêt de trump, que les tweets fractionnenet nécessairement. Ce qui en en question dans la mise en pratique nest pas seulement la question du choix de lindice mais aussi la définition de lunité de calcul. La diversité lexical concerne sans doute plus le discours que la phrase. là, encore la nécessité davoir des points de repère, des échelles. lexdiv&lt;-tokens(foo$text) %&gt;% textstat_lexdiv(foo$text, measure = c(&quot;CTTR&quot;),remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_hyphens = FALSE) #la fonction de calcul de diversité foo&lt;-cbind(foo,lexdiv[,2]) foo1&lt;-foo %&gt;% group_by(Year) %&gt;% summarise(CTTR=mean(CTTR, na.rm=TRUE)) %&gt;% gather(variable, value, -Year) ggplot(foo1,aes(x=Year, y=value, group=variable))+ geom_line(size=1.2, aes(color=variable), stat=&quot;identity&quot;)+ facet_wrap(vars(variable), scale=&quot;free&quot;, ncol=1)+ labs(title = &quot;Evolution de la diversité lexicale des tweets de Trump&quot;, x=NULL, y=NULL) Figure 6.5: Evolution de la lisibilité moyenne des tweets de Trump 6.4 Conclusion Nous aurons appris à compter le nombre de document mesurer la complexité du langage mesurer la diversité de son vocabulaire. Ces mesures nont ne sens que si elles peuvent être comparées de manière interne de manière externe References "],["tokenisation.html", "Chapitre 7 Tokenisation 7.1 Le principe 7.2 Les outils 7.3 Ngrams 7.4 Choisir des n grams pertinents 7.5 Propriétés statistiques des ngrams", " Chapitre 7 Tokenisation Létape intiale de toute analyse textuelle est de découper le texte en unités danalyse. Les tokenizers sont les outils indispensables à cette tâche. 7.1 Le principe Découper un texte en unités qui peuvent être Des lettres Des syllabes Des mots Des phrases Des paragraphes Des sections Des chapitres Des livres 7.2 Les outils Pour les exemples on se concentre sur le package tokenizer. Commençons par un tout petit exemple, une citation réputée de Max Weber. #Les données MaxWeber &lt;- paste0(&quot;Bureaucratie: le moyen le plus rationnel que lon connaisse pour exercer un contrôle impératif sur des êtres humains&quot;) #On tokenise, plus on transforme en dataframe le résultat. toc_maxweber&lt;-tokenize_characters(MaxWeber)%&gt;% as.data.frame()%&gt;% rename(tokens=1) #On compte pour chaque token sa fréquence d&#39;apparition foo&lt;-toc_maxweber %&gt;%mutate(n=1) %&gt;% group_by(tokens)%&gt;% summarise(n=sum(n))%&gt;%filter(n&gt;0) #On représente par un diagramme en barre cette distribution des occuences d&#39;apparition, en classant les tokens par fréquence ggplot(foo, aes(x=reorder(tokens,n), y=n))+ geom_bar(stat=&quot;identity&quot;, fill=&quot;royalblue&quot;)+ coord_flip()+labs(title = &quot;unigram, bigram et trigram&quot;, x=&quot;ngram&quot;, y=&quot;nombre d&#39;occurences&quot;, caption =&quot; &#39;Bureaucratie: le moyen le plus rationnel que lon connaisse pour exercer un contrôle impératif sur des êtres humains&#39; &quot;) Figure 7.1: Distribution du nombre de lettres 7.3 Ngrams Les ngrams sont des séquences de n tokens. Il peuvent être consécutifs, ou être 7.3.1 Les lettres toc_maxweber&lt;-tokenize_character_shingles(MaxWeber,n=3, n_min=1) %&gt;% as.data.frame()%&gt;%rename(tokens=1) ft&lt;-flextable(head(toc_maxweber)) ft .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-f03fe9d2{border-collapse:collapse;}.cl-f0330d52{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-f0333444{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-f0335b36{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f0335b37{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f0335b38{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} tokensbbuburuurure foo&lt;-toc_maxweber %&gt;%mutate(n=1) %&gt;% group_by(tokens)%&gt;% summarise(n=sum(n))%&gt;%filter(n&gt;1) ggplot(foo, aes(x=reorder(tokens,n), y=n))+ geom_bar(stat=&quot;identity&quot;, fill=&quot;royalblue&quot;)+ coord_flip()+labs(title = &quot;unigram, bigram et trigram&quot;, x=&quot;ngram&quot;, y=&quot;nombre d&#39;occurences&quot;) Figure 7.2: Distribution du nombre de mots par post le principe de textcat est fondée sur ces ngram de lettre. Chaque langue se charactérise par une distribution particulière des ngrams. Pour décider de lappartenance dun text à une langue, si on dispose des profils de distribution, on comparera la distribution des ngrams du texte à ces références. On pourra ainsi calculer une distance et attribuer le texte à la langue dont il est le plus proche. 7.3.2 Les mots On refait la même opération, mais avec un texte complété. Il y a bien moins de mots que de lettres! #Les données MaxWeber &lt;- paste0(&quot;Bureaucratie: le moyen le plus rationnel que lon connaisse pour exercer un contrôle impératif sur des êtres humains. la bureaucratie est une forme d&#39;organisation générale caractérisée par la prépondérance des règles et de procédures qui sont appliquées de façon impersonnelle par des agents spécialisés. Ces agents appliquent les règles sans discuter des objectifs ou des raisons qui les fondent. Ils doivent faire preuve de neutralité et oublier leurs propres intérêts personnels au profit de lintérêt général&quot;) #On tokenise, plus on transforme en dataframe le résultat. toc_maxweber&lt;-tokenize_words(MaxWeber)%&gt;% as.data.frame()%&gt;% rename(tokens=1) #On compte pour chaque token sa fréquence d&#39;apparition foo&lt;-toc_maxweber %&gt;%mutate(n=1) %&gt;% group_by(tokens)%&gt;% summarise(n=sum(n))%&gt;%filter(n&gt;0) #On représente par un diagramme en barre cette distribution des occurrences, en classant les tokens par fréquence ggplot(foo, aes(x=reorder(tokens,n), y=n))+ geom_bar(stat=&quot;identity&quot;, fill=&quot;royalblue&quot;)+ coord_flip()+labs(title = &quot;unigram, bigram et trigram&quot;, x=&quot;ngram&quot;, y=&quot;nombre d&#39;occurences&quot;, caption =&quot; &#39;Bureaucratie: le moyen le plus rationnel que lon connaisse pour exercer un contrôle impératif sur des êtres humains&#39; &quot;) Figure 7.3: Distribution du nombre de mots 7.4 Choisir des n grams pertinents Dans ce livre lunité principales danalyse restera le mot. Mais nous savons, au moins intuitivement que certaines combinaisons de mots représentent des expressions qui ont la valeur dun mot, une valuer sémantique. Par exemple, lexpression \" Assemblée Nationale\". Ces deux mots réunis constituent un syntagme. Donc une unité de sens. Comment les identifier dans le flot des caractères? La technique est simple : si deux mots se retrouvent dans un ordre donné plus fréquemment que ce que le produit de leurs probabilités dapparition laisse espérer, cest quils constituent une expression. On peut imaginer faire un test du chi² pour décider si un couple de mots constitue une unité sémantique ou non. Le package quanteda propose une bonne solution à ce problème avec la fonction collocation. 7.5 Propriétés statistiques des ngrams Sur dun base dun corpus importants on peut calculer les probabilité dapparitions dun ngram. Cest une ressource de fournit google avec son Books Ngram Viewer. Processus de markov application à la correction "],["analyse-du-sentiment.html", "Chapitre 8 Analyse du sentiment 8.1 Un exemple avec syuzhet 8.2 La généralisation par le Liwc 8.3 Encore dautres généralisations 8.4 construire son propre dictionnaire", " Chapitre 8 Analyse du sentiment liu ping est le fondateur de lanalyse du sentiment et dès 2012 en donne une synthèse complète (Liu 2012) . Depuis des développement considérables ont été apportée par des méthodes de deep learning, et notamment les modèles transformer et renouvvelent consirablement le domaine. On restra ici à un niveau classique ou compositionnel. On travaillera sur un corpus davis trip advisor, sur la période avant co df&lt;-readRDS(&quot;AvisTripadvisor.rds&quot;) 8.1 Un exemple avec syuzhet On utilise le package syuzhet et en particulier le dictionnaire nrc developpé et traduit par Mohammad and Turney (2013) ( Index Feel) Le même outil fournit un autre systéme dannotation qui compte les mentions déléments positifs ou négatifs, ainsi que démotions définies sur la base de linventaire de Plutchik (1982) on utilise simplement la fonction get_nrc_sentiment, en précisant le dictionnaire adéquat. Léchelle comprend en fait deux éléments : les 8 émotion de base *au sens de pluchik, et deux indicateurs de polarité. Lopérationnalisation réalisée par Mohammad and Turney (2013) sinscrit dans une tradition de la recherche en marketing, se souvenir de (???) et de (???). library(syuzhet) #analyse du sentimeent #paramétres method &lt;- &quot;nrc&quot; lang &lt;- &quot;french&quot; phrase&lt;-as.character(paste0(df$Titre,&quot; &quot;,df$Commetaire)) #extraction emotions &lt;- get_nrc_sentiment(phrase,language = &quot;french&quot;) polarity&lt;-subset(emotions,select=c(positive, negative)) df&lt;-cbind(df,polarity) On sintéresse surtout aux mentions positives et négatives (les émotions cest pour plus tard. (la mesure permet ainsi une dissymétrie des deux polarités, il y a le bien, le mal, le mal et le bien, mais aussi si qui nest ni mal ni bien). Les textes étant inégaux en taille on va ramener lindicateur de polarité au nombre de caractéres (sur une base de 500 c) de chaque contribution. En effet lalgo compte les valence et leur intensité est proportionnel à la longueur du texte. Ce qui est clairement démontré par la seconde figure. A partir de ces deux mesures, 4 indicateurs peuvent étre construits * Positivité : nombre de termes positifs pour 500 signes. * Négativitivé : nombre de termes négatifs pour 500 signes. * Valence : rapport du nombre de termes positifs sur les négatifs. * Expressivité : nombre de termes positifs et négatifs. le dernier graphe nous apprend que les jugements plutôt positifs sont aussi les plus expressifs. La froideur des avis les plus négatifs refléte-t-elle une crainte de la désaprobation sociale. Cest une piste de recherche à poursuivre, on pourrait sattendre à ce que les avis les plus négatifs surgissent plus facilement si la densité des négatives est plus importante et observer une sorte dautocorrélation. G1&lt;-ggplot(df, aes(x=positive))+geom_histogram(binwidth = 1, fill=&quot;darkred&quot;)+theme_minimal() G1 G2&lt;-ggplot(df, aes(x=negative))+geom_histogram(binwidth = 1,fill=&quot;Royalblue&quot;)+theme_minimal() G2 8.1.1 Valence et expression la linguistique donne aux mots une valence : elle peut être positive (bonheur), négative (malheur) ou neutre ( tranquité). Cest un régime ternaire. Chaque mot dune phrase est neutre, positif ou négatif. On peut doser les effets On a des dictionnaires df$text&lt;-as.character(paste0(df$Titre,&quot; &quot;,df$Commetaire)) df$WC&lt;-str_count(df$text, &quot;\\\\S+&quot;) df$positivity&lt;-(df$positive)/(df$WC) df$negativity&lt;-(df$negative)/(df$WC) df$valence&lt;-df$positivity-df$negativity df$expressivity&lt;-df$positivity+df$negativity G11&lt;-ggplot(df, aes(x=valence,y=expressivity ))+geom_point(color=&quot;grey&quot;)+geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x, bs = &quot;cs&quot;))+theme_minimal() G12&lt;-ggplot(df, aes(x=negativity,y=positivity ))+geom_point(color=&quot;grey&quot;)+geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x, bs = &quot;cs&quot;))+theme_minimal() G13&lt;-ggplot(df, aes(x=negativity,y= expressivity))+geom_point(color=&quot;grey&quot;)+geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x, bs = &quot;cs&quot;))+theme_minimal() G14&lt;-ggplot(df, aes(x=positivity,y= expressivity))+geom_point(color=&quot;grey&quot;)+geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x, bs = &quot;cs&quot;))+theme_minimal() plot_grid(G11, G12, G13,G14, labels = c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;,&#39;D&#39;)) 8.2 La généralisation par le Liwc Le liwc vient de lidée simple dun psychiatre qui a souhaité faire des diagnoistics de trauma craniens à partir des entretiens menés avec lmes patients atteints. (Tausczik and Pennebaker 2010) Le LIWC permet dobtenir dautres indicateurs du sentiment, une partie des 80 indicateurs proposés est relatif à des dimensions topicales dont trois groupes vont retenir notre attention dans la mesure où ils décrivent une partie de lexpérience relatée dans les commentaires. * La sensorialité ( voir, entendre, sentir) * Lorientation temporelle ( passé, présent, futur) * les émotions négatives (tristesse, colére, ) La procédure pour extraire ces notions est fort simple . On utilise linfrasturure de quanteda et une version francophone du dictionnaire (Piolat et al. 2011) # the devtools package needs to be installed for this to work #devtools::install_github(&quot;kbenoit/quanteda.dictionaries&quot;) library(cleanNLP) library(&quot;quanteda.dictionaries&quot;) dict_liwc_french &lt;- dictionary(file = &quot;FrenchLIWCDictionary.dic&quot;, format = &quot;LIWC&quot;) test&lt;-liwcalike(df$Commetaire,dictionary = dict_liwc_french) df&lt;-cbind(df,test) 8.3 Encore dautres généralisations Lapproche par dictionnaire sest déplacée vers lidentification dautre catégorie les valeurs morales 8.4 construire son propre dictionnaire faire des listes de lmots References "],["annotations-lexicales-et-syntaxiques.html", "Chapitre 9 Annotations lexicales et syntaxiques 9.1 Tokenization 9.2 Stemmatisation, lemmatisation et synonymisation 9.3 Part of Speech (POS) 9.4 Dépendances syntaxiques 9.5 reconnaissance dentités nommées 9.6 co-reférence", " Chapitre 9 Annotations lexicales et syntaxiques Pour aller au-delà de lanalyse du seul lexique et de lanalyse de la cooccurence des termes à travers les textes, comme le font les méthodes de typologie et danalyse factorielle des correspondance depuis longtemps, il est néçessaire danalyser le texte en tenant compte de ses propriétés syntaxiques. Depuis une dizaine dannées, des outils puissants, les annotateurs, sont proposés de manière accessible. Les plus connus sont Spacy, Stanford NLP et désormais UDpipe. Dans lenvironnement r différentes ressources sont disponibles : Quanteda, clean_nlp, Udpipe,  Ils sont disponibles désormais dans de nombreuses langues même si la richesse et la précision obtenues varient dune langue à lautres Ils sappuient sur des corpus plus ou moins étendus et spécialisés dannotations manuelle : les Treebanks. Ils réalisent souvent plusieurs tâches dont les principales sont les suivantes : Tokeniser Lemmatiser Identifier les parts of speech Identifier les dépendances syntaxiques Identifier les entités nommées. identifier les co-reférence 9.1 Tokenization 9.1.1 Les niveaux de tokenisation Un token est une unité danalyse dont la granularité est plus ou moins fine Le paragraphe est sans lunité lunité la plus générale, quand un texte est correctement rédigé, un paragraphe développe une idée. La phrase est lunité de langage, lui correpond un argument, une proposition. Lusage du point suivi dun espace est assez général pour les identifier. Cest lobjet de tokenizers qui feront mieux en analysant le contexte de la phrase pour décider plus précisément si le point sépare bien deux phrase distinctes. Cette unité de phrase elle essentielle. Le mot est la fois le niveau le plus évident et le plus courant. On peut aussi souhaiter extraire dun mot les suffixe et préfixe On peut pour certains problème descendre au niveau de la syllabe et donc du phonème. La lettre reste lunité insécable. 9.1.2 Un exemple en tidytext 9.2 Stemmatisation, lemmatisation et synonymisation Les mots prennent des formes variées, il peut être intéressant dans certains cas de réduire cette variété et ne considérer que lidée des mots. Deux techniques sont disponibles 9.2.1 la stemmatisation cest un 9.2.2 la lemmatisation Un lemme est un mot racine, sans inflexions de genre, de nombre ou de conjugaison. Cest généralement celui quon trouve dans les dictionnaire. 9.2.3 Synonymisation le cas de wordnet et linvention des synset synonymes, antonymes, hipponyne, hyperonymes.. https://cran.r-project.org/web/packages/wordnet/vignettes/wordnet.pdf 9.3 Part of Speech (POS) Dans une phrase les mots non pas la même valeur. Certains sont des nombres propres, ils se réfèrent à ce que nous venons de voir, cest à dire des entitées nommées, dautres désignent des catégories dobjet. Ce sont les noms communs qui se rapportent à des catégories de choses. Un marteau - si jen avais un - peut être nimporte quel marteau, la masse qui casse la pierre, ou ce petit marteau qui me permet denfoncer un clou dans le cadre du tableau. Des typologies universelles ont été construites, elles recouvrent des typologies plus spécifiques à certaines langues. Les désinences du latin ont par exemple disparu du français. Cette forme est spécifiques au latin, on la retrouvera en allemand. La notion de morphosntaxique désigne présisément que les variations de formes des mots dépendent dune règle syntaxique. Prenons le verbe, et sa forme, être, dont la forme au passé simple est était. La forme des mots change, mais lidée reste. Une catégorisation en 17 éléments est proposée. En voici les éléments et les définitions Un petit exemple avec le package UDpipe. library(udpipe) fr &lt;- udpipe_download_model(language = &quot;french&quot;) udmodel_french &lt;- udpipe_load_model(file = &quot;french-gsd-ud-2.5-191206.udpipe&quot;) Citations &lt;- read_csv(&quot;Citations.csv&quot;) Flaubert&lt;-Citations %&gt;% filter(doc==1) UD &lt;- udpipe_annotate(udmodel_french, x=Flaubert$text) x &lt;- as.data.frame(UD) foo&lt;-x %&gt;% select(doc_id,paragraph_id, sentence_id, token_id,token,lemma,head_token_id, upos,feats)%&gt;%filter(sentence_id==1) flextable(foo) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-32e4fec6{border-collapse:collapse;}.cl-32da57b4{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-32da7ea6{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-32da7ea7{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-32daf390{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-32daf391{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-32daf392{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-32daf393{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-32daf394{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-32daf395{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} doc_idparagraph_idsentence_idtoken_idtokenlemmahead_token_iduposfeatsdoc1111Lele2DETDefinite=Def|Gender=Masc|Number=Sing|PronType=Artdoc1112lendemainlendemain9NOUNGender=Masc|Number=Singdoc1113futêtre9AUXMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Findoc1114,,6PUNCTdoc1115pourpour6ADPdoc1116EmmaEmma9PROPNdoc1117,,6PUNCTdoc1118uneun9DETDefinite=Ind|Gender=Fem|Number=Sing|PronType=Artdoc1119journéejournée0NOUNGender=Fem|Number=Singdoc11110funèbrefunèbre9ADJGender=Fem|Number=Singdoc11111..9PUNCT Les trois première colonnes identifient le document, les phrases et les mots. Des lemmes sont proposées. La colonne UPOS donne les part of Speech universel. 9.4 Dépendances syntaxiques Cest à Lucien Tesnière que lon doit lidée de la grammaire de la dépendance qui est au coeur du NLP moderne. Lidée est de déterminer au niveau de la phrase les relations entre ses termes de manière hierarchisée selon un principe de gouvernant à subordonné. Verdelhan-Bourgade (2020) résume son analyse de manière précise et concise : Tous les mots nont pas le même statut. Les mots pleins, qui « expriment directement la pensée » (p. 59), relèvent de quatre catégories structurales : les substantifs (notés par O), les adjectifs (A), les verbes (I), les adverbes (E). Les mots dits vides (souvent désigné de manière pratique par les stopwords aujourdhui) précisent le sens des autres, ou servent à marquer des relations.La connexion établit la relation entre mot régissant et mot subordonné. Lorsquun régissant commande un subordonné, cela constitue un nud, qui peut se faire à partir dune des quatre espèces de mots pleins. Il en donne lexemple suivant : « Très souvent mon vieil ami chante cette fort jolie chanson à ma fille » où lon peut repèrer: un nud verbal, central, qui commande des actants (ami, chanson, fille) et des circonstants (souvent). La valence est « le nombre de crochets par lesquels un verbe peut attraper des actants », à peu près équivalente à « voix ». les nud substantivaux (ami, chanson, fille), qui commandent des compléments (mon, vieil, cette, jolie, ma) le nud adjectival (jolie) qui commande ici le subordonné fort le nud adverbial, très étant subordonné à souvent. \" 9.4.1 Arbre syntaxique Larbre syntaxique est obtenue en analysant les relations entre les termes. Nous poursuivons avec UPpipe, lannovation précédente a déjà fait le travail. A chaque mot deux informations sont associée : la première est lindex du mot auxquel il se rapporte, la seconde est la nature de la relation. Onn utilise ici une fonction écrite par (bnosac](http://www.bnosac.be/index.php/blog/93-dependency-parsing-with-udpipe) pour donner une représentation graphique de larbre. plot_annotation &lt;- function(x, size = 3){ stopifnot(is.data.frame(x) &amp; all(c(&quot;doc_id&quot;,&quot;paragraph_id&quot;, &quot;sentence_id&quot;, &quot;token_id&quot;,&quot;token&quot;,&quot;lemma&quot;,&quot;head_token_id&quot;, &quot;upos&quot;,&quot;feats&quot;, &quot;dep_rel&quot;) %in% colnames(x))) x &lt;- x[!is.na(x$head_token_id), ] x &lt;- x[x$sentence_id %in% min(x$sentence_id), ] edges &lt;- x[x$head_token_id != 0, c(&quot;token_id&quot;, &quot;head_token_id&quot;, &quot;dep_rel&quot;)] edges$label &lt;- edges$dep_rel g &lt;- graph_from_data_frame(edges, vertices = x[, c(&quot;token_id&quot;, &quot;token&quot;, &quot;lemma&quot;, &quot;upos&quot;, &quot;xpos&quot;, &quot;feats&quot;)], directed = TRUE) ggraph(g, layout = &quot;linear&quot;) + geom_edge_arc(ggplot2::aes(label = dep_rel, vjust = -0.20), arrow = grid::arrow(length = unit(4, &#39;mm&#39;), ends = &quot;last&quot;, type = &quot;closed&quot;), end_cap = ggraph::label_rect(&quot;wordswordswords&quot;), label_colour = &quot;red&quot;, check_overlap = TRUE, label_size = size) + geom_node_label(ggplot2::aes(label = token), col = &quot;darkgreen&quot;, size = size, fontface = &quot;bold&quot;) + geom_node_text(ggplot2::aes(label = upos), nudge_y = -0.35, size = size) + labs(title = &quot;Tokenisation, PoS &amp; dependency relations&quot;) } plot_annotation(x, size = 3) Figure 9.1: arbre de dépendance 9.4.2 Vers des application plus générale Dans la phrase precédente on note que funèbre est ladjectif de journée. On peut être tenté de retrouver ces relations qui caractérisent des choses (les nouns ou noms choses) à des adjectifs. On souhaite faire une liste de ces paires. Lexemple va être court : un poème de Maupassant Maupassant&lt;-Citations %&gt;% dplyr::filter(doc==2) UD &lt;- udpipe_annotate(udmodel_french, x=Maupassant$text) foo &lt;- as.data.frame(UD) foo&lt;- foo %&gt;% select(paragraph_id,sentence_id, token_id,lemma,upos,head_token_id,dep_rel)%&gt;% mutate(key1=paste0(paragraph_id,sentence_id, token_id),key2=paste0(paragraph_id,sentence_id, head_token_id) ) On va donc construires un tableau lemme_cible x lemmes_associés, les premiers risqueront dêtres les noms communs, les seconds leurs adjectifs. # selection de la relation. res &lt;- foo %&gt;% filter(dep_rel == &quot;amod&quot;) #on y joint les dependences dep&lt;-res %&gt;% left_join(foo, by = c(&quot;key2&quot; = &quot;key1&quot;)) #on construit la tables des relations lemmes cibles -adjectifs table&lt;-as.data.frame.matrix(table(dep$lemma.x, dep$lemma.y)) #table$n&lt;-rowSums(table) #table$adj&lt;-rownames(table) #row.names(table) &lt;- table$adj le tableau obtenu est en fait la structure dun graphe bipartite. la représentation passe par un de igraph avec pour paramètres importants : * Taille des arcs (edge) : est proportionnelle à la force du lien ( nombre de relations) * Taille des noeud : proportiennel au rangs du noeud. * Couleur et forme des noeuds : lemme et lemme cible. * Un algorithme de force de Fruchterman and Reingold (1991) est employé pour représenter les positions relatives des mots et minimiser les superpositions. Dessiner le réseau bg &lt;-graph_from_incidence_matrix(table, weighted=TRUE) summary(bg) ## IGRAPH 3384a25 UNWB 36 20 -- ## + attr: type (v/l), name (v/c), weight (e/n) #E(bg)$weight# See the vertex attributes #V(bg)$type #V(bg)$name # Plot the network shape = ifelse(V(bg)$type, &quot;circle&quot;, &quot;square&quot;) # assign shape by node type col = ifelse(V(bg)$type, &quot;peachpuff&quot;, &quot;darkolivegreen1&quot;) # assign color by node type plot(bg, vertex.shape = shape, vertex.label.cex=.9,vertex.label.color=&quot;black&quot;,vertex.color = col,edge.color=&quot;azure2&quot;,vertex.frame.color=col,vertex.label.family=&quot;TT Arial&quot;, vertex.size=0.5*igraph::degree(bg),layout=layout_with_fr,edge.width=1*E(bg)$weight,edge.curved=0.5) 9.5 reconnaissance dentités nommées En français courant les entités nommées correspondent largement à lidée de noms propres. Un nom propre à une entité. Une chose qui est est indépendemment des catégories qui peuvent létiqueter. John Dupont, né le 19 février 1898 à Glasgow et abattu à Verdun le 8 août 1917, est un personnage unique. John Dupont ne désigne par une catégorie, mais bien une personne singulière. La designation peut cependant être ambigüe, il y a un Paris, Texas., et un Paris sur Seine. La morphologie ne ressout pas lambiguité. les entités nommées appartiennent à différentes catégories dobjets : des noms de lieux, des noms de personnes, des noms de marques, des acronymes dorganisation, Elles ne représentent jamais une catégorie mais une unité singulière. https://cran.r-project.org/web/packages/nametagger/nametagger.pdf 9.6 co-reférence References "],["constitution-du-corpus-1.html", "Chapitre 10 Constitution du corpus 10.1 Scrapping 10.2 API", " Chapitre 10 Constitution du corpus Some significant applications are demonstrated in this chapter. 10.1 Scrapping De ces deux approches on pourra considérer que la première correspond à un internet sauvage où la collecte dinformation se traduit par une technique de chasseurs-cueilleurs, le glanage. Le recours aux APIs est civilisé, ne serait-ce parce quon introduit une sorte détiquette, des règles de courtoisie, un système de reconnaissance réciproque et dattribution de droits. Le scraping est lactivité qui consiste à moissonner les informations disponibles sur le net en simulant et en automatisant la lecture par un butineur. Elle consiste à construire un robot capable de lire et denregistrer les informations disponibles sous forme html puis à les distribuer (parsing) dans des tableaux structurés, selon une stratégie dexploration du web préalablement définie. De nombreuses ressources sont disponibles, mais pour en rester à r , le package rvest permet de réaliser des extractions simples mais suffisantes pour de nombreux usages. Les caractéristiques clés du scraping :  La nécessité de programmer de manière ad hoc, en fonction des spécificités de chaque site.  Des stratégies mécaniques, en boules de neige.  Le risque de deny of service, cest à dire de saturer ou de parasiter un système et de sexposer à ses contre-mesures.  Les conditions légales ne sont pas homogènes et relèvent de différents droits : de la propriété intellectuelle, du respect de la vie privée, du droit de la concurrence. Cependant des facilités et tolérances sont souvent accordées quand cest dans un objectif de recherche et que des précautions minimales danonymisation ou de pseudonymisation sont prises, et que les règles de conservation et de destruction des données sont précisées.  La question éthique va au-delà du droit, elle concerne les conséquences de cette action sur lévolution densemble. On notera quelle participe à la robotisation du web ( plus de 50% du trafic résulterait de la circulation des spiders , scrapers2, sniffers et autres bots. Et quelle fait lobjet de contre-mesures. Parmi les contre-mesures, le développement des APIs qui permet un accès contrôlé par le détenteur des informations en échange dune facilitation de laccès, et dune plus grande fiabilité des données. Lutilisation dAPI lève lambiguïté légale qui accompagne le scraping est peut ainsi paraître comme plus civilisée. Elle nécessite naturellement que le gestionnaire de la base de données fournissent les moyens de sidentifier et de requêter, elle peut avoir linconvénient dêtre coûteuse quand laccès est payant. Elle affirme aussi le pouvoir de ceux qui contrôlent les données. Ces dernières ne sont pas, insuffisamment pour le chercheur, un bien commun. 10.1.1 Prendre avantage de la structure du html 10.1.2 des problèmes de droits et déthique 10.1.3 rvest avec r 10.2 API 10.2.1 Un tour dhorizon des API 10.2.2 Un exemple avec Rtwitter library (twittr) # on appelle la librairie twittr qui permet les requêtes consumerKey&lt;-Xq #paramètres requis par l API de twitter (Ouvrir un compte au prélable) consumerSecret&lt;-30l access_token&lt;-27A access_secret&lt;-zA7 fonction dinitialisation des requêtes setup_twitter_oauth(consumerKey, consumerSecret, access_token,access_secret) recherche des tweets tweets1 &lt;- searchTwitter(#IA, n = 2000, lang = fr, resultType = recent, since = 2019-08-01) transformer en data frame tweets_df1 &lt;- twListToDF(tweets1) "],["vectorisation-du-corpus.html", "Chapitre 11 Vectorisation du corpus 11.1 Word2vec 11.2 paragraph2vec 11.3 lavenir des modèles pré-entrainés", " Chapitre 11 Vectorisation du corpus 11.1 Word2vec 11.1.1 vectorisation pipe 11.1.2 closest2 11.1.3 map with tsne 11.2 paragraph2vec 11.3 lavenir des modèles pré-entrainés "],["topic-analysis.html", "Chapitre 12 Topic Analysis 12.1 LDA 12.2 STM", " Chapitre 12 Topic Analysis 12.1 LDA 12.1.1 le modèle de blei 12.1.2 implementation avec wor2vec 12.1.3 Représentation graphique 12.1.4 La 12.2 STM 12.2.1 12.2.2 "],["machine-learning-with-text.html", "Chapitre 13 Machine learning with text 13.1 simples models 13.2 art of featuring", " Chapitre 13 Machine learning with text 13.1 simples models 13.1.1 Naives bayes 13.1.2 elastic net 13.1.3 RF 13.2 art of featuring utiliser les plongements "],["deep-learning.html", "Chapitre 14 Deep Learning 14.1 Lenvironnement keras 14.2 Les architectures du texte : RNN, LTSM, Transformer et reformer 14.3 Les cas dapplications remarquables", " Chapitre 14 Deep Learning Pour lutilisateur en sciences sociales même si le deep learning est accessible dans un environnement r, et nous examinerons les possibilités offerte par kera, constuire son modèle de langage est sans doute hors de portée. Il sagira donc le plus souvent demployer des modèles pré-entrainés, et éventuellement de les réentrainer sur nos coprus de données. 14.1 Lenvironnement keras linterface de r pour Keras 14.1.1 Les fonctions principales cpu et gpu 14.1.2 Un premier exemple lequel? 14.1.3 Un deuxième exemple Lequel? 14.2 Les architectures du texte : RNN, LTSM, Transformer et reformer Les évolutions de ces 10 dernières années se caractèreisent par la recherche darchitectures qui prennent en comptent la structure du texte : il y a un ordre séquentiel : les mots font sens quand on les lis ou les entends après une succision dordre mots dont on connait les règles de composition. 14.2.1 rnn Il était logique que les rnn soient la première architecture qui a rendu des résultats intéressants. Leur acaractère autorégressif 14.2.2 ltsm Les ltsm on apportant une amélioration en prenant en compte des coorélations immédiates mais aussi plus lointaine dans le régl&amp;age de loubli et de la mémoire 14.2.3 transformer Linnovation des transformer, fondée sur des modèles a trou.. 14.2.4 reformer Les reformer étendent léchelle des corrélations possibles. Parfois ce qui donne le sens dun texte après 500 mots est peut _être le premier, même si nous ne lons pas perçu, il devient le contexte de la chute par exemple. exemple? 14.3 Les cas dapplications remarquables 14.3.1 Detection dintention quand la théorie des actes de langages rencontre linformatique 14.3.2 détection de toxicité des contenus 14.3.3 la detection des trolls 14.3.4 détection des sophismes et autres fallacies la lutte anti fake 14.3.5 La détection du sarcasme et de lironie 14.3.6 Lextraction dargumennts triplet "],["translation.html", "Chapitre 15 Translation 15.1 simples models 15.2 art of featuring", " Chapitre 15 Translation 15.1 simples models 15.1.1 Naives bayes 15.1.2 elastic net 15.1.3 RF 15.2 art of featuring utiliser les plongements "],["modèles-génératifs.html", "Chapitre 16 Modèles génératifs 16.1 simples models 16.2 art of featuring", " Chapitre 16 Modèles génératifs 16.1 simples models 16.1.1 Naives bayes 16.1.2 elastic net 16.1.3 RF 16.2 art of featuring utiliser les plongements "],["references.html", "References", " References "]]
