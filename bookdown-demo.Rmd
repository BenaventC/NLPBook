--- 
title: "NLP avec r et en français"
author: "Sophie Balech et Christophe Benavent"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook."
---

# Préface

Ce livre synthètise ce que nous avons appris depuis quelques années.

Il a pour but d'être reproductible


## La structure du livre

L' analyse NLP peut être analysée comme un processus qui va de la collecte ( ou production ) jusqu' à la diffusion. 

## Les jeux de données 

Plusieurs cas, et les données afférantes sont utilisées : 

* [Trump Twitter Archive](https://www.thetrumparchive.com/) : L'intégralité des tweets de Trump jusqu'à son banissement le 8 Janvier 2021.
* Confinement Jour J
*
*

## Prérequis

Ceci est un _livre_ écrit en **Markdown**. avec le package **Bookdown** 

Le code s'appuie très largement sur **tidyverse** et emploie largement les ressources de **ggplot** . Les packages seront introduits au fur et à mesure. En voici la liste complète.


```{r pack}
knitr::opts_chunk$set(echo = TRUE, message=FALSE,warning=FALSE)

#boite à outils
library(tidyverse) # inclut ggplot pour la viz, readr et 
library(cowplot) #pour créer des graphiques composés
library(colorspace) #pour les couleurs
library(ggridges)
# NLP
library(rtweet)
library(quanteda)
library(quanteda.textstats)
library(udpipe)
library(tidytext)
library(cleanNLP)

#mise en page des tableau
library(flextable)

#HREPR2SENTATION DE R2SEAUX
library(igraph)
library(ggraph)

#statistiques et modèles

#ML
theme_set(theme_bw())

```

L'ensemble du code est disponible [ici]()

Quelques conventions d'écriture du code r

 * On appele les dataframes  de manière générale `df`, les tableaux intermédiaires sont appelé systématiquement `foo` et 
 * gestion des palettes de couleurs

## A propos de bookdown


The **bookdown** package can be installed from CRAN or Github:

```{r eval=FALSE}
#install.packages("bookdown")
# or the development version
#devtools::install_github("rstudio/bookdown")
```

Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading `#`.

To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): <https://yihui.name/tinytex/>.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

# Introduction {#intro}


Le texte connaît une double révolution. la première est celle de son système de production, la seconde est celle de sa lecture. 

Il se produit désormais tant de textes que personne ne peut plus tous lire, même en réduisant son effort à sa sphère d’intérêt et de compétence. La production primaire de texte se soumet ensuite à ceux qui en contrôlent les flux et en exploitent les contenus, qui les mettent en avant ou les écartent, définissant la composition de ce que chacun va lire. La révolution de la lecture est venue avec les moteurs de recherche, et les pratiques de curations (ref), c’est une lecture sélectionnée et digérée par les moteurs de recommandation. (ref). S’il ne fallait qu’un exemple on pourrait évoquer la transformation radicale de la littérature dite scientifique sur le plan technique. La recherche par mots clés est complétée de plus en plus par des outils de veille, l’indexation a donné naissance à l’immatriculation de la moindre note, les fichiers ont adopté des standards, l’interopérabilité est de mise, le réseau des co-citation est maintenu en temps réel. Les scores qualifient autant les articles que leurs auteurs et les revues qui les accueillent. 

## Un nouveau champs méthodologique

Pour le chercheur qui étudie les organisations, cette révolution textuelle offre de nouvelles opportunités d’obtenir et d’analyser des données pour vérifier ses hypothèses. La production abondante d’avis de consommateurs, de discours de dirigeants, de compte-rendus de conseils, d’articles techniques, rencontre une multiplication des techniques, provenant de la linguistique computationnelle, de la fouille de données, des moteurs de recommandation, de la traduction automatique, et des ressources nouvelles et précieuses pour traiter l’abondance des données. Un nouveau paradigme méthodologique se construit à la croisée de données abondantes et de techniques de traitement intelligentes. Il permet d’aller plus loin que l’analyse lexicale traditionnelle en incorporant des éléments syntaxiques, sémantiques, et pragmatiques, proposés par l’ensemble des outils des techniques de traitement du langage naturel.
Il se dessine surtout une nouvelle approche méthodologique qui prend place entre l’analyse qualitative, et les traditionnelles enquêtes par questionnaires capables de traiter des corpus d’une taille inédite. Le travail de (Humphreys et Wang 2018) en donne une première synthèse dans le cadre d’un processus qui s’articule autour des différentes phases d’une recherche : la formulation de la question de recherche, la définition des construits, la récolte des données, l’opérationnalisation des construits, et enfin l'interprétation, l’analyse et la validation des résultats obtenus. Dans le champ du management, on trouvera des synthèses pour la recherche en éthique (Lock et Seele 2015), en comportement du consommateur @humphreys_automated_2018  en management public  (Anastasopoulos, Moldogaziev, et Scott 2017) ou en organisation (Kobayashi et al. 2018). ( et ajouter en sociologie le texte recent de xxx)

## Les facteurs de développement
Ces développements sont favorisés par un environnement fertile dont trois facteurs se renforcent mutuellement.
 * Le premier est l’expansion de deux langages, proprement statistiques pour r et plus généraliste pour python. Le propre de ces langages est, prenons le cas de r, de permettre d’élaborer des fonctions, dont un ensemble cohérent pour réaliser certaines tâches peuvent être rassemblées dans une bibliothèque. Coder une analyse revient ainsi à jouer avec un immense jeu de lego, dont de nombreuses pièces sont déjà pré-assemblées. D’un point de vue pratique, les lignes d’écriture sont fortement simplifiées permettant à un chercheur sans compétence de codage d’effectuer simplement des opérations complexes. L’autre conséquence est la croissance exponentielle du nombre de packages disponibles : près de 20 000 pour r. Dans le domaine de l’analyse du texte, on citera l’ancêtre “tm", le sophistiqué “quanteda”, “ cleanNLP”, “tidytext”. La dimension langagière se traduit aussi dans l’édition : le rôle du markdown et des carnets Jupyters1. De plus de nouvelles familles de techniques se généralisent et sont accessibles en open-source au travers de langages tels que python ou r, qui est privilégié dans ce chapitre. 
 * Le second, intimement lié au premier, est la constitution d’une large communauté de développeurs et d’utilisateurs qui se retrouvent aujourd’hui dans des plateformes de dépots (Github, Gitlab), d’arxiv, de plateformes de type quora (StalkOverFlow), de tutoriaux, de blogs (BloggeR) et de journaux (Journal of Statistical Software). Des ressources abondantes sont ainsi disponibles et facilitent la formation des chercheurs et des data scientists. Toutes les conditions sont réunies pour  engendrer une effervescence créative. 
 * Le troisième est la multiplication des sources de données et leur facilité d’accès. Les données privées, et en particulier celles des réseaux sociaux,  même si un péage doit être payé pour accéder aux APIs, popularisent le traitement de données massives. Le mouvement des données ouvertes (open data) proposent et facilitent l’accès à des milliers de corps de données : retards de la SNCF, grand débat, le formidable travail de l’Insee, european survey etc.
 
## Le projet de l’ouvrage

Dans ce chapitre, on choisit de présenter les différentes facettes de ce qui s'appelle TAl, NPL, Text Mining,  dans une approche procédurale qui suit les principales étapes du traitement des données. On rendra compte à chaque étape des techniques disponibles, et on illustre d’exemples. Nous suivrons ici une approche plus fidèle au processus de traitement des données, lequel peut connaître une stratégie inférentielle et exploratoire - quelles informations sont utiles au sein d’un corpus de texte -, tout aussi bien qu’une stratégie hypothético-déductive. Nous resterons agnostiques sur cette question, restant délibérément à un niveau technique et procédural.


## Labelliser les figures et les tableaux 

You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods).

Figures and tables with captions will be placed in `figure` and `table` environments, respectively.

```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}
par(mar = c(4, 4, .1, .1))
plot(pressure, type = 'b', pch = 19)
```

Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab).

```{r nice-tab, tidy=FALSE}
knitr::kable(
  head(iris, 20), caption = 'Here is a nice table!',
  booktabs = TRUE
)
```

You can write citations, too. For example, we are using the **bookdown** package [@R-bookdown] in this sample book, which was built on top of R Markdown and **knitr** [@xie2015].

<!--chapter:end:01-intro.Rmd-->

# Constitution du corpus

La constitution d'un corpus est la première étape d'un projet NLP. Il se définit d'abord par des méthodes de collecte, ensuite par des techniques d'échantillonnage, et enfin par des techniques de filtrage.


## Méthodes de collecte

Un corpus est un ensemble de documents.  Ils peuvent être courts, les tweets, paas trop long - abstract articles court - long ( article de recherche, ou très long ( livres).  

### Les techniques de recueil direct

Ces document sont d'abord obtenus directements
les entretiens 
le processus de la transcription

#### La gestion d'un ensemble de documents



pdftool


### L'exploitation de base de données textuelles

Prenons l'exemple de factiva

https://github.com/koheiw/newspapers


### Scrapping

De ces deux approches on pourra considérer que la première correspond à un internet sauvage où la collecte d’information se traduit par une technique de chasseurs-cueilleurs, le glanage. Le recours aux APIs est civilisé, ne serait-ce parce qu'on introduit une sorte d’étiquette, des règles de courtoisie, un système de reconnaissance réciproque et d’attribution de droits. 
Le scraping est l’activité qui consiste à moissonner les informations disponibles sur le net en simulant et en automatisant la lecture par un butineur. Elle consiste à construire un robot capable de lire et d’enregistrer les informations disponibles sous forme html puis à les distribuer (parsing) dans des tableaux structurés, selon une stratégie d’exploration du web préalablement définie.
De nombreuses ressources sont disponibles, mais pour en rester à r , le package rvest permet de réaliser des extractions simples mais suffisantes pour de nombreux usages. 

Les caractéristiques clés du scraping :

    * La nécessité de programmer de manière ad hoc, en fonction des spécificités de chaque site. 
    * Des stratégies mécaniques, en boule de neige. 
    


#### Prendre avantage de la structure du html

Le langage html est un langage à balise

Les balises sont la cible du scrapping


#### des problèmes pratiques, juridiques et éthiques

La pratique du scrapping se heurte à différents problèmes éthiques et juridique. Si elle n'est pas interdite en tant que telle, elle se confronte à différents droits et principes éthiques

En termes pratiques

 * Le risque de deny of service, c’est à dire de saturer ou de parasiter un système et de s’exposer à ses contre-mesures.
 * Le risque d'information parcellaires, tronquées, inexactes qui résultent de ces contre-mesures. Les producteurs développent des stratégies moins naives. L'exemple des pages numérotée par ordre de production auxquels on substitue un nombre au hasard pour annihilier l'information temporelle.

En termes de droits Les conditions légales ne sont pas homogènes et relèvent de différents droits : 

 * de la propriété intellectuelle, 
 * du respect de la vie privée, 
 * du droit de la concurrence. 

Cependant des facilités et tolérances sont souvent accordées quand c’est dans un objectif de recherche et que des précautions minimales d’anonymisation ou de pseudonymisation sont prises, et que les règles de conservation et de destruction des données sont précisées. 
    
En termes éthiques

 * Un principe éthique essentiel dans la recherche, et ailleurs, et de ne pas nuire à la socité dans son ensemble, hors cette technique participe à la “robotisation” du web (plus de 50% du trafic résulterait de la circulation des spiders , scrapers, sniffers et autres bots). 
 * Elle contribue à la complexification du web, et implique une consommation excessive de ressources energétiques.  

#### rvest avec r

le package rvest


### API

Les API doivent être considérées comme la voie normale d'accès à l'information: les requêtes sont reproductibles au moins par les requêtes, les bases visée peuvent varier.

Parmi les contre-mesures, le développement des APIs qui permet un accès contrôlé par le détenteur des informations en échange d’une facilitation de l’accès, et d’une plus grande fiabilité des données.

L’utilisation d’API lève l'ambiguïté légale qui accompagne le scraping est peut ainsi paraître comme plus civilisée. Elle nécessite naturellement que le gestionnaire de la base de données fournissent les moyens de s’identifier et de requêter, elle peut avoir l'inconvénient d’être coûteuse quand l’accès est payant. Elle affirme aussi le pouvoir de ceux qui contrôlent les données. Ces dernières ne sont pas, insuffisamment pour le chercheur, un bien commun.


#### Un tour d'horizon des API

La plus part des grande plateformes offrent des API plus ou moins ouvertes

- twitter est celle qu'on utilise dans cet ouvrage


#### Un exemple avec Rtwitter

library (twittr) # on appelle la librairie twittr qui permet les requêtes
consumerKey<-"Xq..."   #paramètres requis par l’ API de twitter (Ouvrir un compte au prélable)
consumerSecret<-"30l..."
access_token<-"27A..."
access_secret<-"zA7..."

fonction d’initialisation des requêtes
setup_twitter_oauth(consumerKey, consumerSecret, access_token,access_secret)

recherche des tweets
tweets1 <- searchTwitter("#IA", n = 2000, lang = "fr", resultType = "recent", since 
= "2019-08-01") 

transformer en data frame 
tweets_df1 <- twListToDF(tweets1) 



## Un corpus reste un échantillon

La collecte doit rester raisonnée. 

Les unités de texte

Une unité de texte : un chaine de caractère intégrée dans un document. Celui ci peut être un livre un article, une note, une transcription, 

 * Un document
 * Un ou des auteurs du document
 * Une date
 * Un endroit
 * Un contexte : les unités précedente, et subséquentes. 
 

## Filtrage

Dans un corpus massif il est souvent néçéssaire de filtrer le contenu d'éléments qui sortent du champs d'études. POur étudier l'opinion inutile d'analyser ce qui est produit par les bots ou les trolls.

Il faut aussi réduire les contenus en éliminant les doublons

### Eliminer les bots et les fakes

botometer

### Eliminer les doublons
unique

## références



<!--chapter:end:02-constitution-de-corpus.Rmd-->

# Préparation des données


Avant de se lancer dans l'analyse, il est nécessaire de préparer le texte, de le pré-traiter. Il s'agira d'abord de savoir dans quelle langue il est exprimé, ensuite de simplifier le teste en enlevant l'information redondante


## Identifier la langue

Pour travailler la langue, il faut avant tout l'identifier. Nombreux sont les corpus multi-lingues. Les commentaire de airbnb par exemple. De manière générale les outils de traitement du langage nécessitent que l'on détermine la langue.



Le package textcat offre une solution basée sur la frénquence des ngrams.

en voici une appplication sur le corpus airbnb.


## Nettoyer le texte


 * enlever les mentions
 * enlever les url
 * enlever ou recoder les emojis
 * enlever la ponctuation
 * enlever les nombres
 

## Corriger le texte

Dans un tiers des cas le mot " opinion" s'orthographie "opignons". Chaque mot du lexique s'évanouit dans la langue vernaculaire dans des morphologies nombreuses. 

plusieurs stratégies sont possibles

### La correction orthographique automatique

voir hunspell

https://cran.r-project.org/web/packages/hunspell/vignettes/intro.html#Custom_Dictionaries


### L'usage des regex

Une expression régulière est un masque qui permet d'identifier des formes principales et leurs variétés. 


<!--chapter:end:03-preparation-des-donnees.Rmd-->

# Une première analyse quantitative

Chargeons le fichier de données. On en profite pour compter le nombre de post et de variables

```{r chap01_00}

df <- read_csv("TrumpTwitterArchive01-08-2021.csv")
nrow<-nrow(df)
ncol<-ncol(df)

```
## Comptons les mots

Il y `r nrow` tweets et `r ncol` variables. On peut vouloir compter le nombre de mots. A cette fin on emploie une fonction de `stringr`. 

```{r 1, fig.cap='Distribution du nombre de mots par post', out.width='80%', fig.asp=.75, fig.align='center'}

df$nb_mots<-str_count(df$text, " ")+1 # l'astuce : compter les espaces et ajouter 1, pour compter les mots
sum_mots<-sum(df$nb_mots)             #ON COMPTE LE NOMBRE DE MOTS
ggplot(df, aes(x=nb_mots))+
  geom_histogram(fill="deepskyblue3")+
  labs(title=paste0("Nombre total de mots du corpus : ",sum_mots), x="Nombre de mots par post", y="Fréquence")

```

La bimodalité provient surement du changement de taille maximum effectué en setembre 2017. On peut le vérifier en examinant cette même distribution - par les courbes de densité - pour chacune des années, avec cette technique rendue fameuse par la pochette de l'album de Joy Division : un graphique en crêtes (ridges plot) avec [ggridges](https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html)

```{r 2, fig.cap=' Evolution de la distribution du nombre de mots', out.width='80%', fig.asp=.75, fig.align='center'}

df$Year<-format(df$date, format = "%Y") #on extrait l'année de la date

ggplot(df,aes(x = nb_mots, y = Year, group = Year)) +
  geom_density_ridges(scale = 3, fill="peachpuff")+
  theme_ridges() +
  scale_x_continuous(limits = c(1, 70), expand = c(0, 0)) +
  coord_cartesian(clip = "off")+labs(x="Nombre de mots par post", y=NULL)
```

## la production dans le temps


Le résultat remarquable est que si Trump dans un premier temps exploite cette nouvel fonctionnalité, il en revient avec un phrasé de 20 mots en moyenne, gardant cependant à l'occasion d'autre contenu en 50 mots environ.

Concluons en examiner le nombre de tweets produit au cours du temps

```{r 3, fig.cap='Evolution de la production mensuelle des tweets de Trump', out.width='80%', fig.asp=.75, fig.align='center'}

## plot time series of tweets
ts_plot(df, "1 month", color="darkblue") + theme(plot.title = element_text(face = "bold")) + 
  labs( x = NULL, y = "Nombre de tweets par mois",title = "Fréquence des posts twitters Donald Trump")+
  scale_x_datetime(date_breaks = "1 year", labels = scales::label_date_short())
#raf : labeliser avec les dates clés
```


## Lisibilité et complexité lexicale

Pour aller un peu plus loin, dans les comptages, introduisons deux quantifications utiles du texte : la lisibilité et la complexité lexicale. On gardera le principe d'une analyse longitudinale

https://fr.wikipedia.org/wiki/Covfefe

### Les indices de lisibilité

La lisibilité est une vieille notion autant que sa mesure (par exemple @coleman_computer_1975). Il s'agit d'évaluer la complexité d'un texte à partir de deux critères : la complexité des mots capturée par le nombre moyen de syllabes par mot, et la complexité des phrases mesurée par le nombre de mots. 

Le nombre d'indicateurs est considérable et le package compagnon de quanteda , quanteda.textstats , en fournit au moins une trentaine. On en utilise les ressources.

```{r 4, fig.cap='Evolution de la lisibilité des tweets de Trump', out.width='80%', fig.asp=.75, fig.align='center'}



readability<-textstat_readability(df$text, measure = "Flesch")
df<-cbind(df,readability[,2])
foo<-df %>% 
  group_by(Year)%>%
  summarize(Flesch=mean(Flesch, na.rm=TRUE))
ggplot(foo,aes(x=Year, y=Flesch, group=1))+geom_line(size=1.2, color="darkblue")+
  labs(title = "Evolution de la lisibilité des tweets de Trump", x=NULL, y="indice de Flesch")

```

### Les indices de complexité lexicale

La complexité lexicale rend compte de la diversité du vocabulaire, elle consiste à rapporter le nombre de mot uniques sur le nombre total de mots. La difficulté est que la taille des corpus joue fortement sur cette mesure et que l'orsque cette taille est hétérogène, l'indicateur marque plus cette variété que les variations de complexité lexicales. 


C'est un pb auquel se sont attaché x et y

Dans notre univers trumpesque, ce n'est pas trop sensible, d'autant plus que nous allons moyenner les tweets par période.

Une manière plus fine sera de considérer chaque période comme un texte, 

<!--chapter:end:04-Analyse-quantitative-du-corpus.Rmd-->

# Tokenisation

L'étape intiale de toute analyse textuelle est de découper le texte en unités d'analyse. Les tokenizers sont les outils indispensables à cette tâche. 


## Le principe

découper un texte en unités

 * des lettres
 * des syllabes
 * des mots
 * des phrases
 * des paragraphes
 * des sections
 * des chapitres
 * des livres

## les outils

quanteda


## Collocation

Dans ce livres l'unité principales d'analyse restera le mot. Mais nous savons, au moins intuitivement que certaines combinaisons de mots repréentent des expressions qui ont la valeur d'un mot, une valuer sémantique. Par exemple, l'expression " Assemblée Nationale". Ces deux mots réunis constitue un syntagme. Et une unité de sens.

Comment les identifier dans le flot des caractères? 

La technique est simple : si deux morts se retrouvent dans un ordre donné plus pfrquemment que ce que le produit de leur probabilité d'apparition laisse espérer, c'est qu'il constitue une expression régulière. On peut imaginer faire un test du chi² pour décider si un couple de mots constitue une unité sémantique ou non. 

Le package quanteda p^ropose une bonne solution à ce problème avec la fonction collocation.


<!--chapter:end:05-tokenisation.Rmd-->

# Analyse du sentiment

liu ping est le fondateur




## Valence et expression

la linguistique donne aux mots une valence : elle peut être positive (bonheur), négative (malheur) ou neutre ( tranquité).

C'est un régime ternaire. Chaque mot d'une phrase est neutre, positif ou négatif. 

On peut doser les effets

On a des dictionnaires



## La généralisation par le liwc

Le liwc vient de l'idée simple d'un psychiatre qui a souhaité faire des diagnoistics de trauma craniens à partir des entretiens menés avec lmes patients atteints. 

80 dictionnaires permettent de saisirs des 


## Encore d'autres généralisations

L'approche par dictionnaire s'est déplacée vers l'identification d'autre catégorie

 * les valeurs morales
 
 
## construire son propre dictionnaire


faire des listes de lmots





<!--chapter:end:06-Analyse-du-sentiment.Rmd-->

# Annotations lexicales et syntaxiques

Pour aller au-delà de l'analyse du seul lexique et de l'analyse de la cooccurence des termes à travers les textes, comme le font les méthodes de typologie et d'analyse factorielle des correspondance depuis longtemps, il est néçessaire d'analyser le texte en tenant compte de ses propriétés syntaxiques. Depuis une dizaine d'années, des outils puissants, les annotateurs, sont proposés de manière accessible.

Les plus connus sont Spacy, Stanford NLP et désormais UDpipe.

Dans l'environnement r différentes ressources sont disponibles : Quanteda, clean_nlp, Udpipe, ...

Ils sont disponibles désormais dans de nombreuses langues même si la richesse et la précision obtenues varient d'une langue à l'autres

Ils s'appuient sur des corpus plus ou moins étendus et spécialisés d'annotations manuelle : les Treebanks.

Ils réalisent souvent plusieurs tâches dont les principales sont les suivantes :

- Tokeniser
- Lemmatiser
- Identifier les parts of speech
- Identifier les dépendances syntaxiques
- Identifier les entités nommées. 
- identifier les co-reférence


## Tokenization


### Les niveaux de tokenisation

Un token est une unité d'analyse dont la granularité est plus ou moins fine

 * Le paragraphe est sans l'unité l'unité la plus générale, quand un texte est correctement rédigé, un paragraphe développe une idée.
 * La phrase est l'unité de langage, lui correpond un argument, une proposition. L'usage du point suivi d'un espace est assez général pour les identifier. C'est l'objet de tokenizers qui feront mieux en analysant le contexte de la phrase pour décider plus précisément si le point sépare bien deux phrase distinctes. Cette unité de phrase elle essentielle.
 * Le mot est la fois le niveau le plus évident et le plus courant.
 * On peut aussi souhaiter extraire d'un mot les suffixe et préfixe 
 * On peut pour certains problème descendre au niveau de la syllabe et donc du phonème. 
 * La lettre reste l'unité insécable. 
 
### Un exemple en tidytext

## Stemmatisation,  lemmatisation et synonymisation

Les mots prennent des formes variées, il peut être intéressant dans certains cas de réduire cette variété et ne considérer que l'idée des mots. Deux techniques sont disponibles

### la stemmatisation

c'est un

### la lemmatisation

Un lemme est un mot racine, sans inflexions de genre, de nombre ou de conjugaison. C'est généralement celui qu'on trouve dans les dictionnaire.

### Synonymisation

le cas de wordnet et l'invention des synset

synonymes, antonymes, hipponyne, hyperonymes.....

https://cran.r-project.org/web/packages/wordnet/vignettes/wordnet.pdf



## Part of Speech (POS)

Dans une phrase les mots n'on pas la même valeur. Certains sont des nombres propres, ils se réfèrent à ce que nous venons de voir, c'est à dire des entitées nommées, d'autres désignent des catégories d'objet. Ce sont les noms communs qui se rapportent à des catégories de choses. Un marteau - si j'en avais un - peut être n'importe quel marteau, la masse qui casse la pierre, ou ce petit marteau qui me permet d'enfoncer un clou dans le cadre du tableau. 

Des typologies universelles ont été construites, elles recouvrent des typologies plus spécifiques à certaines langues. Les désinences du latin ont par exemple disparu du français. Cette forme est spécifiques au latin, on la retrouvera en allemand. La notion de morphosntaxique désigne présisément que les variations de formes des mots dépendent d'une règle syntaxique. Prenons le verbe, et sa forme,  "être", dont la forme au passé simple est "était". La forme des mots change, mais l'idée reste.

Une catégorisation en 17 éléments est proposée. En voici les [éléments et les définitions](https://universaldependencies.org/u/pos/)

Un petit exemple avec le package `UDpipe`. 

```{r 71, fig.cap='p', out.width='80%', fig.asp=.75, fig.align='center'}

library(udpipe)
fr <- udpipe_download_model(language = "french")
udmodel_french <- udpipe_load_model(file = "french-gsd-ud-2.5-191206.udpipe")
Citations <- read_csv("Citations.csv")

Flaubert<-Citations %>%
  filter(doc==1)

UD <- udpipe_annotate(udmodel_french, x=Flaubert$text)
x <- as.data.frame(UD)
foo<-x %>% 
  select(doc_id,paragraph_id, sentence_id, token_id,token,lemma,head_token_id, upos,feats)%>%filter(sentence_id==1)
flextable(foo)

```


Les trois première colonnes identifient le document, les phrases et les mots. Des lemmes sont proposées. La colonne UPOS donne les part of Speech universel.


## Dépendances syntaxiques

C'est à [Lucien Tesnière](https://www.ac-sciences-lettres-montpellier.fr/academie_edition/fichiers_conf/VERDELHAN-BOURGADE-2020.pdf) que l'on doit l'idée de la grammaire de la dépendance qui est au coeur du NLP moderne. L'idée est de déterminer au niveau de la phrase les relations entre ses termes de manière hierarchisée selon un principe de gouvernant à subordonné. 

@verdelhan-bourgade_lucien_2020 résume son analyse de manière précise et concise :

 * "Tous les mots n’ont pas le même statut. Les mots pleins, qui « expriment directement la pensée » (p. 59), relèvent de quatre catégories structurales : les substantifs (notés par O), les adjectifs (A), les verbes (I), les adverbes (E). Les mots dits vides (souvent désigné de manière pratique par les stopwords aujourdh'ui) précisent le sens des autres, ou servent à marquer des relations.La connexion établit la relation entre mot régissant et mot subordonné. Lorsqu’un régissant commande un subordonné, cela constitue un nœud, qui peut se faire à partir d’une des quatre espèces de mots pleins".
 
 Il en donne l’exemple suivant : « Très souvent mon vieil ami chante cette fort jolie chanson à ma fille » où l'on peut repèrer:
 
- un nœud verbal, central, qui commande des actants (ami, chanson, fille) et des circonstants (souvent). La valence est « le nombre de crochets par lesquels un verbe peut attraper des actants », à peu près équivalente à « voix ».
- les nœud substantivaux (ami, chanson, fille), qui commandent des compléments (mon, vieil, cette, jolie, ma)
- le nœud adjectival (jolie) qui commande ici le subordonné ‘fort’
- le nœud adverbial, très' étant subordonné à ‘souvent’.  "

![](Tesnière_dependance_syntaxique.jpg)

### Arbre syntaxique

L'arbre syntaxique est obtenue en analysant les relations entre les termes. Nous poursuivons avec UPpipe, l'annovation précédente a déjà fait le travail. A chaque mot deux informations sont associée : la première est l'index du mot auxquel il se rapporte, la seconde est la nature de la relation.

Onn utilise ici une fonction écrite par (bnosac](http://www.bnosac.be/index.php/blog/93-dependency-parsing-with-udpipe) pour donner une représentation graphique de l'arbre.

```{r 72, fig.cap='arbre de dépendance', out.width='80%', fig.asp=.75, fig.align='center'}

plot_annotation <- function(x, size = 3){
  stopifnot(is.data.frame(x) & all(c("doc_id","paragraph_id", "sentence_id", "token_id","token","lemma","head_token_id", "upos","feats", "dep_rel") %in% colnames(x)))
  x <- x[!is.na(x$head_token_id), ]
  x <- x[x$sentence_id %in% min(x$sentence_id), ]
  edges <- x[x$head_token_id != 0, c("token_id", "head_token_id", "dep_rel")]
  edges$label <- edges$dep_rel
  g <- graph_from_data_frame(edges,
                             vertices = x[, c("token_id", "token", "lemma", "upos", "xpos", "feats")],
                             directed = TRUE)
  ggraph(g, layout = "linear") +
    geom_edge_arc(ggplot2::aes(label = dep_rel, vjust = -0.20),
                  arrow = grid::arrow(length = unit(4, 'mm'), ends = "last", type = "closed"),
                  end_cap = ggraph::label_rect("wordswordswords"),
                  label_colour = "red", check_overlap = TRUE, label_size = size) +
    geom_node_label(ggplot2::aes(label = token), col = "darkgreen", size = size, fontface = "bold") +
    geom_node_text(ggplot2::aes(label = upos), nudge_y = -0.35, size = size)  +
    labs(title = "Tokenisation, PoS & dependency relations")
}

plot_annotation(x, size = 3)

```

### Vers des application plus générale

Dans la phrase precédente on note que funèbre est l'adjectif de journée. On peut être tenté de retrouver ces relations qui caractérisent des choses (les "nouns" ou noms choses) à des adjectifs. On souhaite faire une liste de ces paires.

L'exemple va être court : un poème de Maupassant
 
```{r 73, fig.cap='p', out.width='80%', fig.asp=.75, fig.align='center'}

Maupassant<-Citations %>%
  dplyr::filter(doc==2)

UD <- udpipe_annotate(udmodel_french, x=Maupassant$text)
foo <- as.data.frame(UD)
foo<- foo %>% select(paragraph_id,sentence_id, token_id,lemma,upos,head_token_id,dep_rel)%>%
  mutate(key1=paste0(paragraph_id,sentence_id, token_id),key2=paste0(paragraph_id,sentence_id, head_token_id) )

```


On va donc construires un tableau lemme_cible x lemmes_associés, les premiers risqueront d'êtres les noms communs, les seconds leurs adjectifs. 

```{r 74}
# selection de la relation.
res <- foo %>% filter(dep_rel == "amod")
#on y joint les dependences
dep<-res %>% left_join(foo, by = c("key2" = "key1"))

#on construit la tables des relations lemmes cibles -adjectifs
table<-as.data.frame.matrix(table(dep$lemma.x, dep$lemma.y)) 
#table$n<-rowSums(table)
#table$adj<-rownames(table)
#row.names(table) <- table$adj

```

le tableau obtenu est en fait la structure d'un graphe bipartite. la représentation passe par un  de `igraph` avec pour paramètres importants : 
* Taille des arcs (edge) : est proportionnelle à la force du lien ( nombre de relations)
* Taille des noeud : proportiennel au rangs du noeud.
* Couleur et forme des noeuds : lemme et lemme cible. 
* Un algorithme de force de @fruchterman_graph_1991 est employé pour représenter les positions relatives des mots et minimiser les superpositions.
 
Dessiner le réseau

```{r 75, fig.width=8, fig.height=7, caption ="Réseaux sémantiques"}
bg <-graph_from_incidence_matrix(table, weighted=TRUE)
summary(bg)

#E(bg)$weight# See the vertex attributes 
#V(bg)$type 
#V(bg)$name

# Plot the network
shape = ifelse(V(bg)$type, "circle", "square") # assign shape by node type
col = ifelse(V(bg)$type, "peachpuff", "darkolivegreen1") # assign color by node type
plot(bg, vertex.shape = shape, vertex.label.cex=.9,vertex.label.color="black",vertex.color = col,edge.color="azure2",vertex.frame.color=col,vertex.label.family="TT Arial", vertex.size=0.5*igraph::degree(bg),layout=layout_with_fr,edge.width=1*E(bg)$weight,edge.curved=0.5)
```

 
## reconnaissance d'entités nommées

En français courant les entités nommées correspondent largement à l'idée de noms propres. Un nom propre à une entité. Une chose qui est est indépendemment des catégories qui peuvent l'étiqueter. John Dupont, né le 19 février 1898 à Glasgow et abattu à Verdun  le 8 août 1917, est un personnage unique. John Dupont ne désigne par une catégorie, mais bien une personne singulière. La designation peut cependant être ambigüe, il y a un "Paris, Texas.", et un Paris sur Seine. La morphologie ne ressout pas l'ambiguité.

les entités nommées appartiennent à différentes catégories d'objets : des noms de lieux, des noms de personnes, des noms de marques, des acronymes d'organisation, 

Elles ne représentent jamais une catégorie mais une unité singulière.

https://cran.r-project.org/web/packages/nametagger/nametagger.pdf


## co-reférence




<!--chapter:end:07-Annotation-syntaxique.Rmd-->

# Constitution du corpus

Some _significant_ applications are demonstrated in this chapter.

## Scrapping

De ces deux approches on pourra considérer que la première correspond à un internet sauvage où la collecte d’information se traduit par une technique de chasseurs-cueilleurs, le glanage. Le recours aux APIs est civilisé, ne serait-ce parce qu'on introduit une sorte d’étiquette, des règles de courtoisie, un système de reconnaissance réciproque et d’attribution de droits. 
Le scraping est l’activité qui consiste à moissonner les informations disponibles sur le net en simulant et en automatisant la lecture par un butineur. Elle consiste à construire un robot capable de lire et d’enregistrer les informations disponibles sous forme html puis à les distribuer (parsing) dans des tableaux structurés, selon une stratégie d’exploration du web préalablement définie.
De nombreuses ressources sont disponibles, mais pour en rester à r , le package rvest permet de réaliser des extractions simples mais suffisantes pour de nombreux usages. 

Les caractéristiques clés du scraping :
    • La nécessité de programmer de manière ad hoc, en fonction des spécificités de chaque site. 
    • Des stratégies mécaniques, en boules de neige. 
    • Le risque de deny of service, c’est à dire de saturer ou de parasiter un système et de s’exposer à ses contre-mesures.
    • Les conditions légales ne sont pas homogènes et relèvent de différents droits : de la propriété intellectuelle, du respect de la vie privée, du droit de la concurrence. Cependant des facilités et tolérances sont souvent accordées quand c’est dans un objectif de recherche et que des précautions minimales d’anonymisation ou de pseudonymisation sont prises, et que les règles de conservation et de destruction des données sont précisées. 
    • La question éthique va au-delà du droit, elle concerne les conséquences de cette action sur l’évolution d’ensemble. On notera qu’elle participe à la “robotisation” du web ( plus de 50% du trafic résulterait de la circulation des spiders , scrapers2, sniffers et autres bots. Et qu’elle fait l’objet de contre-mesures.
Parmi les contre-mesures, le développement des APIs qui permet un accès contrôlé par le détenteur des informations en échange d’une facilitation de l’accès, et d’une plus grande fiabilité des données.
L’utilisation d’API lève l'ambiguïté légale qui accompagne le scraping est peut ainsi paraître comme plus civilisée. Elle nécessite naturellement que le gestionnaire de la base de données fournissent les moyens de s’identifier et de requêter, elle peut avoir l'inconvénient d’être coûteuse quand l’accès est payant. Elle affirme aussi le pouvoir de ceux qui contrôlent les données. Ces dernières ne sont pas, insuffisamment pour le chercheur, un bien commun.


### Prendre avantage de la structure du html

### des problèmes de droits et d'éthique


### rvest avec r




## API



### Un tour d'horizon des API

### Un exemple avec Rtwitter

library (twittr) # on appelle la librairie twittr qui permet les requêtes
consumerKey<-"Xq..."   #paramètres requis par l’ API de twitter (Ouvrir un compte au prélable)
consumerSecret<-"30l..."
access_token<-"27A..."
access_secret<-"zA7..."

fonction d’initialisation des requêtes
setup_twitter_oauth(consumerKey, consumerSecret, access_token,access_secret)

recherche des tweets
tweets1 <- searchTwitter("#IA", n = 2000, lang = "fr", resultType = "recent", since 
= "2019-08-01") 

transformer en data frame 
tweets_df1 <- twListToDF(tweets1) 

<!--chapter:end:08-LSA-NMF.Rmd-->

# Vectorisation du corpus


## Word2vec



### vectorisation pipe

### closest2

### map with tsne




## paragraph2vec



## l'avenir des modèles pré-entrainés



<!--chapter:end:09-Vectorisation.Rmd-->

# Topic  Analysis


## LDA

### le modèle de blei

### implementation avec wor2vec

### Représentation graphique

### La 


## STM

###

###




<!--chapter:end:10-Topic-analysis.Rmd-->

# Machine learning with text

## simples models


### Naives bayes

### elastic net

### RF

## art of featuring

utiliser les plongements




<!--chapter:end:11-ML-with-text.Rmd-->

# Machine learning with text

## simples models


### Naives bayes

### elastic net

### RF

## art of featuring

utiliser les plongements




<!--chapter:end:12-deep-Learning-with-text.Rmd-->

# Machine learning with text

## simples models


### Naives bayes

### elastic net

### RF

## art of featuring

utiliser les plongements




<!--chapter:end:13-Translation.Rmd-->

# Machine learning with text

## simples models


### Naives bayes

### elastic net

### RF

## art of featuring

utiliser les plongements




<!--chapter:end:14-Generative-model.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`

<!--chapter:end:15-References.Rmd-->

